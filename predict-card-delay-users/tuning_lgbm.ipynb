{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just to Get Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory optimization from  6.86MB to  1.92MB (72.1% reduction)\n",
      "Memory optimization from  2.59MB to  0.72MB (72.1% reduction)\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import joblib\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from itertools import combinations\n",
    "from eli5.permutation_importance import get_score_importances\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "from sklearn import cluster\n",
    "\n",
    "import optuna\n",
    "from optuna import Trial\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.visualization import plot_contour, plot_optimization_history\n",
    "from optuna.visualization import plot_parallel_coordinate, plot_slice, plot_param_importances\n",
    "from hyperopt.pyll.base import scope\n",
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "from catboost import Pool, CatBoostClassifier\n",
    "from kaggler.model import AutoLGB\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, GridSearchCV, StratifiedShuffleSplit, train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, log_loss\n",
    "import random\n",
    "\n",
    "train = pd.read_csv('data/train.csv').drop(['index', 'FLAG_MOBIL'], axis=1).fillna('NAN')\n",
    "test = pd.read_csv('data/test.csv').drop(['index', 'FLAG_MOBIL'], axis=1).fillna('NAN')\n",
    "sample_submission = pd.read_csv('data/sample_submission.csv')\n",
    "\n",
    "# train데이터와 test데이터 변수를 함께 조정하기 위해 병합\n",
    "merge_data = pd.concat([train, test], axis = 0)\n",
    "\n",
    "# DAYS_BIRTH\n",
    "merge_data['DAYS_BIRTH_month']=np.floor((-merge_data['DAYS_BIRTH'])/30)-(\n",
    "    (np.floor((-merge_data['DAYS_BIRTH'])/30)/12).astype(int)*12)\n",
    "merge_data['DAYS_BIRTH_week']=np.floor((-merge_data['DAYS_BIRTH'])/7)-(\n",
    "    (np.floor((-merge_data['DAYS_BIRTH'])/7)/4).astype(int)*4)\n",
    "\n",
    "# DAYS_EMPLOYED\n",
    "merge_data['DAYS_EMPLOYED_month']=np.floor((-merge_data['DAYS_EMPLOYED'])/30)-(\n",
    "    (np.floor((-merge_data['DAYS_EMPLOYED'])/30)/12).astype(int)*12)\n",
    "merge_data['DAYS_EMPLOYED_week']=np.floor((-merge_data['DAYS_EMPLOYED'])/7)-(\n",
    "    (np.floor((-merge_data['DAYS_EMPLOYED'])/7)/4).astype(int)*4)\n",
    "\n",
    "# before_EMPLOYED\n",
    "merge_data['before_EMPLOYED']=merge_data['DAYS_BIRTH']-merge_data['DAYS_EMPLOYED']\n",
    "merge_data['before_EMPLOYED_month']=np.floor((-merge_data['before_EMPLOYED'])/30)-(\n",
    "    (np.floor((-merge_data['before_EMPLOYED'])/30)/12).astype(int)*12)\n",
    "merge_data['before_EMPLOYED_week']=np.floor((-merge_data['before_EMPLOYED'])/7)-(\n",
    "    (np.floor((-merge_data['before_EMPLOYED'])/7)/4).astype(int)*4)\n",
    "\n",
    "# DAYS_BIRTH / Income\n",
    "merge_data['DAYS_BIRTH_month/income_total'] = merge_data['DAYS_BIRTH_month'] / merge_data['income_total']\n",
    "merge_data['DAYS_BIRTH_week/income_total'] = merge_data['DAYS_BIRTH_week'] / merge_data['income_total']\n",
    "\n",
    "# DAYS_EMPLOYED / Income\n",
    "merge_data['DAYS_EMPLOYED_month/income_total'] = merge_data['DAYS_EMPLOYED_month'] / merge_data['income_total']\n",
    "merge_data['DAYS_EMPLOYED_week/income_total'] = merge_data['DAYS_EMPLOYED_week'] / merge_data['income_total']\n",
    "\n",
    "# before_EMPLOYED / Income\n",
    "merge_data['before_EMPLOYED/income_total'] = merge_data['before_EMPLOYED'] / merge_data['income_total']\n",
    "merge_data['before_EMPLOYED_month/income_total'] = merge_data['before_EMPLOYED_month'] / merge_data['income_total']\n",
    "merge_data['before_EMPLOYED_week/income_total'] = merge_data['before_EMPLOYED_week'] / merge_data['income_total']\n",
    "\n",
    "# Income / Family\n",
    "merge_data['income_total/family_size'] = merge_data['income_total'] / merge_data['family_size']\n",
    "\n",
    "merge_data['child_num/income_total'] = merge_data['child_num'] / merge_data['income_total']\n",
    "merge_data['family_size/income_total'] = merge_data['family_size'] / merge_data['income_total']\n",
    "merge_data['DAYS_BIRTH/income_total'] = merge_data['DAYS_BIRTH'] / merge_data['income_total']\n",
    "merge_data['DAYS_EMPLOYED/income_total'] = merge_data['DAYS_EMPLOYED'] / merge_data['income_total']\n",
    "merge_data['DAYS_EMPLOYED/DAYS_BIRTH'] =  merge_data['DAYS_EMPLOYED'] / merge_data['DAYS_BIRTH']\n",
    "\n",
    "# Income skewed-data\n",
    "merge_data['income_total'] = np.log1p(merge_data['income_total'])\n",
    "# merge_data['log_income_total'] = np.log(merge_data['income_total'])\n",
    "# merge_data['sqrt_income_total'] = np.sqrt(merge_data['income_total'])\n",
    "# merge_data['boxcox_income_total'] = stats.boxcox(merge_data['income_total'])[0]\n",
    "\n",
    "merge_data = merge_data.fillna(-999)\n",
    "train = merge_data[merge_data['credit'] != -999]\n",
    "test = merge_data[merge_data['credit'] == -999]\n",
    "test.drop('credit', axis = 1, inplace = True)\n",
    "\n",
    "train_cols = list(train.columns); train_cols.remove('credit'); train_cols.append('credit')\n",
    "train = train[train_cols]\n",
    "\n",
    "train_x = train.drop(['credit'], axis=1) # 데이터 나누기\n",
    "train_y = train['credit']\n",
    "test_x = test.copy()\n",
    "\n",
    "enc = LabelEncoder()\n",
    "for col in train_x.columns:\n",
    "    if (train_x[col].dtypes=='O'):\n",
    "        train_x[col] = enc.fit_transform(train_x[col])\n",
    "        test_x[col] = enc.fit_transform(test_x[col])\n",
    "\n",
    "def reduce_mem_usage(data):\n",
    "    numerics = ['int8', 'int16', 'int32', 'int64', 'float32', 'float64']\n",
    "    start_memory = data.memory_usage().sum() / 1024**2    \n",
    "    for col in data.columns:\n",
    "        col_type = data[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = data[col].min()\n",
    "            c_max = data[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    data[col] = data[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    data[col] = data[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    data[col] = data[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    data[col] = data[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    data[col] = data[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    data[col] = data[col].astype(np.float32)\n",
    "                else:\n",
    "                    data[col] = data[col].astype(np.float64)    \n",
    "    end_memory = data.memory_usage().sum() / 1024**2\n",
    "    print('Memory optimization from {:5.2f}MB to {:5.2f}MB ({:.1f}% reduction)'\n",
    "          .format(start_memory, end_memory, 100 * (start_memory - end_memory) / start_memory))\n",
    "    return data\n",
    "train_x = reduce_mem_usage(train_x)\n",
    "test_x = reduce_mem_usage(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_objective(trial: Trial) -> float:\n",
    "    params_lgb = {\n",
    "        \"random_state\": 91373,\n",
    "        \"verbosity\": -1,\n",
    "        \"n_estimators\": 10000,\n",
    "        \"objective\": \"multiclass\",\n",
    "        \"metric\": \"multi_logloss\",\n",
    "        'learning_rate': trial.suggest_uniform(\"learning_rate\", 0.003, 0.006),\n",
    "        \"reg_alpha\": trial.suggest_uniform(\"reg_alpha\", 0.8, 1),\n",
    "        \"reg_lambda\": trial.suggest_uniform(\"reg_lambda\", 0.9, 1),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 10, 15),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 1000, 1400),\n",
    "        \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0.3, 0.4), # feature_fraction\n",
    "        \"subsample\": trial.suggest_uniform(\"subsample\", 0.6, 1.0),\n",
    "        \"subsample_freq\": trial.suggest_int(\"subsample_freq\", 10, 30),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 2, 5),\n",
    "        \"min_child_weight\": trial.suggest_uniform(\"min_child_weight\", 1, 3),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 150, 400),\n",
    "    }\n",
    "    \n",
    "    seed = 91373\n",
    "    kfold = StratifiedKFold(n_splits=5, random_state = seed, shuffle = True) # Cross-validation cv=5\n",
    "    cv = np.zeros((train_x.shape[0], 3))\n",
    "\n",
    "    for n, (train_idx, val_idx) in enumerate(kfold.split(train_x, train_y)):\n",
    "\n",
    "        x_train, x_val = train_x.iloc[train_idx], train_x.iloc[val_idx]\n",
    "        y_train, y_val = train_y.iloc[train_idx], train_y.iloc[val_idx]\n",
    "\n",
    "        lgbmodel = LGBMClassifier(**params_lgb)\n",
    "                                                                                        # 진행상황 보고싶을때 None을 100으로\n",
    "        lgbmodel.fit(x_train, y_train, eval_set=[(x_val, y_val)], early_stopping_rounds=30, verbose=None) \n",
    "        cv[val_idx, :] = lgbmodel.predict_proba(x_val)\n",
    "    # print('multi_logloss:', log_loss(train_y, cv))\n",
    "\n",
    "    \n",
    "    return log_loss(train_y, cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = TPESampler(seed=42)\n",
    "lgb_study = optuna.create_study(study_name=\"lgbm_parameter_opt\", direction=\"minimize\", sampler=sampler)\n",
    "lgb_study.optimize(lgb_objective, n_trials=80)\n",
    "\n",
    "lgb_best_hyperparams = lgb_study.best_trial.params\n",
    "lgb_base_hyperparams = {'objective':'multiclass', 'n_estimators':10000,\n",
    "                        'lambda_l1':lgb_best_hyperparams['reg_alpha'],\n",
    "                        'lambda_l2':lgb_best_hyperparams['reg_lambda'],\n",
    "                        'reg_alpha':None, 'reg_lambda':None}\n",
    "lgb_best_hyperparams.update(lgb_base_hyperparams)\n",
    "print(\"The best hyperparameters are:\\n\", lgb_best_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 나오자마자 복붙해서 카톡방에 올려서 따로 저장해두기!! (파라미터값, logg값 둘 다)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_optimization_history(lgb_study) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_param_importances(lgb_study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_slice(lgb_study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위 그래프들은 지우지 말고 그대로 커밋해주기 (결과 보고 범위 다시 지정해야함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.6874434552997744\n",
    "lgb_best_hyperparams = {'learning_rate': 0.00584665661176, 'reg_alpha': 0.9690688297671034, 'reg_lambda': 0.827557613304815, \n",
    "                        'max_depth': 11, 'num_leaves': 1039, 'colsample_bytree': 0.3684233026512157, 'subsample': 0.7760609974958406, \n",
    "                        'subsample_freq': 12, 'min_child_samples': 3, 'min_child_weight': 1.1375540844608736, 'max_bin': 378}\n",
    "lgb_base_hyperparams = {'objective':'multiclass', 'n_estimators':10000,\n",
    "                        'lambda_l1':lgb_best_hyperparams['reg_alpha'],\n",
    "                        'lambda_l2':lgb_best_hyperparams['reg_lambda'],\n",
    "                        'reg_alpha':None, 'reg_lambda':None}\n",
    "lgb_best_hyperparams.update(lgb_base_hyperparams)\n",
    "print(\"The best hyperparameters are:\\n\", lgb_best_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.6880994851132427\n",
    "lgb_best_hyperparams = {'learning_rate': 0.0034367023002911156, 'reg_alpha': 0.3317220484544238, 'reg_lambda': 0.6130217550401365, \n",
    "                        'max_depth': 12, 'num_leaves': 1121, 'colsample_bytree': 0.3751708953699282, 'subsample': 0.7999312558934519, \n",
    "                        'subsample_freq': 18, 'min_child_samples': 3, 'min_child_weight': 2.4433589918560807, 'max_bin': 223}\n",
    "lgb_base_hyperparams = {'objective':'multiclass', 'n_estimators':10000,\n",
    "                        'lambda_l1':lgb_best_hyperparams['reg_alpha'],\n",
    "                        'lambda_l2':lgb_best_hyperparams['reg_lambda'],\n",
    "                        'reg_alpha':None, 'reg_lambda':None}\n",
    "lgb_best_hyperparams.update(lgb_base_hyperparams)\n",
    "print(\"The best hyperparameters are:\\n\", lgb_best_hyperparams)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
