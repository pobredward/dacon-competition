{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import glob\n",
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "from catboost import Pool, CatBoostClassifier\n",
    "from kaggler.model import AutoLGB\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, GridSearchCV, StratifiedShuffleSplit, train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, log_loss\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 문제 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 데이터 수집"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) 데이콘 기본 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv').drop(['index'], axis=1).fillna('NAN')\n",
    "test = pd.read_csv('data/test.csv').drop(['index'], axis=1).fillna('NAN')\n",
    "sample_submission = pd.read_csv('data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 탐색적 데이터 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 변수 조정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) 이상치 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_data = pd.concat([train, test], axis = 0)\n",
    "\n",
    "# DAYS_BIRTH\n",
    "merge_data['DAYS_BIRTH_month']=np.floor((-merge_data['DAYS_BIRTH'])/30)-(\n",
    "    (np.floor((-merge_data['DAYS_BIRTH'])/30)/12).astype(int)*12)\n",
    "merge_data['DAYS_BIRTH_week']=np.floor((-merge_data['DAYS_BIRTH'])/7)-(\n",
    "    (np.floor((-merge_data['DAYS_BIRTH'])/7)/4).astype(int)*4)\n",
    "\n",
    "# DAYS_EMPLOYED\n",
    "merge_data['DAYS_EMPLOYED_month']=np.floor((-merge_data['DAYS_EMPLOYED'])/30)-(\n",
    "    (np.floor((-merge_data['DAYS_EMPLOYED'])/30)/12).astype(int)*12)\n",
    "merge_data['DAYS_EMPLOYED_week']=np.floor((-merge_data['DAYS_EMPLOYED'])/7)-(\n",
    "    (np.floor((-merge_data['DAYS_EMPLOYED'])/7)/4).astype(int)*4)\n",
    "\n",
    "# before_EMPLOYED\n",
    "merge_data['before_EMPLOYED']=merge_data['DAYS_BIRTH']-merge_data['DAYS_EMPLOYED']\n",
    "merge_data['before_EMPLOYED_month']=np.floor((-merge_data['before_EMPLOYED'])/30)-(\n",
    "    (np.floor((-merge_data['before_EMPLOYED'])/30)/12).astype(int)*12)\n",
    "merge_data['before_EMPLOYED_week']=np.floor((-merge_data['before_EMPLOYED'])/7)-(\n",
    "    (np.floor((-merge_data['before_EMPLOYED'])/7)/4).astype(int)*4)\n",
    "\n",
    "# DAYS_BIRTH\n",
    "merge_data['1new_1'] = merge_data['DAYS_BIRTH_month'] / merge_data['income_total']\n",
    "merge_data['2new_1'] = merge_data['DAYS_BIRTH_week'] / merge_data['income_total']\n",
    "\n",
    "# DAYS_EMPLOYED\n",
    "merge_data['10new_1'] = merge_data['DAYS_EMPLOYED_month'] / merge_data['income_total']\n",
    "merge_data['11new_1'] = merge_data['DAYS_EMPLOYED_week'] / merge_data['income_total']\n",
    "\n",
    "# before_EMPLOYED\n",
    "merge_data['12new_1'] = merge_data['before_EMPLOYED'] / merge_data['income_total']\n",
    "merge_data['13new_1'] = merge_data['before_EMPLOYED_month'] / merge_data['income_total']\n",
    "merge_data['14new_1'] = merge_data['before_EMPLOYED_week'] / merge_data['income_total']\n",
    "\n",
    "# 융합 삭제\n",
    "#merge_data['3new_1'] = merge_data['DAYS_EMPLOYED_month'] / merge_data['DAYS_BIRTH_month']\n",
    "#merge_data['4new_1'] = merge_data['DAYS_EMPLOYED_month'] / merge_data['DAYS_BIRTH_week']\n",
    "#merge_data['5new_1'] = merge_data['DAYS_EMPLOYED_week'] / merge_data['DAYS_BIRTH_month']\n",
    "#merge_data['6new_1'] = merge_data['DAYS_EMPLOYED_week'] / merge_data['DAYS_BIRTH_week']\n",
    "\n",
    "#merge_data['7new_1'] =  merge_data['begin_month'] / merge_data['DAYS_BIRTH_month']\n",
    "#merge_data['8new_1'] =  merge_data['begin_month'] / merge_data['DAYS_EMPLOYED_month']\n",
    "#merge_data['9new_1'] =  merge_data['begin_month'] / merge_data['before_EMPLOYED_month']\n",
    "\n",
    "\n",
    "# 소득대비 \n",
    "merge_data['DAYS_BIRTH'] = merge_data['DAYS_BIRTH'] / -365\n",
    "merge_data['DAYS_EMPLOYED'] = merge_data['DAYS_EMPLOYED'] / -365\n",
    "\n",
    "merge_data['new_1'] = merge_data['child_num'] / merge_data['income_total']\n",
    "merge_data['new_2'] = merge_data['family_size'] / merge_data['income_total']\n",
    "merge_data['new_3'] = merge_data['DAYS_BIRTH'] / merge_data['income_total']\n",
    "merge_data['new_4'] = merge_data['DAYS_EMPLOYED'] / merge_data['income_total']\n",
    "merge_data['new_5'] = merge_data['begin_month'] / merge_data['income_total']\n",
    "merge_data['new_6'] =  merge_data['DAYS_EMPLOYED'] / merge_data['DAYS_BIRTH']\n",
    "\n",
    "# 소득 skewed-data 처리\n",
    "# merge_data['log_income_total'] = np.log(merge_data['income_total'])\n",
    "# merge_data['sqrt_income_total'] = np.sqrt(merge_data['income_total'])\n",
    "# merge_data['boxcox_income_total'] = stats.boxcox(merge_data['income_total'])[0]\n",
    "\n",
    "merge_data = merge_data.fillna(-999)\n",
    "train = merge_data[merge_data['credit'] != -999]\n",
    "test = merge_data[merge_data['credit'] == -999]\n",
    "test.drop('credit', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "train = train[train['child_num']<=6].reset_index(drop=True) # 아이의 수가 7명 이상인 데이터 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_oh = train.copy()\n",
    "train_noh = train.copy()\n",
    "test_oh = test.copy()\n",
    "test_noh = test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_col = []\n",
    "for col in train_noh.columns:\n",
    "    if train_noh[col].dtype == 'object':\n",
    "        train_noh[col] = train_noh[col].astype('category')\n",
    "        test_noh[col] = test_noh[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gender', 'car', 'reality', 'income_type', 'edu_type', 'family_type', 'house_type', 'occyp_type']\n"
     ]
    }
   ],
   "source": [
    "object_col = []\n",
    "for col in train_oh.columns:\n",
    "    if train_oh[col].dtype == 'object':\n",
    "        object_col.append(col)\n",
    "print(object_col)        \n",
    "enc = OneHotEncoder()\n",
    "enc.fit(train.loc[:,object_col])\n",
    "\n",
    "\n",
    "train_onehot_df = pd.DataFrame(enc.transform(train_oh.loc[:,object_col]).toarray(), \n",
    "             columns=enc.get_feature_names(object_col))\n",
    "train_oh.drop(object_col, axis=1, inplace=True)\n",
    "train_oh = pd.concat([train_oh, train_onehot_df], axis=1)    \n",
    "\n",
    "test_onehot_df = pd.DataFrame(enc.transform(test_oh.loc[:,object_col]).toarray(), \n",
    "             columns=enc.get_feature_names(object_col))\n",
    "test_oh.drop(object_col, axis=1, inplace=True)\n",
    "test_oh = pd.concat([test_oh, test_onehot_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "true = train[['credit']]\n",
    "true['0'] = true['credit'][true['credit']==0.0]\n",
    "true['1'] = true['credit'][true['credit']==1.0]\n",
    "true['2'] = true['credit'][true['credit']==2.0]\n",
    "del true['credit']\n",
    "true = true.replace([0.0, 2.0], [1.0, 1.0])\n",
    "true = true.fillna(0)\n",
    "true = true.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dict = {}\n",
    "pred_test_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_noh.drop(['credit'], axis=1)\n",
    "train_y = train_noh['credit']\n",
    "test_x = test_noh.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 seeds x 5 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi_logloss : 0.6915101725662152\n",
      "multi_logloss : 0.6915843058850291\n",
      "multi_logloss : 0.6912776413331292\n"
     ]
    }
   ],
   "source": [
    "lucky_seeds=[42,2019,91373]\n",
    "for i, seed in enumerate(lucky_seeds):\n",
    "\n",
    "    kfold = KFold(n_splits=10, random_state = seed, shuffle = True)\n",
    "    cv=np.zeros((train.shape[0], 3))\n",
    "    pred_test = np.zeros((test_x.shape[0], 3), dtype=float)\n",
    "    \n",
    "    for n, (train_idx, val_idx) in enumerate(kfold.split(train)):\n",
    "\n",
    "        x_train, x_val = train_x.iloc[train_idx], train_x.iloc[val_idx]\n",
    "        y_train, y_val = train_y.iloc[train_idx], train_y.iloc[val_idx]\n",
    "\n",
    "        lgbmodel = LGBMClassifier(learning_rate=0.005, objective='multiclass', n_estimators=10000, num_leaves=1000, \n",
    "                                  max_depth=-1, min_child_weight=2, colsample_bytree=0.4,  \n",
    "                                   n_jobs=-1, random_state=seed)\n",
    "\n",
    "        lgbmodel.fit(x_train, y_train, eval_set=[(x_val, y_val)], early_stopping_rounds=30, verbose=None) \n",
    "        #joblib.dump(lgbmodel, f'./pred_pkl/LGB_{n+1}_fold_{seed}_seed_lgb.pkl')\n",
    "\n",
    "        # CROSS-VALIDATION , EVALUATE CV\n",
    "        cv[val_idx,:] = lgbmodel.predict_proba(x_val)\n",
    "        pred_test += lgbmodel.predict_proba(test_x) / 10\n",
    "    pred_dict['lgb'+str(i+1)] = cv\n",
    "    pred_test_dict['lgb'+str(i+1)] = pred_test\n",
    "        \n",
    "    print('multi_logloss :', log_loss(true, cv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lgbmodels_path = os.listdir('./pred_pkl/')\n",
    "lgbmodels_list = [x for x in lgbmodels_path if x.endswith(\"lgb.pkl\")]\n",
    "assert len(lgbmodels_list) == 15\n",
    "lgb_preds = np.zeros((test_x.shape[0], 3))\n",
    "\n",
    "for m in lgbmodels_list:\n",
    "    lgbmodel = joblib.load('./pred_pkl/'+m)\n",
    "    lgb_preds_proba = lgbmodel.predict_proba(test)\n",
    "    lgb_preds += lgb_preds_proba/15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_oh.drop(['credit'], axis=1)\n",
    "train_y = train_oh['credit']\n",
    "test_x = test_oh.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Tuning (hyperopt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_x, train_y, test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with params:                                \n",
      "{'colsample_bylevel': 0.65, 'colsample_bytree': 0.35000000000000003, 'eta': 0.008, 'eval_metric': 'mlogloss', 'gamma': 0.75, 'max_depth': 65, 'min_child_weight': 4.0, 'num_class': 3, 'objective': 'multi:softprob', 'seed': 0, 'subsample': 0.8500000000000001}\n",
      "[0]\ttrain-mlogloss:1.09368\tvalid-mlogloss:1.09467    \n",
      "\n",
      "[646]\ttrain-mlogloss:0.34869\tvalid-mlogloss:0.68941  \n",
      "\n",
      "\tlog_loss <function log_loss at 0x7ff390cbe9d0>      \n",
      "\n",
      "\n",
      "Training with params:                                                           \n",
      "{'colsample_bylevel': 0.8500000000000001, 'colsample_bytree': 0.45, 'eta': 0.005, 'eval_metric': 'mlogloss', 'gamma': 0.9, 'max_depth': 36, 'min_child_weight': 2.0, 'num_class': 3, 'objective': 'multi:softprob', 'seed': 0, 'subsample': 0.8}\n",
      "[0]\ttrain-mlogloss:1.09492\tvalid-mlogloss:1.09593                               \n",
      "\n",
      "[866]\ttrain-mlogloss:0.33132\tvalid-mlogloss:0.69113                             \n",
      "\n",
      "\tlog_loss <function log_loss at 0x7ff390cbe9d0>                                 \n",
      "\n",
      "\n",
      "Training with params:                                                           \n",
      "{'colsample_bylevel': 0.35000000000000003, 'colsample_bytree': 0.8, 'eta': 0.004, 'eval_metric': 'mlogloss', 'gamma': 0.8500000000000001, 'max_depth': 57, 'min_child_weight': 4.0, 'num_class': 3, 'objective': 'multi:softprob', 'seed': 0, 'subsample': 0.9500000000000001}\n",
      "[0]\ttrain-mlogloss:1.09581\tvalid-mlogloss:1.09649                               \n",
      "\n",
      "[988]\ttrain-mlogloss:0.35572\tvalid-mlogloss:0.69544                             \n",
      "\n",
      "\tlog_loss <function log_loss at 0x7ff390cbe9d0>                                 \n",
      "\n",
      "\n",
      "100%|██████████| 3/3 [19:26<00:00, 388.90s/trial, best loss: 0.6892133230527895]\n",
      "The best hyperparameters are:  \n",
      "\n",
      "{'colsample_bylevel': 0.65, 'colsample_bytree': 0.35000000000000003, 'eta': 0.008, 'gamma': 0.75, 'max_depth': 45, 'min_child_weight': 4.0, 'subsample': 0.8500000000000001}\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------------------------#\n",
    "SEED=0\n",
    "# Scoring and optimization functions\n",
    "\n",
    "\n",
    "def score(params):\n",
    "    print(\"Training with params: \")\n",
    "    print(params)\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dvalid = xgb.DMatrix(X_test, label=y_test)\n",
    "    watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n",
    "    gbm_model = xgb.train(params, dtrain, 100000,\n",
    "                          evals=watchlist,\n",
    "                          early_stopping_rounds=30,\n",
    "                          verbose_eval=100000)\n",
    "    predictions = gbm_model.predict(dvalid,\n",
    "                                    ntree_limit=gbm_model.best_iteration + 1)\n",
    "    score = log_loss(y_test, predictions)\n",
    "    # TODO: Add the importance for the selected features\n",
    "    print(\"\\tlog_loss {0}\\n\\n\".format(log_loss))\n",
    "    # The score function should return the loss (1-score)\n",
    "    # since the optimize function looks for the minimum\n",
    "    return {'loss': score, 'status': STATUS_OK}\n",
    "\n",
    "\n",
    "def optimize(random_state=SEED):\n",
    "    \"\"\"\n",
    "    This is the optimization function that given a space (space here) of \n",
    "    hyperparameters and a scoring function (score here), finds the best hyperparameters.\n",
    "    \"\"\"\n",
    "    # To learn more about XGBoost parameters, head to this page: \n",
    "    # https://github.com/dmlc/xgboost/blob/master/doc/parameter.md\n",
    "    param = {'objective':'multi:softprob', 'seed':seed, 'num_class': 3, 'eval_metric':'mlogloss', \n",
    "         'eta': 0.01, 'min_child_weight': 3,\n",
    "         'colsample_bytree': 0.3, 'colsample_bylevel': 0.6, 'subsample': 0.8\n",
    "        }\n",
    "    space = {\n",
    "        'eta': hp.quniform('eta', 0.004, 0.01, 0.001),\n",
    "        # A problem with max_depth casted to float instead of int with\n",
    "        # the hp.quniform method.\n",
    "        'max_depth':  hp.choice('max_depth', np.arange(20, 70, dtype=int)),\n",
    "        'min_child_weight': hp.quniform('min_child_weight', 1, 6, 1),\n",
    "        'subsample': hp.quniform('subsample', 0.5, 1, 0.05),\n",
    "        'gamma': hp.quniform('gamma', 0.5, 1, 0.05),\n",
    "        'colsample_bytree': hp.quniform('colsample_bytree', 0.3, 1, 0.05),\n",
    "        'colsample_bylevel': hp.quniform('colsample_bylevel', 0.3, 1, 0.05),\n",
    "        'eval_metric': 'mlogloss',\n",
    "        'objective' : 'multi:softprob',\n",
    "        # Increase this number if you have more cores. Otherwise, remove it and it will default \n",
    "        # to the maxium number. \n",
    "        'num_class' : 3,\n",
    "        'seed': random_state\n",
    "    }\n",
    "    # Use the fmin function from Hyperopt to find the best hyperparameters\n",
    "    best = fmin(score, space, algo=tpe.suggest, \n",
    "                # trials=trials, \n",
    "                max_evals=3)\n",
    "    return best\n",
    "\n",
    "#-------------------------------------------------#\n",
    "\n",
    "\n",
    "# Load processed data\n",
    "\n",
    "# You could use the following script to generate a well-processed train and test data sets:\n",
    "# https://www.kaggle.com/yassinealouini/predicting-red-hat-business-value/features-processing\n",
    "# I have only used the .head() of the data sets since the process takes a long time to run.\n",
    "# I have also put the act_train and act_test data sets since I don't have the processed data sets \n",
    "# loaded. \n",
    "\n",
    "\n",
    "\n",
    "#-------------------------------------------------#\n",
    "\n",
    "# Run the optimization\n",
    "\n",
    "# Trials object where the history of search will be stored\n",
    "# For the time being, there is a bug with the following version of hyperopt.\n",
    "# You can read the error messag on the log file.\n",
    "# For the curious, you can read more about it here: https://github.com/hyperopt/hyperopt/issues/234\n",
    "# => So I am commenting it.\n",
    "# trials = Trials()\n",
    "\n",
    "best_hyperparams = optimize()\n",
    "print(\"The best hyperparameters are: \", \"\\n\")\n",
    "print(best_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = pd.DataFrame(y_test, columns=['credit'])\n",
    "y_test['0'] = y_test['credit'][y_test['credit']==0.0]\n",
    "y_test['1'] = y_test['credit'][y_test['credit']==1.0]\n",
    "y_test['2'] = y_test['credit'][y_test['credit']==2.0]\n",
    "del y_test['credit']\n",
    "y_test = y_test.replace([0.0, 2.0], [1.0, 1.0])\n",
    "y_test = y_test.fillna(0)\n",
    "y_test = y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.DataFrame(y_train, columns=['credit'])\n",
    "y_train['0'] = y_train['credit'][y_train['credit']==0.0]\n",
    "y_train['1'] = y_train['credit'][y_train['credit']==1.0]\n",
    "y_train['2'] = y_train['credit'][y_train['credit']==2.0]\n",
    "del y_train['credit']\n",
    "y_train = y_train.replace([0.0, 2.0], [1.0, 1.0])\n",
    "y_train = y_train.fillna(0)\n",
    "y_train = y_train.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "true = train[['credit']]\n",
    "true['0'] = true['credit'][true['credit']==0.0]\n",
    "true['1'] = true['credit'][true['credit']==1.0]\n",
    "true['2'] = true['credit'][true['credit']==2.0]\n",
    "del true['credit']\n",
    "true = true.replace([0.0, 2.0], [1.0, 1.0])\n",
    "true = true.fillna(0)\n",
    "true = true.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Tuning (GridSearchCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    5.9s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   9 | elapsed:    5.9s remaining:   21.0s\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   9 | elapsed:    5.9s remaining:   12.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   9 | elapsed: 26.4min remaining: 33.0min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   9 | elapsed: 32.3min remaining: 25.8min\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   9 | elapsed: 33.0min remaining: 16.5min\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of   9 | elapsed: 39.0min remaining: 11.1min\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed: 44.4min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed: 44.4min finished\n",
      "[10:03:31] DEBUG: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/tree/updater_gpu_hist.cu:899: [GPU Hist]: Configure\n",
      "[10:03:31] ======== Monitor: SketchContainer ========\n",
      "[10:03:31] MakeCuts: 0.001705s, 1 calls @ 1705us\n",
      "\n",
      "[10:03:31] Prune: 0.000932s, 1 calls @ 932us\n",
      "\n",
      "[10:03:31] ScanInput: 0.001104s, 1 calls @ 1104us\n",
      "\n",
      "[10:03:31] Unique: 0.000557s, 1 calls @ 557us\n",
      "\n",
      "[10:03:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:11:13] ======== Monitor: Learner ========\n",
      "[10:11:13] Configure: 0.003938s, 1 calls @ 3938us\n",
      "\n",
      "[10:11:13] EvalOneIter: 0.002027s, 100 calls @ 2027us\n",
      "\n",
      "[10:11:13] GetGradient: 0.0091s, 100 calls @ 9100us\n",
      "\n",
      "[10:11:13] PredictRaw: 0.223043s, 100 calls @ 223043us\n",
      "\n",
      "[10:11:13] UpdateOneIter: 457.245s, 100 calls @ 457245108us\n",
      "\n",
      "[10:11:13] ======== Monitor: GBTree ========\n",
      "[10:11:13] BoostNewTrees: 457.005s, 100 calls @ 457005257us\n",
      "\n",
      "[10:11:13] CommitModel: 0.00014s, 100 calls @ 140us\n",
      "\n",
      "[10:11:13] ======== Device 0 Memory Allocations:  ========\n",
      "[10:11:13] Peak memory usage: 377MiB\n",
      "[10:11:13] Number of allocations: 4524583\n",
      "[10:11:13] ======== Monitor: updater_gpu_hist ========\n",
      "[10:11:13] InitData: 2.88471s, 300 calls @ 2884714us\n",
      "\n",
      "[10:11:13] InitDataOnce: 2.88466s, 1 calls @ 2884658us\n",
      "\n",
      "[10:11:13] Update: 456.876s, 300 calls @ 456875881us\n",
      "\n",
      "[10:11:13] ======== Monitor: gradient_based_sampler ========\n",
      "[10:11:13] Sample: 0.032102s, 300 calls @ 32102us\n",
      "\n",
      "[10:11:13] ======== Monitor: GPUHistMakerDevice0 ========\n",
      "[10:11:13] AllReduce: 0.088616s, 464498 calls @ 88616us\n",
      "\n",
      "[10:11:13] BuildHist: 4.36747s, 464198 calls @ 4367466us\n",
      "\n",
      "[10:11:13] EvaluateSplits: 8.21575s, 464198 calls @ 8215747us\n",
      "\n",
      "[10:11:13] FinalisePosition: 0.021678s, 300 calls @ 21678us\n",
      "\n",
      "[10:11:13] InitRoot: 0.643591s, 300 calls @ 643591us\n",
      "\n",
      "[10:11:13] Reset: 360.443s, 300 calls @ 360443313us\n",
      "\n",
      "[10:11:13] UpdatePosition: 78.2491s, 464198 calls @ 78249120us\n",
      "\n",
      "[10:11:13] ======== Monitor: ellpack_page ========\n",
      "[10:11:13] BinningCompression: 0.004642s, 1 calls @ 4642us\n",
      "\n",
      "[10:11:13] InitCompressedData: 0.000472s, 1 calls @ 472us\n",
      "\n",
      "[10:11:13] Quantiles: 0.024513s, 1 calls @ 24513us\n",
      "\n",
      "Wall time: 52min 6s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3,\n",
       "             estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                     colsample_bylevel=0.8,\n",
       "                                     colsample_bynode=None,\n",
       "                                     colsample_bytree=0.8, gamma=None,\n",
       "                                     gpu_id=None, importance_type='gain',\n",
       "                                     interaction_constraints=None,\n",
       "                                     learning_rate=None, max_delta_step=None,\n",
       "                                     max_depth=None, min_child_weight=2,\n",
       "                                     missing=nan, monotone_constraints=None,\n",
       "                                     n_estimators=100, n_jobs=-1, num_class=3,\n",
       "                                     num_parallel_tree=None,\n",
       "                                     objective='multiclass', random_state=None,\n",
       "                                     reg_alpha=None, reg_lambda=None,\n",
       "                                     scale_pos_weight=None, subsample=0.8,\n",
       "                                     tree_method='gpu_hist',\n",
       "                                     validate_parameters=None, verbosity=None),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'learning_rate': [0.02, 0.01, 0.006],\n",
       "                         'max_depth': [30]},\n",
       "             verbose=300)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "params = {'learning_rate':[0.02, 0.01, 0.006],\n",
    "          'max_depth': [30] # 튜닝할 파라미터 삽입\n",
    "            }\n",
    "xgb_clf = XGBClassifier(n_estimators=100, min_child_weight=2, \n",
    "                        colsample_bytree=0.8, colsample_bylevel=0.8, subsample=0.8, tree_method='gpu_hist',\n",
    "                        num_class=3, objective='multiclass', n_jobs=-1)\n",
    "\n",
    "grid_cv = GridSearchCV(xgb_clf, param_grid=params, cv=3, n_jobs=-1, verbose=300)\n",
    "grid_cv.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.715538</td>\n",
       "      <td>{'learning_rate': 0.02, 'max_depth': 30}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.716824</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': 30}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>{'learning_rate': 0.006, 'max_depth': 30}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_test_score                                     params\n",
       "0         0.715538   {'learning_rate': 0.02, 'max_depth': 30}\n",
       "1         0.716824   {'learning_rate': 0.01, 'max_depth': 30}\n",
       "2              NaN  {'learning_rate': 0.006, 'max_depth': 30}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hr_grid_df = pd.DataFrame(grid_cv.cv_results_)\n",
    "hr_grid_df.loc[:, ['mean_test_score', \"params\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:  8.4min\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of  12 | elapsed:  8.4min remaining: 42.0min\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  12 | elapsed:  8.5min remaining: 25.4min\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of  12 | elapsed:  8.7min remaining: 17.5min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  12 | elapsed:  8.8min remaining: 12.3min\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of  12 | elapsed:  8.8min remaining:  8.8min\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  12 | elapsed: 10.0min remaining:  7.1min\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of  12 | elapsed: 10.0min remaining:  5.0min\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of  12 | elapsed: 10.0min remaining:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  12 | elapsed: 10.1min remaining:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed: 10.2min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed: 10.2min finished\n",
      "[09:14:57] DEBUG: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/tree/updater_gpu_hist.cu:899: [GPU Hist]: Configure\n",
      "[09:14:57] ======== Monitor: SketchContainer ========\n",
      "[09:14:57] MakeCuts: 0.001582s, 1 calls @ 1582us\n",
      "\n",
      "[09:14:57] Prune: 0.000937s, 1 calls @ 937us\n",
      "\n",
      "[09:14:57] ScanInput: 0.000853s, 1 calls @ 853us\n",
      "\n",
      "[09:14:57] Unique: 0.00057s, 1 calls @ 570us\n",
      "\n",
      "[09:14:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[09:16:00] ======== Monitor: Learner ========\n",
      "[09:16:00] Configure: 0.000914s, 1 calls @ 914us\n",
      "\n",
      "[09:16:00] EvalOneIter: 0.001043s, 100 calls @ 1043us\n",
      "\n",
      "[09:16:00] GetGradient: 0.007837s, 100 calls @ 7837us\n",
      "\n",
      "[09:16:00] PredictRaw: 0.175531s, 100 calls @ 175531us\n",
      "\n",
      "[09:16:00] UpdateOneIter: 59.3822s, 100 calls @ 59382235us\n",
      "\n",
      "[09:16:00] ======== Monitor: GBTree ========\n",
      "[09:16:00] BoostNewTrees: 59.1958s, 100 calls @ 59195806us\n",
      "\n",
      "[09:16:00] CommitModel: 6.2e-05s, 100 calls @ 62us\n",
      "\n",
      "[09:16:00] ======== Device 0 Memory Allocations:  ========\n",
      "[09:16:00] Peak memory usage: 377MiB\n",
      "[09:16:00] Number of allocations: 2188605\n",
      "[09:16:00] ======== Monitor: updater_gpu_hist ========\n",
      "[09:16:00] InitData: 0.024002s, 300 calls @ 24002us\n",
      "\n",
      "[09:16:00] InitDataOnce: 0.023956s, 1 calls @ 23956us\n",
      "\n",
      "[09:16:00] Update: 59.1008s, 300 calls @ 59100804us\n",
      "\n",
      "[09:16:00] ======== Monitor: gradient_based_sampler ========\n",
      "[09:16:00] Sample: 0.016215s, 300 calls @ 16215us\n",
      "\n",
      "[09:16:00] ======== Monitor: GPUHistMakerDevice0 ========\n",
      "[09:16:00] AllReduce: 0.06441s, 267472 calls @ 64410us\n",
      "\n",
      "[09:16:00] BuildHist: 3.10114s, 267172 calls @ 3101145us\n",
      "\n",
      "[09:16:00] EvaluateSplits: 6.31328s, 267172 calls @ 6313282us\n",
      "\n",
      "[09:16:00] FinalisePosition: 0.022651s, 300 calls @ 22651us\n",
      "\n",
      "[09:16:00] InitRoot: 0.569734s, 300 calls @ 569734us\n",
      "\n",
      "[09:16:00] Reset: 0.367396s, 300 calls @ 367396us\n",
      "\n",
      "[09:16:00] UpdatePosition: 47.4946s, 267172 calls @ 47494591us\n",
      "\n",
      "[09:16:00] ======== Monitor: ellpack_page ========\n",
      "[09:16:00] BinningCompression: 0.004696s, 1 calls @ 4696us\n",
      "\n",
      "[09:16:00] InitCompressedData: 0.000498s, 1 calls @ 498us\n",
      "\n",
      "[09:16:00] Quantiles: 0.017643s, 1 calls @ 17643us\n",
      "\n",
      "Wall time: 11min 12s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3,\n",
       "             estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                     colsample_bylevel=0.8,\n",
       "                                     colsample_bynode=None,\n",
       "                                     colsample_bytree=0.8, gamma=None,\n",
       "                                     gpu_id=None, importance_type='gain',\n",
       "                                     interaction_constraints=None,\n",
       "                                     learning_rate=None, max_delta_step=None,\n",
       "                                     max_depth=None, min_child_weight=2,\n",
       "                                     missing=nan, monotone_constraints=None,\n",
       "                                     n_estimators=100, n_jobs=-1, num_class=3,\n",
       "                                     num_parallel_tree=None,\n",
       "                                     objective='multiclass', random_state=None,\n",
       "                                     reg_alpha=None, reg_lambda=None,\n",
       "                                     scale_pos_weight=None, subsample=0.8,\n",
       "                                     tree_method='gpu_hist',\n",
       "                                     validate_parameters=None, verbosity=None),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'learning_rate': [0.03, 0.01], 'max_depth': [18, 23]},\n",
       "             verbose=300)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "params = {'learning_rate':[0.03, 0.01],\n",
    "          'max_depth': [18, 23] # 튜닝할 파라미터 삽입\n",
    "            }\n",
    "xgb_clf = XGBClassifier(n_estimators=100, min_child_weight=2, \n",
    "                        colsample_bytree=0.8, colsample_bylevel=0.8, subsample=0.8, tree_method='gpu_hist',\n",
    "                        num_class=3, objective='multiclass', n_jobs=-1)\n",
    "\n",
    "grid_cv = GridSearchCV(xgb_clf, param_grid=params, cv=3, n_jobs=-1, verbose=300)\n",
    "grid_cv.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.716370</td>\n",
       "      <td>{'learning_rate': 0.03, 'max_depth': 18}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.714669</td>\n",
       "      <td>{'learning_rate': 0.03, 'max_depth': 23}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.714178</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': 18}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.715463</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': 23}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_test_score                                    params\n",
       "0         0.716370  {'learning_rate': 0.03, 'max_depth': 18}\n",
       "1         0.714669  {'learning_rate': 0.03, 'max_depth': 23}\n",
       "2         0.714178  {'learning_rate': 0.01, 'max_depth': 18}\n",
       "3         0.715463  {'learning_rate': 0.01, 'max_depth': 23}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hr_grid_df = pd.DataFrame(grid_cv.cv_results_)\n",
    "hr_grid_df.loc[:, ['mean_test_score', \"params\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 seeds x 5 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.09467\tvalid-mlogloss:1.09470\n",
      "[100]\ttrain-mlogloss:0.87353\tvalid-mlogloss:0.87954\n",
      "[200]\ttrain-mlogloss:0.80531\tvalid-mlogloss:0.82073\n",
      "[300]\ttrain-mlogloss:0.77498\tvalid-mlogloss:0.79960\n",
      "[400]\ttrain-mlogloss:0.75462\tvalid-mlogloss:0.78835\n",
      "[500]\ttrain-mlogloss:0.73858\tvalid-mlogloss:0.78039\n",
      "[600]\ttrain-mlogloss:0.72476\tvalid-mlogloss:0.77373\n",
      "[700]\ttrain-mlogloss:0.71219\tvalid-mlogloss:0.76847\n",
      "[800]\ttrain-mlogloss:0.70067\tvalid-mlogloss:0.76373\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-110-5d8e85c86eba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m                 }\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mxgbmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwatchlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;31m#joblib.dump(xgbmodel, f'./pred_pkl/XGB_{n+1}_fold_{seed}_seed_xgb.pkl')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0mBooster\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtrained\u001b[0m \u001b[0mbooster\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m     \"\"\"\n\u001b[0;32m--> 227\u001b[0;31m     bst = _train_internal(params, dtrain,\n\u001b[0m\u001b[1;32m    228\u001b[0m                           \u001b[0mnum_boost_round\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_boost_round\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                           \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks, evals_result, maximize, verbose_eval, early_stopping_rounds)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1280\u001b[0;31m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,\n\u001b[0m\u001b[1;32m   1281\u001b[0m                                                     \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1282\u001b[0m                                                     dtrain.handle))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lucky_seeds=[42, 2019, 91373]\n",
    "xgtest = xgb.DMatrix(test_x)\n",
    "for i, seed in enumerate(lucky_seeds):\n",
    "\n",
    "    kfold = KFold(n_splits=10, random_state = seed, shuffle = True)\n",
    "    cv = np.zeros((train.shape[0], 3))\n",
    "    pred_test = np.zeros((test_x.shape[0], 3), dtype=float)\n",
    "\n",
    "    for n, (train_idx, val_idx) in enumerate(kfold.split(train)):\n",
    "        \n",
    "        x_train, x_val = train_x.iloc[train_idx], train_x.iloc[val_idx]\n",
    "        y_train, y_val = train_y.iloc[train_idx], train_y.iloc[val_idx]\n",
    "        \n",
    "        dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "        dvalid = xgb.DMatrix(x_val, label=y_val)\n",
    "        watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n",
    "\n",
    "        param = {'objective':'multi:softprob', 'seed':seed, 'num_class': 3, 'eval_metric':'mlogloss', \n",
    "                 'eta': 0.01, 'min_child_weight': 3,\n",
    "                 'colsample_bytree': 0.3, 'colsample_bylevel': 0.6, 'subsample': 0.8\n",
    "                }\n",
    "\n",
    "        xgbmodel = xgb.train(param, dtrain, 100000, watchlist, early_stopping_rounds=30, verbose_eval=100)\n",
    "        #joblib.dump(xgbmodel, f'./pred_pkl/XGB_{n+1}_fold_{seed}_seed_xgb.pkl')\n",
    "\n",
    "        cv[val_idx, :] = xgbmodel.predict(dvalid)\n",
    "        pred_test += xgbmodel.predict(xgtest) / 10\n",
    "        \n",
    "    pred_dict['xgb'+str(i+1)] = cv\n",
    "    pred_test_dict['xgb'+str(i+1)] = pred_test\n",
    "    print('multi_logloss:', log_loss(true, cv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xgbmodels_path = os.listdir('./pred_pkl/')\n",
    "xgbmodels_list = [x for x in xgbmodels_path if x.endswith(\"xgb.pkl\")]\n",
    "assert len(xgbmodels_list) == 15\n",
    "xgb_preds = np.zeros((test_x.shape[0], 3))\n",
    "xgtest = xgb.DMatrix(test_X)\n",
    "\n",
    "for m in xgbmodels_list:\n",
    "    xgbmodel = joblib.load('./pred_pkl/'+m)\n",
    "    xgb_preds_proba = xgbmodel.predict_proba(xgtest)\n",
    "    xgb_preds += xgb_preds_proba/15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "params = {'max_depth': [55, 60, 65] # 튜닝할 파라미터 삽입\n",
    "            }\n",
    "\n",
    "rf_clf = RandomForestClassifier(random_state = 0, n_estimators = 1000, \n",
    "                                min_samples_leaf=2, min_samples_split=2,\n",
    "                                criterion='entropy', n_jobs = -1)\n",
    "grid_cv = GridSearchCV(rf_clf, param_grid = params, cv = 5, n_jobs = -1)\n",
    "grid_cv.fit(df_train, y)\n",
    "\n",
    "print('최적 하이퍼 파라미터: ', grid_cv.best_params_)\n",
    "print('최고 예측 정확도: {:.4f}'.format(grid_cv.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 seeds, 5 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lucky_seeds=[42,2019,91373]\n",
    "for i, seed in enumerate(lucky_seeds):\n",
    "\n",
    "    kfold = KFold(n_splits=10, random_state = seed, shuffle = True)\n",
    "    cv = np.zeros((train.shape[0], 3))\n",
    "    pred_test = np.zeros((test_x.shape[0], 3), dtype=float)\n",
    "    \n",
    "    for n, (train_idx, val_idx) in enumerate(kfold.split(train)):\n",
    "        \n",
    "        x_train, x_val = train_x.iloc[train_idx], train_x.iloc[val_idx]\n",
    "        y_train, y_val = train_y.iloc[train_idx], train_y.iloc[val_idx]\n",
    "\n",
    "        rfmodel = RandomForestClassifier(n_estimators=1000, criterion='entropy', max_depth=55,\n",
    "                                         min_samples_leaf=2, min_samples_split=2,\n",
    "                                         random_state=seed)\n",
    "        rfmodel.fit(x_train, y_train)\n",
    "        #joblib.dump(rfmodel, f'./pred_pkl/RF_{n+1}_fold_{seed}_seed_rf.pkl')\n",
    "        \n",
    "        cv[val_idx, :] = rfmodel.predict_proba(x_val)        \n",
    "        pred_test += rfmodel.predict_proba(test_x) / 10\n",
    "        \n",
    "    pred_dict['rf'+str(i+1)] = cv\n",
    "    pred_test_dict['rf'+str(i+1)] = pred_test\n",
    "    print('multi_logloss :', log_loss(true, cv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rfmodels_path = os.listdir('./pred_pkl/')\n",
    "rfmodels_list = [x for x in rfmodels_path if x.endswith(\"rf.pkl\")]\n",
    "assert len(rfmodels_list) == 15\n",
    "rf_preds = np.zeros((test_x.shape[0], 3))\n",
    "\n",
    "for m in rfmodels_list:\n",
    "    rfmodel = joblib.load('./pred_pkl/'+m)\n",
    "    rf_preds_proba = rfmodel.predict_proba(test_x)\n",
    "    rf_preds += rf_preds_proba/15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) Catboost (성능X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lucky_seeds=[42,2019,91373]\n",
    "for i, seed in enumerate(lucky_seeds):\n",
    "\n",
    "    kfold = KFold(n_splits=5, random_state = seed, shuffle = True)\n",
    "    cv = np.zeros((train.shape[0], 3))\n",
    "    pred_test = np.zeros((test_x.shape[0], 3), dtype=float)\n",
    "    \n",
    "    for n, (train_idx, val_idx) in enumerate(kfold.split(train)):\n",
    "        \n",
    "        x_train, x_val = train_x.iloc[train_idx], train_x.iloc[val_idx]\n",
    "        y_train, y_val = train_y.iloc[train_idx], train_y.iloc[val_idx]\n",
    "        _train = Pool(x_train, label=y_train)\n",
    "        _valid = Pool(x_val, label=y_val)\n",
    "\n",
    "        catmodel =  CatBoostClassifier(loss_function='MultiClass', early_stopping_rounds=50, \n",
    "                                       random_state=seed, learning_rate=0.02, iterations=100000\n",
    "                                       #task_type=\"GPU\"\n",
    "                                      )\n",
    "        \n",
    "        catmodel.fit(_train, eval_set=_valid, use_best_model=True, verbose=2000)\n",
    "        #joblib.dump(rfmodel, f'./pred_pkl/RF_{n+1}_fold_{seed}_seed_rf.pkl')\n",
    "        \n",
    "        cv[val_idx, :] = catmodel.predict_proba(x_val)        \n",
    "        pred_test += catmodel.predict_proba(test_x) / 5\n",
    "        \n",
    "    pred_dict['cat'+str(i+1)] = cv\n",
    "    pred_test_dict['cat'+str(i+1)] = pred_test\n",
    "    print('multi_logloss :', log_loss(true, cv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) Stacking (AutoLGB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 27features = 3seed(42, 2019, 91373) x 3model(lgb, xgb, rf) x 3class(0, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(np.hstack([x for _, x in pred_dict.items()]))\n",
    "X_test = pd.DataFrame(np.hstack([x for _, x in pred_test_dict.items()]))\n",
    "\n",
    "pred = np.zeros((X_train.shape[0], 3), dtype=float)\n",
    "pred_test = np.zeros((X_test.shape[0], 3), dtype=float)\n",
    "#kfold = KFold(n_splits=5, random_state = seed, shuffle = True)\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for i_cv, (i_trn, i_val) in enumerate(cv.split(X_train, train_y)):\n",
    "    if i_cv == 0:\n",
    "        clf = AutoLGB(objective='multiclass', metric='multi_logloss', params={'num_class': 3}, \n",
    "                      feature_selection=False, n_est=10000)\n",
    "        clf.tune(X_train.iloc[i_trn], train_y[i_trn])\n",
    "        n_best = clf.n_best\n",
    "        features = clf.features\n",
    "        params = clf.params\n",
    "        print(f'best iteration: {n_best}')\n",
    "        print(f'selected features ({len(features)}): {features}')        \n",
    "        print(params)\n",
    "        clf.fit(X_train.iloc[i_trn], train_y[i_trn])\n",
    "    else:\n",
    "        train_data = lgb.Dataset(X_train[features].iloc[i_trn], label=train_y[i_trn])\n",
    "        clf = lgb.train(params, train_data, n_best, verbose_eval=100)\n",
    "    \n",
    "    pred[i_val] = clf.predict(X_train[features].iloc[i_val])\n",
    "    pred_test += clf.predict(X_test[features]) / 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'CV Log Loss: {log_loss(train_y, pred):.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 결과 제출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = sample_submission.copy()\n",
    "submission.iloc[:, 1:] = pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission/submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
