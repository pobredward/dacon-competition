{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Module Import & Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import joblib\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from itertools import combinations\n",
    "from eli5.permutation_importance import get_score_importances\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "from sklearn import cluster\n",
    "\n",
    "import optuna\n",
    "from optuna import Trial\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.visualization import plot_contour, plot_optimization_history\n",
    "from optuna.visualization import plot_parallel_coordinate, plot_slice, plot_param_importances\n",
    "from hyperopt.pyll.base import scope\n",
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "from catboost import Pool, CatBoostClassifier\n",
    "from kaggler.model import AutoLGB\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, GridSearchCV, StratifiedShuffleSplit, train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, log_loss\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv').drop(['index', 'FLAG_MOBIL'], axis=1).fillna('NAN')\n",
    "test = pd.read_csv('data/test.csv').drop(['index', 'FLAG_MOBIL'], axis=1).fillna('NAN')\n",
    "sample_submission = pd.read_csv('data/sample_submission.csv')\n",
    "\n",
    "# train데이터와 test데이터 변수를 함께 조정하기 위해 병합\n",
    "merge_data = pd.concat([train, test], axis = 0)\n",
    "\n",
    "# DAYS_BIRTH\n",
    "merge_data['DAYS_BIRTH_month']=np.floor((-merge_data['DAYS_BIRTH'])/30)-(\n",
    "    (np.floor((-merge_data['DAYS_BIRTH'])/30)/12).astype(int)*12)\n",
    "merge_data['DAYS_BIRTH_week']=np.floor((-merge_data['DAYS_BIRTH'])/7)-(\n",
    "    (np.floor((-merge_data['DAYS_BIRTH'])/7)/4).astype(int)*4)\n",
    "\n",
    "# DAYS_EMPLOYED\n",
    "merge_data['DAYS_EMPLOYED_month']=np.floor((-merge_data['DAYS_EMPLOYED'])/30)-(\n",
    "    (np.floor((-merge_data['DAYS_EMPLOYED'])/30)/12).astype(int)*12)\n",
    "merge_data['DAYS_EMPLOYED_week']=np.floor((-merge_data['DAYS_EMPLOYED'])/7)-(\n",
    "    (np.floor((-merge_data['DAYS_EMPLOYED'])/7)/4).astype(int)*4)\n",
    "\n",
    "# before_EMPLOYED\n",
    "merge_data['before_EMPLOYED']=merge_data['DAYS_BIRTH']-merge_data['DAYS_EMPLOYED']\n",
    "merge_data['before_EMPLOYED_month']=np.floor((-merge_data['before_EMPLOYED'])/30)-(\n",
    "    (np.floor((-merge_data['before_EMPLOYED'])/30)/12).astype(int)*12)\n",
    "merge_data['before_EMPLOYED_week']=np.floor((-merge_data['before_EMPLOYED'])/7)-(\n",
    "    (np.floor((-merge_data['before_EMPLOYED'])/7)/4).astype(int)*4)\n",
    "\n",
    "# DAYS_BIRTH / Income\n",
    "merge_data['DAYS_BIRTH_month/income_total'] = merge_data['DAYS_BIRTH_month'] / merge_data['income_total']\n",
    "merge_data['DAYS_BIRTH_week/income_total'] = merge_data['DAYS_BIRTH_week'] / merge_data['income_total']\n",
    "\n",
    "# DAYS_EMPLOYED / Income\n",
    "merge_data['DAYS_EMPLOYED_month/income_total'] = merge_data['DAYS_EMPLOYED_month'] / merge_data['income_total']\n",
    "merge_data['DAYS_EMPLOYED_week/income_total'] = merge_data['DAYS_EMPLOYED_week'] / merge_data['income_total']\n",
    "\n",
    "# before_EMPLOYED / Income\n",
    "merge_data['before_EMPLOYED/income_total'] = merge_data['before_EMPLOYED'] / merge_data['income_total']\n",
    "merge_data['before_EMPLOYED_month/income_total'] = merge_data['before_EMPLOYED_month'] / merge_data['income_total']\n",
    "merge_data['before_EMPLOYED_week/income_total'] = merge_data['before_EMPLOYED_week'] / merge_data['income_total']\n",
    "\n",
    "# Income / Family\n",
    "merge_data['income_total/family_size'] = merge_data['income_total'] / merge_data['family_size']\n",
    "\n",
    "merge_data['child_num/income_total'] = merge_data['child_num'] / merge_data['income_total']\n",
    "merge_data['family_size/income_total'] = merge_data['family_size'] / merge_data['income_total']\n",
    "merge_data['DAYS_BIRTH/income_total'] = merge_data['DAYS_BIRTH'] / merge_data['income_total']\n",
    "merge_data['DAYS_EMPLOYED/income_total'] = merge_data['DAYS_EMPLOYED'] / merge_data['income_total']\n",
    "merge_data['DAYS_EMPLOYED/DAYS_BIRTH'] =  merge_data['DAYS_EMPLOYED'] / merge_data['DAYS_BIRTH']\n",
    "\n",
    "# Income skewed-data\n",
    "merge_data['income_total'] = np.log1p(merge_data['income_total'])\n",
    "# merge_data['log_income_total'] = np.log(merge_data['income_total'])\n",
    "# merge_data['sqrt_income_total'] = np.sqrt(merge_data['income_total'])\n",
    "# merge_data['boxcox_income_total'] = stats.boxcox(merge_data['income_total'])[0]\n",
    "\n",
    "merge_data = merge_data.fillna(-999)\n",
    "train = merge_data[merge_data['credit'] != -999]\n",
    "test = merge_data[merge_data['credit'] == -999]\n",
    "test.drop('credit', axis = 1, inplace = True)\n",
    "\n",
    "train_cols = list(train.columns); train_cols.remove('credit'); train_cols.append('credit')\n",
    "train = train[train_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dict = {}\n",
    "pred_test_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lab = train.copy()\n",
    "test_lab = test.copy()\n",
    "\n",
    "enc = LabelEncoder()\n",
    "for col in train_lab.columns:\n",
    "    if train_lab[col].dtypes=='object':\n",
    "        train_lab[col] = enc.fit_transform(train_lab[col])\n",
    "        test_lab[col] = enc.fit_transform(test_lab[col])\n",
    "\n",
    "train_x = train_lab.drop(['credit'], axis=1) # 데이터 나누기\n",
    "train_y = train_lab['credit']\n",
    "test_x = test_lab.copy()\n",
    "\n",
    "print('Label Encoding Completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(data):\n",
    "    numerics = ['int8', 'int16', 'int32', 'int64', 'float32', 'float64']\n",
    "    start_memory = data.memory_usage().sum() / 1024**2    \n",
    "    for col in data.columns:\n",
    "        col_type = data[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = data[col].min()\n",
    "            c_max = data[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    data[col] = data[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    data[col] = data[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    data[col] = data[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    data[col] = data[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    data[col] = data[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    data[col] = data[col].astype(np.float32)\n",
    "                else:\n",
    "                    data[col] = data[col].astype(np.float64)    \n",
    "    end_memory = data.memory_usage().sum() / 1024**2\n",
    "    print('Memory optimization from {:5.2f}MB to {:5.2f}MB ({:.1f}% reduction)'\n",
    "          .format(start_memory, end_memory, 100 * (start_memory - end_memory) / start_memory))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = reduce_mem_usage(train_x)\n",
    "test_x = reduce_mem_usage(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.6874110257206558\n",
    "lgb_best_hyperparams = {'learning_rate': 0.00584665661176, 'reg_alpha': 0.9931264066149119, 'reg_lambda': 0.9808397348116461, \n",
    "                        'max_depth': 11, 'num_leaves': 1039, 'colsample_bytree': 0.3684233026512157, 'subsample': 0.7760609974958406, \n",
    "                        'subsample_freq': 12, 'min_child_samples': 3, 'min_child_weight': 1.0687770422304368, 'max_bin': 378}\n",
    "lgb_base_hyperparams = {'objective':'multiclass', 'n_estimators':10000,\n",
    "                        'lambda_l1':lgb_best_hyperparams['reg_alpha'],\n",
    "                        'lambda_l2':lgb_best_hyperparams['reg_lambda'],\n",
    "                        'reg_alpha':None, 'reg_lambda':None}\n",
    "lgb_best_hyperparams.update(lgb_base_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lucky_seeds=[2283, 8217, 91373] # Lucky seed 늘려가면서 하기\n",
    "for i, seed in enumerate(lucky_seeds):\n",
    "\n",
    "    kfold = StratifiedKFold(n_splits=10, random_state = seed, shuffle = True) # CV 늘려가면서 하기\n",
    "    cv=np.zeros((train_x.shape[0], 3))\n",
    "    pred_test = np.zeros((test_x.shape[0], 3), dtype=float)\n",
    "    \n",
    "    for n, (train_idx, val_idx) in enumerate(kfold.split(train_x, train_y)):\n",
    "\n",
    "        x_train, x_val = train_x.iloc[train_idx], train_x.iloc[val_idx]\n",
    "        y_train, y_val = train_y.iloc[train_idx], train_y.iloc[val_idx]\n",
    "\n",
    "        lgbmodel = LGBMClassifier(**lgb_best_hyperparams)\n",
    "\n",
    "                                                                                    # 진행상황 보고싶을때 None을 100으로\n",
    "        lgbmodel.fit(x_train, y_train, eval_set=[(x_val, y_val)], early_stopping_rounds=30, verbose=None) \n",
    "        \n",
    "        cv[val_idx,:] = lgbmodel.predict_proba(x_val)\n",
    "        #print(f'fold{n+1} multi_logloss: {log_loss(y_val, cv[val_idx, :])}') # Fold마다 점수 체크하려면 주석 해제\n",
    "        pred_test += lgbmodel.predict_proba(test_x) / 10 # CV 바꾸면 이 숫자도 똑같이 바꿔야함\n",
    "    pred_dict['lgb'+str(seed)] = cv\n",
    "    pred_test_dict['lgb'+str(seed)] = pred_test\n",
    "    print(seed, 'multi_logloss :', log_loss(train_y, cv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.6821672016092287\n",
    "xgb_best_hyperparams = {'learning_rate': 0.004219566178881841, 'reg_alpha': 0.017314214531008332, 'reg_lambda': 0.7804799483256929, \n",
    "                       'max_depth': 16, 'colsample_bytree': 0.464918668234781, 'colsample_bylevel': 0.2112468031800087, \n",
    "                       'subsample': 0.9035127015017239, 'gamma': 0.7793451203919987, 'min_child_weight': 2.458581150016787, \n",
    "                       'max_bin': 309}\n",
    "xgb_base_hyperparams = {'objective':'multi:softprob', \"num_class\": 3, \"eval_metric\": \"mlogloss\", \"random_state\": 91373}\n",
    "xgb_best_hyperparams.update(xgb_base_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lucky_seeds=[2283, 8217, 91373]\n",
    "xgtest = xgb.DMatrix(test_x)\n",
    "for i, seed in enumerate(lucky_seeds):\n",
    "\n",
    "    kfold = StratifiedKFold(n_splits=10, random_state = seed, shuffle = True) # 늘려가면서\n",
    "    cv = np.zeros((train.shape[0], 3))\n",
    "    pred_test = np.zeros((test_x.shape[0], 3), dtype=float)\n",
    "\n",
    "    for n, (train_idx, val_idx) in enumerate(kfold.split(train_x, train_y)):\n",
    "        \n",
    "        x_train, x_val = train_x.iloc[train_idx], train_x.iloc[val_idx]\n",
    "        y_train, y_val = train_y.iloc[train_idx], train_y.iloc[val_idx]\n",
    "        \n",
    "        dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "        dvalid = xgb.DMatrix(x_val, label=y_val)\n",
    "        watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n",
    "        \n",
    "                                                                                            # 진행상황 보고싶을때 None을 100으로\n",
    "        xgbmodel = xgb.train(xgb_best_hyperparams, dtrain, 100000, watchlist, early_stopping_rounds=30, verbose_eval=None)\n",
    "\n",
    "        cv[val_idx, :] = xgbmodel.predict(dvalid)\n",
    "        #print(f'fold{n+1} multi_logloss: {log_loss(y_val, cv[val_idx, :])}') # Fold마다 점수 체크하려면 주석 해제\n",
    "        pred_test += xgbmodel.predict(xgtest) / 10 # CV 바꾸면 이 숫자도 똑같이 바꿔야함\n",
    "        \n",
    "    pred_dict['xgb'+str(seed)] = cv\n",
    "    pred_test_dict['xgb'+str(seed)] = pred_test\n",
    "    print(seed, 'multi_logloss :', log_loss(train_y, cv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.6888343594936606\n",
    "rf_best_hyperparams = {'n_estimators': 776, 'max_depth': 95, 'max_features': 0.18938529117093866, \n",
    "                      'min_samples_split': 12, 'max_samples': 0.9089712797972337}\n",
    "rf_base_hyperparams = {'random_state': 91373, 'n_jobs': -1}\n",
    "rf_best_hyperparams.update(rf_base_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 seeds, 10 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lucky_seeds=[2283, 8217, 91373]\n",
    "for i, seed in enumerate(lucky_seeds):\n",
    "\n",
    "    kfold = StratifiedKFold(n_splits=10, random_state = seed, shuffle = True) # 늘려가면서\n",
    "    cv = np.zeros((train_x.shape[0], 3))\n",
    "    pred_test = np.zeros((test_x.shape[0], 3), dtype=float)\n",
    "    \n",
    "    for n, (train_idx, val_idx) in enumerate(kfold.split(train_x, train_y)):\n",
    "        \n",
    "        x_train, x_val = train_x.iloc[train_idx], train_x.iloc[val_idx]\n",
    "        y_train, y_val = train_y.iloc[train_idx], train_y.iloc[val_idx]\n",
    "\n",
    "        rfmodel = RandomForestClassifier(**rf_best_hyperparams)\n",
    "        rfmodel.fit(x_train, y_train)\n",
    "     \n",
    "        cv[val_idx, :] = rfmodel.predict_proba(x_val)      \n",
    "        #print(f'fold{n+1} multi_logloss: {log_loss(y_val, cv[val_idx, :])}')\n",
    "        pred_test += rfmodel.predict_proba(test_x) / 10 # CV 바꾸면 이 숫자도 똑같이 바꿔야함\n",
    "        \n",
    "    pred_dict['rf'+str(seed)] = cv\n",
    "    pred_test_dict['rf'+str(seed)] = pred_test\n",
    "    print(seed, 'multi_logloss :', log_loss(train_y, cv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) Stacking (AutoLGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_dict(model, pred_dict, pred_test_dict):\n",
    "    pred_dict_local = {}\n",
    "    for key, value in pred_dict.items():\n",
    "        if model in key:\n",
    "            pred_dict_local[key]=value\n",
    "\n",
    "    pred_test_dict_local = {}\n",
    "    for key, value in pred_test_dict.items():\n",
    "        if model in key:\n",
    "            pred_test_dict_local[key]=value\n",
    "\n",
    "    pred_dict_new_local = dict(sorted(pred_dict_local.items(), key=lambda x:log_loss(train_y, list(x[1])), reverse=False)[:3])\n",
    "    pred_test_dict_new_local = {}\n",
    "    for key, value in pred_dict_new_local.items():\n",
    "        pred_test_dict_new_local[key]=pred_test_dict_local[key]\n",
    "        \n",
    "    return pred_dict_new_local, pred_test_dict_new_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dict_lgb, pred_test_dict_lgb = sort_dict('lgb', pred_dict, pred_test_dict)\n",
    "pred_dict_xgb, pred_test_dict_xgb = sort_dict('xgb', pred_dict, pred_test_dict)\n",
    "pred_dict_rf, pred_test_dict_rf = sort_dict('rf', pred_dict, pred_test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dict(model, pred_dict, pred_test_dict):\n",
    "    with open('./pkl/pred_dict_'+model+'.pickle', 'wb') as fw:\n",
    "        pickle.dump(pred_dict, fw)\n",
    "\n",
    "    with open('./pkl/pred_test_dict_'+model+'.pickle', 'wb') as fw:\n",
    "        pickle.dump(pred_test_dict, fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_dict('lgb', pred_dict_lgb, pred_test_dict_lgb)\n",
    "# save_dict('xgb', pred_dict_xgb, pred_test_dict_xgb)\n",
    "# save_dict('rf', pred_dict_rf, pred_test_dict_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dict(model):\n",
    "    with open('./pkl/pred_dict_'+model+'.pickle', 'rb') as fw:\n",
    "        pred_dict_new_local = pickle.load(fw)\n",
    "\n",
    "    with open('./pkl/pred_test_dict_'+model+'.pickle', 'rb') as fw:\n",
    "        pred_test_dict_new_local = pickle.load(fw)\n",
    "        \n",
    "    return pred_dict_new_local, pred_test_dict_new_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_dict_lgb, pred_test_dict_lgb = load_dict('lgb')\n",
    "# pred_dict_xgb, pred_test_dict_xgb = load_dict('xgb')\n",
    "# pred_dict_rf, pred_test_dict_rf = load_dict('rf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dict_total = {**pred_dict_lgb, **pred_dict_xgb, **pred_dict_rf}\n",
    "pred_test_dict_total = {**pred_test_dict_lgb, **pred_test_dict_xgb, **pred_test_dict_rf}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(np.hstack([x for _, x in pred_dict_total.items()]))\n",
    "X_test = pd.DataFrame(np.hstack([x for _, x in pred_test_dict_total.items()]))\n",
    "\n",
    "pred = np.zeros((X_train.shape[0], 3), dtype=float)\n",
    "pred_test = np.zeros((X_test.shape[0], 3), dtype=float)\n",
    "#kfold = KFold(n_splits=5, random_state = seed, shuffle = True)\n",
    "cv = StratifiedKFold(n_splits=12, shuffle=True, random_state=42)\n",
    "\n",
    "for i_cv, (i_trn, i_val) in enumerate(cv.split(X_train, train_y)):\n",
    "    if i_cv == 0:\n",
    "        clf = AutoLGB(objective='multiclass', metric='multi_logloss', params={'num_class': 3}, \n",
    "                      feature_selection=False, n_est=10000)\n",
    "        clf.tune(X_train.iloc[i_trn], train_y[i_trn])\n",
    "        n_best = clf.n_best\n",
    "        features = clf.features\n",
    "        params = clf.params\n",
    "        print(f'best iteration: {n_best}')\n",
    "        print(f'selected features ({len(features)}): {features}')        \n",
    "        print(params)\n",
    "        clf.fit(X_train.iloc[i_trn], train_y[i_trn])\n",
    "    else:\n",
    "        train_data = lgb.Dataset(X_train[features].iloc[i_trn], label=train_y[i_trn])\n",
    "        clf = lgb.train(params, train_data, n_best, verbose_eval=100)\n",
    "    \n",
    "    pred[i_val] = clf.predict(X_train[features].iloc[i_val])\n",
    "    pred_test += clf.predict(X_test[features]) / 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'CV Log Loss: {log_loss(train_y, pred):.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack1_train = pred.copy()\n",
    "stack1_test = pred_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking (XGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_objective(trial: Trial) -> float:\n",
    "    params_xgb = {\n",
    "        \"random_state\": 91373,\n",
    "        \"verbose\": None,\n",
    "        \"num_class\": 3,\n",
    "        \"objective\": \"multi:softprob\",\n",
    "        \"eval_metric\": \"mlogloss\",\n",
    "        #\"tree_method\": \"gpu_hist\",\n",
    "        \"learning_rate\": trial.suggest_uniform(\"learning_rate\", 0.005, 0.01),\n",
    "        #\"reg_alpha\": trial.suggest_uniform(\"reg_alpha\", 0.1, 1.0),\n",
    "        #\"reg_lambda\": trial.suggest_uniform(\"reg_lambda\", 0.1, 1.0),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 5, 10),\n",
    "        \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0.3, 1.0),\n",
    "        \"colsample_bylevel\": trial.suggest_uniform(\"colsample_bylevel\", 0.3, 1.0),\n",
    "        \"subsample\": trial.suggest_uniform(\"subsample\", 0.3, 1.0),\n",
    "        \"gamma\": trial.suggest_uniform(\"gamma\", 0.3, 1.0),\n",
    "        #\"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 200, 500),\n",
    "    }\n",
    "    \n",
    "    # CV=10으로 튜닝\n",
    "    \n",
    "    seed = 91373\n",
    "    kfold = StratifiedKFold(n_splits=5, random_state = seed, shuffle = True) # Cross-validation cv=5\n",
    "    cv = np.zeros((train_x.shape[0], 3))\n",
    "\n",
    "    for n, (train_idx, val_idx) in enumerate(kfold.split(train_x, train_y)):\n",
    "\n",
    "        x_train, x_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_train, y_val = train_y.iloc[train_idx], train_y.iloc[val_idx]\n",
    "        \n",
    "        dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "        dvalid = xgb.DMatrix(x_val, label=y_val)\n",
    "        watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n",
    "                                                                                            # 진행상황 보고싶을때 None을 100으로\n",
    "        stack_xgbmodel = xgb.train(params_xgb, dtrain, 100000, watchlist, early_stopping_rounds=30, verbose_eval=None)\n",
    "\n",
    "        cv[val_idx, :] = stack_xgbmodel.predict(dvalid)\n",
    "        #print(f'fold{n+1} multi_logloss: {log_loss(y_val, cv[val_idx, :])}') # Fold마다 점수 체크하려면 주석 해제\n",
    "    print('multi_logloss:', log_loss(train_y, cv))\n",
    "\n",
    "    return log_loss(train_y, cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(np.hstack([x for _, x in pred_dict_total.items()]))\n",
    "X_test = pd.DataFrame(np.hstack([x for _, x in pred_test_dict_total.items()]))\n",
    "\n",
    "sampler = TPESampler(seed=42)\n",
    "stack_study = optuna.create_study(study_name=\"stack_parameter_opt\", direction=\"minimize\", sampler=sampler)\n",
    "stack_study.optimize(stack_objective, n_trials=30)\n",
    "\n",
    "stack_best_hyperparams = stack_study.best_trial.params\n",
    "stack_base_hyperparams = {'objective':'multi:softprob', \"num_class\": 3, \"eval_metric\": \"mlogloss\", \n",
    "                         #\"tree_method\": \"gpu_hist\", \n",
    "                          \"random_state\": 91373}\n",
    "stack_best_hyperparams.update(stack_base_hyperparams)\n",
    "print(\"The best hyperparameters are:\\n\", stack_best_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_best_hyperparams = {'learning_rate': 0.005171942605576092, 'max_depth': 10, 'colsample_bytree': 0.48114598712001183, \n",
    "                          'colsample_bylevel': 0.7637655990477874, 'subsample': 0.5181977532625877, 'gamma': 0.6640476148244676, \n",
    "                          'max_bin': 364}\n",
    "stack_base_hyperparams = {'objective':'multi:softprob', \"num_class\": 3, \"eval_metric\": \"mlogloss\", \n",
    "                         #\"tree_method\": \"gpu_hist\", \n",
    "                          \"random_state\": 91373}\n",
    "stack_best_hyperparams.update(stack_base_hyperparams)\n",
    "print(\"The best hyperparameters are:\\n\", stack_best_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.zeros((X_train.shape[0], 3), dtype=float)\n",
    "pred_test = np.zeros((X_test.shape[0], 3), dtype=float)\n",
    "kfold = StratifiedKFold(n_splits=12, random_state = 91373, shuffle = True)\n",
    "\n",
    "for n, (train_idx, val_idx) in enumerate(kfold.split(X_train, train_y)):\n",
    "    x_train, x_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_train, y_val = train_y.iloc[train_idx], train_y.iloc[val_idx]\n",
    "\n",
    "    dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "    dvalid = xgb.DMatrix(x_val, label=y_val)\n",
    "    watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n",
    "                                                                                        # 진행상황 보고싶을때 None을 100으로\n",
    "    stack_xgbmodel = xgb.train(stack_best_hyperparams, dtrain, 100000, watchlist, early_stopping_rounds=30, verbose_eval=None)\n",
    "    \n",
    "    pred[val_idx] = stack_xgbmodel.predict(dvalid)\n",
    "    pred_test += stack_xgbmodel.predict(xgb.DMatrix(X_test)) / 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'CV Log Loss: {log_loss(train_y, pred):.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack2_train = pred.copy()\n",
    "stack2_test = pred_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_final = (sum(pred_dict_lgb.values())/3 * 0.1 +\n",
    "              sum(pred_dict_xgb.values())/3 * 0.2 +\n",
    "               sum(pred_dict_rf.values())/3 * 0.1 +\n",
    "                               stack1_train * 0.3 +\n",
    "                               stack2_train * 0.3)\n",
    "log_loss(train_y, pred_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_final = (sum(pred_test_dict_lgb.values())/3 * 0.1 +\n",
    "                   sum(pred_test_dict_xgb.values())/3 * 0.2 +\n",
    "                    sum(pred_test_dict_rf.values())/3 * 0.1 +\n",
    "                                          stack1_test * 0.3 +\n",
    "                                          stack2_test * 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 결과 제출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = sample_submission.copy()\n",
    "submission.iloc[:, 1:] = pred_test_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removed Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기본 모델로 성능 측정하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_lgbmodel(train, verbose=True):\n",
    "    \n",
    "    train_x = train.drop(['credit'], axis=1)\n",
    "    train_y = train['credit']\n",
    "    \n",
    "    np.random.seed(0)\n",
    "    lucky_seeds=np.random.randint(1, 10000, 5)\n",
    "    score_list = []\n",
    "    \n",
    "    for i, seed in enumerate(lucky_seeds):\n",
    "\n",
    "        kfold = StratifiedKFold(n_splits=5, random_state = seed, shuffle = True) # CV 늘려가면서 하기\n",
    "        cv=np.zeros((train_x.shape[0], 3))\n",
    "\n",
    "        for n, (train_idx, val_idx) in enumerate(kfold.split(train_x, train_y)):\n",
    "\n",
    "            x_train, x_val = train_x.iloc[train_idx], train_x.iloc[val_idx]\n",
    "            y_train, y_val = train_y.iloc[train_idx], train_y.iloc[val_idx]\n",
    "\n",
    "            lgbmodel = LGBMClassifier(objective='multiclass', n_estimators=10000, random_state=seed)\n",
    "            lgbmodel.fit(x_train, y_train, eval_set=[(x_val, y_val)], early_stopping_rounds=30, verbose=None) \n",
    "\n",
    "            cv[val_idx,:] = lgbmodel.predict_proba(x_val)\n",
    "        if verbose==True:\n",
    "            print(f'multi_logloss: {log_loss(train_y, cv):.4f}')\n",
    "        score_list.append(log_loss(train_y, cv))\n",
    "    print(f'Average Logloss: {np.mean(score_list):.4f}')\n",
    "    return np.mean(score_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 향상된 모델로 성능 측정하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_lgbmodel(train, verbose=True):\n",
    "    \n",
    "    train_x = train.drop(['credit'], axis=1)\n",
    "    train_y = train['credit']\n",
    "    \n",
    "    np.random.seed(0)\n",
    "    lucky_seeds=np.random.randint(1, 10000, 3)\n",
    "    score_list = []\n",
    "    \n",
    "    for i, seed in enumerate(lucky_seeds):\n",
    "\n",
    "        kfold = StratifiedKFold(n_splits=5, random_state = seed, shuffle = True) # CV 늘려가면서 하기\n",
    "        cv=np.zeros((train_x.shape[0], 3))\n",
    "\n",
    "        for n, (train_idx, val_idx) in enumerate(kfold.split(train_x, train_y)):\n",
    "\n",
    "            x_train, x_val = train_x.iloc[train_idx], train_x.iloc[val_idx]\n",
    "            y_train, y_val = train_y.iloc[train_idx], train_y.iloc[val_idx]\n",
    "\n",
    "            lgbmodel = LGBMClassifier(learning_rate=0.01, objective='multiclass', num_leaves=1000, max_depth=-1,\n",
    "                                      n_estimators=10000, random_state=seed)\n",
    "            lgbmodel.fit(x_train, y_train, eval_set=[(x_val, y_val)], early_stopping_rounds=30, verbose=None) \n",
    "\n",
    "            cv[val_idx,:] = lgbmodel.predict_proba(x_val)\n",
    "        if verbose==True:\n",
    "            print(f'multi_logloss: {log_loss(train_y, cv):.4f}')\n",
    "        score_list.append(log_loss(train_y, cv))\n",
    "    print(f'Average Logloss: {np.mean(score_list):.4f}')\n",
    "    return np.mean(score_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 원 핫 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_oh = train.copy()\n",
    "test_oh = test.copy()\n",
    "\n",
    "object_col = []\n",
    "for col in train_oh.columns:\n",
    "    if (train_oh[col].dtype == 'object'):\n",
    "        object_col.append(col)   \n",
    "enc = OneHotEncoder()\n",
    "enc.fit(train.loc[:,object_col])\n",
    "\n",
    "train_onehot_df = pd.DataFrame(enc.transform(train_oh.loc[:,object_col]).toarray(), \n",
    "             columns=enc.get_feature_names(object_col))\n",
    "train_oh.drop(object_col, axis=1, inplace=True)\n",
    "train_oh = pd.concat([train_oh, train_onehot_df], axis=1)    \n",
    "\n",
    "test_onehot_df = pd.DataFrame(enc.transform(test_oh.loc[:,object_col]).toarray(), \n",
    "             columns=enc.get_feature_names(object_col))\n",
    "test_oh.drop(object_col, axis=1, inplace=True)\n",
    "test_oh = pd.concat([test_oh, test_onehot_df], axis=1)\n",
    "\n",
    "print('One Hot Encoding Completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_lgbmodel(train_oh, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 라벨 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lab = train.copy()\n",
    "test_lab = test.copy()\n",
    "\n",
    "enc = LabelEncoder()\n",
    "for col in train_lab.columns:\n",
    "    if train_lab[col].dtypes=='object':\n",
    "        train_lab[col] = enc.fit_transform(train_lab[col])\n",
    "        test_lab[col] = enc.fit_transform(test_lab[col])\n",
    "\n",
    "    \n",
    "print('Label Encoding Completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_lgbmodel(train_lab, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 카테고리 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cat = train.copy()\n",
    "test_cat = test.copy()\n",
    "\n",
    "for col in train_cat.columns:\n",
    "    if train_cat[col].dtypes=='object':\n",
    "        train_cat[col] =  train_cat[col].astype('category')\n",
    "        test_cat[col] =  test_cat[col].astype('category')\n",
    "    \n",
    "print('Category Encoding Completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_lgbmodel(train_cat, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### income_total 범주화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Initial Logloss', end=' ')\n",
    "# base_lgbmodel(train_cat, verbose=False)\n",
    "# raw_income = train_cat.income_total.copy()\n",
    "# for cut in np.arange(1000, 5000, 500):\n",
    "#     print(f'cut space:{cut}', end=' ')\n",
    "#     cutted_income = pd.cut(raw_income, bins=np.arange(27000, 1575000, cut), labels=False)\n",
    "#     train_cat['income_total'] = cutted_income\n",
    "#     base_lgbmodel(train_cat, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN (income_total, income_type, occyp_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # income_type, occyp_type, income_total을 이용하여 클러스터링하기 위해 따로 원 핫 인코딩\n",
    "# kmeans_train = train[['occyp_type', 'income_type', 'income_total']]\n",
    "# kmeans_test = test[['occyp_type', 'income_type', 'income_total']]\n",
    "# object_col = []\n",
    "# for col in kmeans_train.columns:\n",
    "#     if kmeans_train[col].dtype == 'object':\n",
    "#         object_col.append(col)\n",
    "        \n",
    "# enc = OneHotEncoder()\n",
    "# enc.fit(kmeans_train.loc[:,object_col])\n",
    "\n",
    "# train_onehot_df = pd.DataFrame(enc.transform(kmeans_train.loc[:,object_col]).toarray(), \n",
    "#              columns=enc.get_feature_names(object_col))\n",
    "# kmeans_train.drop(object_col, axis=1, inplace=True)\n",
    "# kmeans_train = pd.concat([kmeans_train, train_onehot_df], axis=1)\n",
    "\n",
    "# test_onehot_df = pd.DataFrame(enc.transform(kmeans_test.loc[:,object_col]).toarray(),\n",
    "#              columns=enc.get_feature_names(object_col))\n",
    "# kmeans_test.drop(object_col, axis=1, inplace=True)\n",
    "# kmeans_test = pd.concat([kmeans_test, test_onehot_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_lgbmodel(train_cat, verbose=False)\n",
    "# # n_clusters를 3부터 10까지 진행하여 하나씩 성능 체크\n",
    "# score_list = {}\n",
    "# k_means_train_total_df = pd.DataFrame()\n",
    "# k_means_test_total_df = pd.DataFrame()\n",
    "# for i in tqdm(range(3, 12)):\n",
    "#     train_cat = train.copy()\n",
    "#     test_cat = test.copy()\n",
    "\n",
    "#     for col in train_cat.columns:\n",
    "#         if train_cat[col].dtypes=='object':\n",
    "#             train_cat[col] =  train_cat[col].astype('category')\n",
    "#             test_cat[col] =  test_cat[col].astype('category')\n",
    "\n",
    "#     # n_cluster를 늘려가며 클러스터링 진행\n",
    "#     k_means_train_df = pd.DataFrame()\n",
    "#     k_means_test_df = pd.DataFrame()\n",
    "#     k_means = cluster.KMeans(n_clusters=i)\n",
    "#     k_means.fit(kmeans_train)\n",
    "#     k_means_train_df = pd.concat([k_means_train_df, pd.DataFrame(k_means.labels_, columns=[f'cluster_{i}'])], axis=1)\n",
    "#     k_means_train_total_df = pd.concat([k_means_train_total_df, pd.DataFrame(k_means.labels_, columns=[f'cluster_{i}'])], axis=1)\n",
    "#     k_means.fit(kmeans_test)\n",
    "#     k_means_test_df = pd.concat([k_means_test_df, pd.DataFrame(k_means.labels_, columns=[f'cluster_{i}'])], axis=1)\n",
    "#     k_means_test_total_df = pd.concat([k_means_test_total_df, pd.DataFrame(k_means.labels_, columns=[f'cluster_{i}'])], axis=1)\n",
    "\n",
    "#     train_cat = pd.concat([train_cat, k_means_train_df], axis=1)\n",
    "#     test_cat = pd.concat([test_cat, k_means_test_df], axis=1)\n",
    "\n",
    "#     # 클러스터링 결과를 category 타입으로 변경\n",
    "#     for col in train_cat.columns:\n",
    "#         if train_cat[col].dtypes=='int32':\n",
    "#             train_cat[col] =  train_cat[col].astype('category')\n",
    "#             test_cat[col] =  test_cat[col].astype('category')\n",
    "#     print(f'cluster: {i}', end=' ')\n",
    "#     score_list[f'cluster_{i}'] = base_lgbmodel(train_cat, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# base_lgbmodel(train_cat, verbose=False)\n",
    "# # n_clusters를 3부터 10까지 진행하여 하나씩 성능 체크\n",
    "# score_list = {}\n",
    "# k_means_train_total_df = pd.DataFrame()\n",
    "# k_means_test_total_df = pd.DataFrame()\n",
    "# for i in tqdm(range(3, 12)):\n",
    "#     train_cat = train.copy()\n",
    "#     test_cat = test.copy()\n",
    "\n",
    "#     for col in train_cat.columns:\n",
    "#         if train_cat[col].dtypes=='object':\n",
    "#             train_cat[col] =  train_cat[col].astype('category')\n",
    "#             test_cat[col] =  test_cat[col].astype('category')\n",
    "\n",
    "#     # n_cluster를 늘려가며 클러스터링 진행\n",
    "#     k_means_train_df = pd.DataFrame()\n",
    "#     k_means_test_df = pd.DataFrame()\n",
    "#     k_means = cluster.KMeans(n_clusters=i)\n",
    "#     k_means.fit(kmeans_train)\n",
    "#     k_means_train_df = pd.concat([k_means_train_df, pd.DataFrame(k_means.labels_, columns=[f'cluster_{i}'])], axis=1)\n",
    "#     k_means_train_total_df = pd.concat([k_means_train_total_df, pd.DataFrame(k_means.labels_, columns=[f'cluster_{i}'])], axis=1)\n",
    "#     k_means.fit(kmeans_test)\n",
    "#     k_means_test_df = pd.concat([k_means_test_df, pd.DataFrame(k_means.labels_, columns=[f'cluster_{i}'])], axis=1)\n",
    "#     k_means_test_total_df = pd.concat([k_means_test_total_df, pd.DataFrame(k_means.labels_, columns=[f'cluster_{i}'])], axis=1)\n",
    "\n",
    "#     train_cat = pd.concat([train_cat, k_means_train_df], axis=1)\n",
    "#     test_cat = pd.concat([test_cat, k_means_test_df], axis=1)\n",
    "\n",
    "#     # 클러스터링 결과를 category 타입으로 변경\n",
    "#     for col in train_cat.columns:\n",
    "#         if train_cat[col].dtypes=='int32':\n",
    "#             train_cat[col] =  train_cat[col].astype('category')\n",
    "#             test_cat[col] =  test_cat[col].astype('category')\n",
    "#     print(f'cluster: {i}', end=' ')\n",
    "#     score_list[f'cluster_{i}'] = base_lgbmodel(train_cat, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # 성능이 좋아지는 클러스터링 개수로만 피처 추출\n",
    "\n",
    "# train_cat = train.copy()\n",
    "# test_cat = test.copy()\n",
    "\n",
    "# for col in train_cat.columns:\n",
    "#     if train_cat[col].dtypes=='object':\n",
    "#         train_cat[col] =  train_cat[col].astype('category')\n",
    "#         test_cat[col] =  test_cat[col].astype('category')\n",
    "        \n",
    "# n = 1 # 성능이 좋아지는 클러스터 개수별로 정렬한 후 앞의 n개만 추출\n",
    "# train_cat = pd.concat([train_cat, k_means_train_total_df.loc[:, sorted(score_list, key=lambda x: score_list[x])[:n]]], axis=1)\n",
    "# test_cat = pd.concat([test_cat, k_means_test_total_df.loc[:, sorted(score_list, key=lambda x: score_list[x])[:n]]], axis=1)\n",
    "\n",
    "# for col in train_cat.columns:\n",
    "#     if train_cat[col].dtypes=='int32':\n",
    "#         train_cat[col] =  train_cat[col].astype('category')\n",
    "#         test_cat[col] =  test_cat[col].astype('category')\n",
    "        \n",
    "# print(k_means_train_total_df.loc[:, sorted(score_list, key=lambda x: score_list[x])[:n]].columns.tolist())\n",
    "# print(base_lgbmodel(train_cat, verbose=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k_means_train_total_df.loc[:, sorted(score_list, key=lambda x: score_list[x])[:n]].to_csv(\n",
    "#     './save/k_means_train.csv', index=False)\n",
    "# k_means_test_total_df.loc[:, sorted(score_list, key=lambda x: score_list[x])[:n]].to_csv(\n",
    "#     './save/k_means_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_cat = pd.concat([train_cat, pd.read_csv('./save/k_means_train.csv')], axis=1)\n",
    "# test_cat = pd.concat([test_cat, pd.read_csv('./save/k_means_test.csv')], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permutation Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enc = LabelEncoder()\n",
    "# for col in train_cat.columns:\n",
    "#     if train_cat[col].dtype.name=='category':\n",
    "#         train_cat[col] = enc.fit_transform(train_cat[col])\n",
    "#         test_cat[col] = enc.fit_transform(test_cat[col])\n",
    "        \n",
    "# print('Label Encoding Completed')\n",
    "\n",
    "# train_x = train_cat.drop(['credit'], axis=1)\n",
    "# train_y = train_cat['credit']\n",
    "# test_x = test_cat.copy()\n",
    "\n",
    "# seeds = np.random.randint(0, 1000, 3)\n",
    "# perm_dicts = {}\n",
    "# cv = np.zeros((train_x.shape[0], 3))\n",
    "# for n, seed in enumerate(seeds):\n",
    "#     kfold = StratifiedKFold(n_splits=5, random_state=seed, shuffle=True)\n",
    "#     for i, (train_idx, val_idx) in enumerate(kfold.split(train_x, train_y)):\n",
    "\n",
    "#         x_train, x_val = train_x.iloc[train_idx], train_x.iloc[val_idx]\n",
    "#         y_train, y_val = train_y.iloc[train_idx], train_y.iloc[val_idx]\n",
    "\n",
    "#         lgbm = LGBMClassifier(n_estimators=10000, objective='multiclass', seed=0)\n",
    "#         lgbm.fit(x_train, y_train, eval_set=[(x_val, y_val)], early_stopping_rounds=30, verbose=None)\n",
    "#         cv[val_idx, :] = lgbm.predict_proba(x_val)\n",
    "        \n",
    "#         perm = PermutationImportance(lgbm, scoring = \"neg_log_loss\", random_state = seed).fit(x_val, y_val)\n",
    "#         perm_dicts[str(seed)+'_seed_'+str(i+1)+'_fold'] = pd.DataFrame({'feature':x_val.columns.tolist(), \n",
    "#                                                                         'importance':perm.feature_importances_}\n",
    "#                                                                       ).sort_values('importance')\n",
    "#     print('multi_logloss:', log_loss(train_y, cv))\n",
    "        \n",
    "# for i, df in enumerate(perm_dicts.values()):\n",
    "#     if i==0:\n",
    "#         perm_df = df\n",
    "#     else:\n",
    "#         perm_df = pd.merge(perm_df, df, on='feature')\n",
    "# perm_remove_df = perm_df.set_index('feature').mean(axis=1)>=0\n",
    "# remove_features = perm_remove_df[perm_remove_df==False].index\n",
    "# train_x = train_x.drop(remove_features, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 변수 하나씩 지우며 성능 체크하는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "# cv = np.zeros((train_x.shape[0], 3))\n",
    "# for n, (train_idx, val_idx) in enumerate(kfold.split(train_x, train_y)):\n",
    "#     x_train, x_val = train_x.iloc[train_idx], train_x.iloc[val_idx]\n",
    "#     y_train, y_val = train_y.iloc[train_idx], train_y.iloc[val_idx]\n",
    "#     lgbm = LGBMClassifier(**lgb_best_hyperparams, seed=0)\n",
    "#     lgbm.fit(x_train, y_train, eval_set=[(x_val, y_val)], early_stopping_rounds=30, verbose=None)\n",
    "#     cv[val_idx] = lgbm.predict_proba(x_val)\n",
    "# Initial_log_loss = log_loss(train_y, cv)\n",
    "# print(f'Initial_multi_logloss: {Initial_log_loss}')\n",
    "\n",
    "# remove_features = {}\n",
    "# for i in range(1, 2):\n",
    "#     for j in tqdm(combinations(list(range(0, train_x.shape[1])), i)):\n",
    "#         train_new_x = train_x.drop(train_x.columns[list(j)], axis=1)\n",
    "        \n",
    "#         kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "#         cv = np.zeros((train_new_x.shape[0], 3))\n",
    "#         for n, (train_idx, val_idx) in enumerate(kfold.split(train_new_x, train_y)):\n",
    "#             x_train, x_val = train_new_x.iloc[train_idx], train_new_x.iloc[val_idx]\n",
    "#             y_train, y_val = train_y.iloc[train_idx], train_y.iloc[val_idx]\n",
    "#             lgbm = LGBMClassifier(**lgb_best_hyperparams)\n",
    "#             lgbm.fit(x_train, y_train, eval_set=[(x_val, y_val)], early_stopping_rounds=30, verbose=None)\n",
    "#             cv[val_idx] = lgbm.predict_proba(x_val)\n",
    "#         remove_features[list(j)[0]] = log_loss(train_y, cv)\n",
    "#         if Initial_log_loss > log_loss(train_y, cv):\n",
    "#             print(f'{list(j)[0]}_multi_logloss: {log_loss(train_y, cv)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove_feature = sorted(remove_features, key=lambda x: remove_features[x])[:3]\n",
    "# train_x = train_x.drop(train_x.columns[remove_feature], axis=1)\n",
    "# test_x =  test_x.drop((test_x.columns[remove_feature], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForest GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {'max_depth': [55, 60, 65] # 튜닝할 파라미터 삽입\n",
    "#             }\n",
    "\n",
    "# rf_clf = RandomForestClassifier(random_state = 0, n_estimators = 1000, \n",
    "#                                 min_samples_leaf=2, min_samples_split=2,\n",
    "#                                 criterion='entropy', n_jobs = -1)\n",
    "# grid_cv = GridSearchCV(rf_clf, param_grid = params, cv = 5, n_jobs = -1)\n",
    "# grid_cv.fit(df_train, y)\n",
    "\n",
    "# print('최적 하이퍼 파라미터: ', grid_cv.best_params_)\n",
    "# print('최고 예측 정확도: {:.4f}'.format(grid_cv.best_score_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
