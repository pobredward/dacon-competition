{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Module Import & Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import joblib\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from itertools import combinations\n",
    "from eli5.permutation_importance import get_score_importances\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "from sklearn import cluster\n",
    "\n",
    "import optuna\n",
    "from optuna import Trial\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.visualization import plot_contour, plot_optimization_history\n",
    "from optuna.visualization import plot_parallel_coordinate, plot_slice, plot_param_importances\n",
    "from hyperopt.pyll.base import scope\n",
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "from catboost import Pool, CatBoostClassifier\n",
    "from kaggler.model import AutoLGB\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, GridSearchCV, StratifiedShuffleSplit, train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, log_loss\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv').drop(['index', 'FLAG_MOBIL'], axis=1).fillna('NAN')\n",
    "test = pd.read_csv('data/test.csv').drop(['index', 'FLAG_MOBIL'], axis=1).fillna('NAN')\n",
    "sample_submission = pd.read_csv('data/sample_submission.csv')\n",
    "\n",
    "# train데이터와 test데이터 변수를 함께 조정하기 위해 병합\n",
    "merge_data = pd.concat([train, test], axis = 0)\n",
    "\n",
    "# DAYS_BIRTH\n",
    "merge_data['DAYS_BIRTH_month']=np.floor((-merge_data['DAYS_BIRTH'])/30)-(\n",
    "    (np.floor((-merge_data['DAYS_BIRTH'])/30)/12).astype(int)*12)\n",
    "merge_data['DAYS_BIRTH_week']=np.floor((-merge_data['DAYS_BIRTH'])/7)-(\n",
    "    (np.floor((-merge_data['DAYS_BIRTH'])/7)/4).astype(int)*4)\n",
    "\n",
    "# DAYS_EMPLOYED\n",
    "merge_data['DAYS_EMPLOYED_month']=np.floor((-merge_data['DAYS_EMPLOYED'])/30)-(\n",
    "    (np.floor((-merge_data['DAYS_EMPLOYED'])/30)/12).astype(int)*12)\n",
    "merge_data['DAYS_EMPLOYED_week']=np.floor((-merge_data['DAYS_EMPLOYED'])/7)-(\n",
    "    (np.floor((-merge_data['DAYS_EMPLOYED'])/7)/4).astype(int)*4)\n",
    "\n",
    "# before_EMPLOYED\n",
    "merge_data['before_EMPLOYED']=merge_data['DAYS_BIRTH']-merge_data['DAYS_EMPLOYED']\n",
    "merge_data['before_EMPLOYED_month']=np.floor((-merge_data['before_EMPLOYED'])/30)-(\n",
    "    (np.floor((-merge_data['before_EMPLOYED'])/30)/12).astype(int)*12)\n",
    "merge_data['before_EMPLOYED_week']=np.floor((-merge_data['before_EMPLOYED'])/7)-(\n",
    "    (np.floor((-merge_data['before_EMPLOYED'])/7)/4).astype(int)*4)\n",
    "\n",
    "# DAYS_BIRTH / Income\n",
    "merge_data['DAYS_BIRTH_month/income_total'] = merge_data['DAYS_BIRTH_month'] / merge_data['income_total']\n",
    "merge_data['DAYS_BIRTH_week/income_total'] = merge_data['DAYS_BIRTH_week'] / merge_data['income_total']\n",
    "\n",
    "# DAYS_EMPLOYED / Income\n",
    "merge_data['DAYS_EMPLOYED_month/income_total'] = merge_data['DAYS_EMPLOYED_month'] / merge_data['income_total']\n",
    "merge_data['DAYS_EMPLOYED_week/income_total'] = merge_data['DAYS_EMPLOYED_week'] / merge_data['income_total']\n",
    "\n",
    "# before_EMPLOYED / Income\n",
    "merge_data['before_EMPLOYED/income_total'] = merge_data['before_EMPLOYED'] / merge_data['income_total']\n",
    "merge_data['before_EMPLOYED_month/income_total'] = merge_data['before_EMPLOYED_month'] / merge_data['income_total']\n",
    "merge_data['before_EMPLOYED_week/income_total'] = merge_data['before_EMPLOYED_week'] / merge_data['income_total']\n",
    "\n",
    "# Income / Family\n",
    "merge_data['income_total/family_size'] = merge_data['income_total'] / merge_data['family_size']\n",
    "\n",
    "merge_data['child_num/income_total'] = merge_data['child_num'] / merge_data['income_total']\n",
    "merge_data['family_size/income_total'] = merge_data['family_size'] / merge_data['income_total']\n",
    "merge_data['DAYS_BIRTH/income_total'] = merge_data['DAYS_BIRTH'] / merge_data['income_total']\n",
    "merge_data['DAYS_EMPLOYED/income_total'] = merge_data['DAYS_EMPLOYED'] / merge_data['income_total']\n",
    "merge_data['DAYS_EMPLOYED/DAYS_BIRTH'] =  merge_data['DAYS_EMPLOYED'] / merge_data['DAYS_BIRTH']\n",
    "\n",
    "# Income skewed-data\n",
    "merge_data['income_total'] = np.log1p(merge_data['income_total'])\n",
    "# merge_data['log_income_total'] = np.log(merge_data['income_total'])\n",
    "# merge_data['sqrt_income_total'] = np.sqrt(merge_data['income_total'])\n",
    "# merge_data['boxcox_income_total'] = stats.boxcox(merge_data['income_total'])[0]\n",
    "\n",
    "merge_data = merge_data.fillna(-999)\n",
    "train = merge_data[merge_data['credit'] != -999]\n",
    "test = merge_data[merge_data['credit'] == -999]\n",
    "test.drop('credit', axis = 1, inplace = True)\n",
    "\n",
    "train_cols = list(train.columns); train_cols.remove('credit'); train_cols.append('credit')\n",
    "train = train[train_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "train = train[train['child_num']<=6].reset_index(drop=True) # 아이의 수가 7명 이상인 데이터 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기본 모델로 성능 측정하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_lgbmodel(train, verbose=True):\n",
    "    \n",
    "    train_x = train.drop(['credit'], axis=1)\n",
    "    train_y = train['credit']\n",
    "    \n",
    "    np.random.seed(0)\n",
    "    lucky_seeds=np.random.randint(1, 10000, 5)\n",
    "    score_list = []\n",
    "    \n",
    "    for i, seed in enumerate(lucky_seeds):\n",
    "\n",
    "        kfold = StratifiedKFold(n_splits=5, random_state = seed, shuffle = True) # CV 늘려가면서 하기\n",
    "        cv=np.zeros((train_x.shape[0], 3))\n",
    "\n",
    "        for n, (train_idx, val_idx) in enumerate(kfold.split(train_x, train_y)):\n",
    "\n",
    "            x_train, x_val = train_x.iloc[train_idx], train_x.iloc[val_idx]\n",
    "            y_train, y_val = train_y.iloc[train_idx], train_y.iloc[val_idx]\n",
    "\n",
    "            lgbmodel = LGBMClassifier(objective='multiclass', n_estimators=10000, random_state=seed)\n",
    "            lgbmodel.fit(x_train, y_train, eval_set=[(x_val, y_val)], early_stopping_rounds=30, verbose=None) \n",
    "\n",
    "            cv[val_idx,:] = lgbmodel.predict_proba(x_val)\n",
    "        if verbose==True:\n",
    "            print(f'multi_logloss: {log_loss(train_y, cv):.4f}')\n",
    "        score_list.append(log_loss(train_y, cv))\n",
    "    print(f'Average Logloss: {np.mean(score_list):.4f}')\n",
    "    return np.mean(score_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 향상된 모델로 성능 측정하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_lgbmodel(train, verbose=True):\n",
    "    \n",
    "    train_x = train.drop(['credit'], axis=1)\n",
    "    train_y = train['credit']\n",
    "    \n",
    "    np.random.seed(0)\n",
    "    lucky_seeds=np.random.randint(1, 10000, 3)\n",
    "    score_list = []\n",
    "    \n",
    "    for i, seed in enumerate(lucky_seeds):\n",
    "\n",
    "        kfold = StratifiedKFold(n_splits=5, random_state = seed, shuffle = True) # CV 늘려가면서 하기\n",
    "        cv=np.zeros((train_x.shape[0], 3))\n",
    "\n",
    "        for n, (train_idx, val_idx) in enumerate(kfold.split(train_x, train_y)):\n",
    "\n",
    "            x_train, x_val = train_x.iloc[train_idx], train_x.iloc[val_idx]\n",
    "            y_train, y_val = train_y.iloc[train_idx], train_y.iloc[val_idx]\n",
    "\n",
    "            lgbmodel = LGBMClassifier(learning_rate=0.01, objective='multiclass', num_leaves=1000, max_depth=-1,\n",
    "                                      n_estimators=10000, random_state=seed)\n",
    "            lgbmodel.fit(x_train, y_train, eval_set=[(x_val, y_val)], early_stopping_rounds=30, verbose=None) \n",
    "\n",
    "            cv[val_idx,:] = lgbmodel.predict_proba(x_val)\n",
    "        if verbose==True:\n",
    "            print(f'multi_logloss: {log_loss(train_y, cv):.4f}')\n",
    "        score_list.append(log_loss(train_y, cv))\n",
    "    print(f'Average Logloss: {np.mean(score_list):.4f}')\n",
    "    return np.mean(score_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 원 핫 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One Hot Encoding Completed\n"
     ]
    }
   ],
   "source": [
    "train_oh = train.copy()\n",
    "test_oh = test.copy()\n",
    "\n",
    "object_col = []\n",
    "for col in train_oh.columns:\n",
    "    if (train_oh[col].dtype == 'object'):\n",
    "        object_col.append(col)   \n",
    "enc = OneHotEncoder()\n",
    "enc.fit(train.loc[:,object_col])\n",
    "\n",
    "train_onehot_df = pd.DataFrame(enc.transform(train_oh.loc[:,object_col]).toarray(), \n",
    "             columns=enc.get_feature_names(object_col))\n",
    "train_oh.drop(object_col, axis=1, inplace=True)\n",
    "train_oh = pd.concat([train_oh, train_onehot_df], axis=1)    \n",
    "\n",
    "test_onehot_df = pd.DataFrame(enc.transform(test_oh.loc[:,object_col]).toarray(), \n",
    "             columns=enc.get_feature_names(object_col))\n",
    "test_oh.drop(object_col, axis=1, inplace=True)\n",
    "test_oh = pd.concat([test_oh, test_onehot_df], axis=1)\n",
    "\n",
    "print('One Hot Encoding Completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_lgbmodel(train_oh, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 라벨 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Encoding Completed\n"
     ]
    }
   ],
   "source": [
    "train_lab = train.copy()\n",
    "test_lab = test.copy()\n",
    "\n",
    "enc = LabelEncoder()\n",
    "for col in train_lab.columns:\n",
    "    if train_lab[col].dtypes=='object':\n",
    "        train_lab[col] = enc.fit_transform(train_lab[col])\n",
    "        test_lab[col] = enc.fit_transform(test_lab[col])\n",
    "\n",
    "    \n",
    "print('Label Encoding Completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_lgbmodel(train_lab, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 카테고리 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category Encoding Completed\n"
     ]
    }
   ],
   "source": [
    "train_cat = train.copy()\n",
    "test_cat = test.copy()\n",
    "\n",
    "for col in train_cat.columns:\n",
    "    if train_cat[col].dtypes=='object':\n",
    "        train_cat[col] =  train_cat[col].astype('category')\n",
    "        test_cat[col] =  test_cat[col].astype('category')\n",
    "    \n",
    "print('Category Encoding Completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_lgbmodel(train_cat, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### income_total 범주화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Initial Logloss', end=' ')\n",
    "# base_lgbmodel(train_cat, verbose=False)\n",
    "# raw_income = train_cat.income_total.copy()\n",
    "# for cut in np.arange(1000, 5000, 500):\n",
    "#     print(f'cut space:{cut}', end=' ')\n",
    "#     cutted_income = pd.cut(raw_income, bins=np.arange(27000, 1575000, cut), labels=False)\n",
    "#     train_cat['income_total'] = cutted_income\n",
    "#     base_lgbmodel(train_cat, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN (income_total, income_type, occyp_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# income_type, occyp_type, income_total을 이용하여 클러스터링하기 위해 따로 원 핫 인코딩\n",
    "kmeans_train = train[['occyp_type', 'income_type', 'income_total']]\n",
    "kmeans_test = test[['occyp_type', 'income_type', 'income_total']]\n",
    "object_col = []\n",
    "for col in kmeans_train.columns:\n",
    "    if kmeans_train[col].dtype == 'object':\n",
    "        object_col.append(col)\n",
    "        \n",
    "enc = OneHotEncoder()\n",
    "enc.fit(kmeans_train.loc[:,object_col])\n",
    "\n",
    "train_onehot_df = pd.DataFrame(enc.transform(kmeans_train.loc[:,object_col]).toarray(), \n",
    "             columns=enc.get_feature_names(object_col))\n",
    "kmeans_train.drop(object_col, axis=1, inplace=True)\n",
    "kmeans_train = pd.concat([kmeans_train, train_onehot_df], axis=1)\n",
    "\n",
    "test_onehot_df = pd.DataFrame(enc.transform(kmeans_test.loc[:,object_col]).toarray(),\n",
    "             columns=enc.get_feature_names(object_col))\n",
    "kmeans_test.drop(object_col, axis=1, inplace=True)\n",
    "kmeans_test = pd.concat([kmeans_test, test_onehot_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Logloss: 0.7107\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db2b3af5eeb645968c6b188f2f309aeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=9.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster: 3 Average Logloss: 0.7106\n",
      "cluster: 4 Average Logloss: 0.7110\n",
      "cluster: 5 Average Logloss: 0.7109\n",
      "cluster: 6 Average Logloss: 0.7107\n",
      "cluster: 7 Average Logloss: 0.7097\n",
      "cluster: 8 Average Logloss: 0.7106\n",
      "cluster: 9 Average Logloss: 0.7113\n",
      "cluster: 10 Average Logloss: 0.7110\n",
      "cluster: 11 Average Logloss: 0.7104\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base_lgbmodel(train_cat, verbose=False)\n",
    "# n_clusters를 3부터 10까지 진행하여 하나씩 성능 체크\n",
    "score_list = {}\n",
    "k_means_train_total_df = pd.DataFrame()\n",
    "k_means_test_total_df = pd.DataFrame()\n",
    "for i in tqdm(range(3, 12)):\n",
    "    train_cat = train.copy()\n",
    "    test_cat = test.copy()\n",
    "\n",
    "    for col in train_cat.columns:\n",
    "        if train_cat[col].dtypes=='object':\n",
    "            train_cat[col] =  train_cat[col].astype('category')\n",
    "            test_cat[col] =  test_cat[col].astype('category')\n",
    "\n",
    "    # n_cluster를 늘려가며 클러스터링 진행\n",
    "    k_means_train_df = pd.DataFrame()\n",
    "    k_means_test_df = pd.DataFrame()\n",
    "    k_means = cluster.KMeans(n_clusters=i)\n",
    "    k_means.fit(kmeans_train)\n",
    "    k_means_train_df = pd.concat([k_means_train_df, pd.DataFrame(k_means.labels_, columns=[f'cluster_{i}'])], axis=1)\n",
    "    k_means_train_total_df = pd.concat([k_means_train_total_df, pd.DataFrame(k_means.labels_, columns=[f'cluster_{i}'])], axis=1)\n",
    "    k_means.fit(kmeans_test)\n",
    "    k_means_test_df = pd.concat([k_means_test_df, pd.DataFrame(k_means.labels_, columns=[f'cluster_{i}'])], axis=1)\n",
    "    k_means_test_total_df = pd.concat([k_means_test_total_df, pd.DataFrame(k_means.labels_, columns=[f'cluster_{i}'])], axis=1)\n",
    "\n",
    "    train_cat = pd.concat([train_cat, k_means_train_df], axis=1)\n",
    "    test_cat = pd.concat([test_cat, k_means_test_df], axis=1)\n",
    "\n",
    "    # 클러스터링 결과를 category 타입으로 변경\n",
    "    for col in train_cat.columns:\n",
    "        if train_cat[col].dtypes=='int32':\n",
    "            train_cat[col] =  train_cat[col].astype('category')\n",
    "            test_cat[col] =  test_cat[col].astype('category')\n",
    "    print(f'cluster: {i}', end=' ')\n",
    "    score_list[f'cluster_{i}'] = base_lgbmodel(train_cat, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cluster_7', 'cluster_11']\n",
      "Average Logloss: 0.7112\n",
      "0.7111579789536809\n"
     ]
    }
   ],
   "source": [
    "# 성능이 좋아지는 클러스터링 개수로만 피처 추출\n",
    "\n",
    "train_cat = train.copy()\n",
    "test_cat = test.copy()\n",
    "\n",
    "for col in train_cat.columns:\n",
    "    if train_cat[col].dtypes=='object':\n",
    "        train_cat[col] =  train_cat[col].astype('category')\n",
    "        test_cat[col] =  test_cat[col].astype('category')\n",
    "        \n",
    "n = 2 # 성능이 좋아지는 클러스터 개수별로 정렬한 후 앞의 n개만 추출\n",
    "train_cat = pd.concat([train_cat, k_means_train_total_df.loc[:, sorted(score_list, key=lambda x: score_list[x])[:n]]], axis=1)\n",
    "test_cat = pd.concat([test_cat, k_means_test_total_df.loc[:, sorted(score_list, key=lambda x: score_list[x])[:n]]], axis=1)\n",
    "\n",
    "for col in train_cat.columns:\n",
    "    if train_cat[col].dtypes=='int32':\n",
    "        train_cat[col] =  train_cat[col].astype('category')\n",
    "        test_cat[col] =  test_cat[col].astype('category')\n",
    "        \n",
    "print(k_means_train_total_df.loc[:, sorted(score_list, key=lambda x: score_list[x])[:n]].columns.tolist())\n",
    "print(base_lgbmodel(train_cat, verbose=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permutation Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enc = LabelEncoder()\n",
    "# for col in train_cat.columns:\n",
    "#     if train_cat[col].dtype.name=='category':\n",
    "#         train_cat[col] = enc.fit_transform(train_cat[col])\n",
    "#         test_cat[col] = enc.fit_transform(test_cat[col])\n",
    "        \n",
    "# print('Label Encoding Completed')\n",
    "\n",
    "# train_x = train_cat.drop(['credit'], axis=1)\n",
    "# train_y = train_cat['credit']\n",
    "# test_x = test_cat.copy()\n",
    "\n",
    "# seeds = np.random.randint(0, 1000, 3)\n",
    "# perm_dicts = {}\n",
    "# cv = np.zeros((train_x.shape[0], 3))\n",
    "# for n, seed in enumerate(seeds):\n",
    "#     kfold = StratifiedKFold(n_splits=5, random_state=seed, shuffle=True)\n",
    "#     for i, (train_idx, val_idx) in enumerate(kfold.split(train_x, train_y)):\n",
    "\n",
    "#         x_train, x_val = train_x.iloc[train_idx], train_x.iloc[val_idx]\n",
    "#         y_train, y_val = train_y.iloc[train_idx], train_y.iloc[val_idx]\n",
    "\n",
    "#         lgbm = LGBMClassifier(n_estimators=10000, objective='multiclass', seed=0)\n",
    "#         lgbm.fit(x_train, y_train, eval_set=[(x_val, y_val)], early_stopping_rounds=30, verbose=None)\n",
    "#         cv[val_idx, :] = lgbm.predict_proba(x_val)\n",
    "        \n",
    "#         perm = PermutationImportance(lgbm, scoring = \"neg_log_loss\", random_state = seed).fit(x_val, y_val)\n",
    "#         perm_dicts[str(seed)+'_seed_'+str(i+1)+'_fold'] = pd.DataFrame({'feature':x_val.columns.tolist(), \n",
    "#                                                                         'importance':perm.feature_importances_}\n",
    "#                                                                       ).sort_values('importance')\n",
    "#     print('multi_logloss:', log_loss(train_y, cv))\n",
    "        \n",
    "# for i, df in enumerate(perm_dicts.values()):\n",
    "#     if i==0:\n",
    "#         perm_df = df\n",
    "#     else:\n",
    "#         perm_df = pd.merge(perm_df, df, on='feature')\n",
    "# perm_remove_df = perm_df.set_index('feature').mean(axis=1)>=0\n",
    "# remove_features = perm_remove_df[perm_remove_df==False].index\n",
    "# train_x = train_x.drop(remove_features, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 변수 하나씩 지우며 성능 체크하는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "# cv = np.zeros((train_x.shape[0], 3))\n",
    "# for n, (train_idx, val_idx) in enumerate(kfold.split(train_x, train_y)):\n",
    "#     x_train, x_val = train_x.iloc[train_idx], train_x.iloc[val_idx]\n",
    "#     y_train, y_val = train_y.iloc[train_idx], train_y.iloc[val_idx]\n",
    "#     lgbm = LGBMClassifier(**lgb_best_hyperparams, seed=0)\n",
    "#     lgbm.fit(x_train, y_train, eval_set=[(x_val, y_val)], early_stopping_rounds=30, verbose=None)\n",
    "#     cv[val_idx] = lgbm.predict_proba(x_val)\n",
    "# Initial_log_loss = log_loss(train_y, cv)\n",
    "# print(f'Initial_multi_logloss: {Initial_log_loss}')\n",
    "\n",
    "# remove_features = {}\n",
    "# for i in range(1, 2):\n",
    "#     for j in tqdm(combinations(list(range(0, train_x.shape[1])), i)):\n",
    "#         train_new_x = train_x.drop(train_x.columns[list(j)], axis=1)\n",
    "        \n",
    "#         kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "#         cv = np.zeros((train_new_x.shape[0], 3))\n",
    "#         for n, (train_idx, val_idx) in enumerate(kfold.split(train_new_x, train_y)):\n",
    "#             x_train, x_val = train_new_x.iloc[train_idx], train_new_x.iloc[val_idx]\n",
    "#             y_train, y_val = train_y.iloc[train_idx], train_y.iloc[val_idx]\n",
    "#             lgbm = LGBMClassifier(**lgb_best_hyperparams)\n",
    "#             lgbm.fit(x_train, y_train, eval_set=[(x_val, y_val)], early_stopping_rounds=30, verbose=None)\n",
    "#             cv[val_idx] = lgbm.predict_proba(x_val)\n",
    "#         remove_features[list(j)[0]] = log_loss(train_y, cv)\n",
    "#         if Initial_log_loss > log_loss(train_y, cv):\n",
    "#             print(f'{list(j)[0]}_multi_logloss: {log_loss(train_y, cv)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove_feature = sorted(remove_features, key=lambda x: remove_features[x])[:3]\n",
    "# train_x = train_x.drop(train_x.columns[remove_feature], axis=1)\n",
    "# test_x =  test_x.drop((test_x.columns[remove_feature], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dict = {}\n",
    "pred_test_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_cat.drop(['credit'], axis=1) # 데이터 나누기\n",
    "train_y = train_cat['credit']\n",
    "test_x = test_cat.copy()\n",
    "\n",
    "train_x_raw = train_x.copy() # 카테고리 인코딩 된 데이터 저장해두기\n",
    "test_x_raw = test_x.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_x = train_x_raw.copy() # XGB, RF 돌리다가 LGB 돌리고싶을땐 주석풀고 이 코드 실행\n",
    "# test_x = test_x_raw.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyperParameter Tuning (optuna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_objective(trial: Trial) -> float:\n",
    "    params_lgb = {\n",
    "        \"random_state\": 91373,\n",
    "        \"verbosity\": -1,\n",
    "        \"n_estimators\": 10000,\n",
    "        \"objective\": \"multiclass\",\n",
    "        \"metric\": \"multi_logloss\",\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.003, 0.009),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-3, 1),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-3, 1),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 8, 30),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 64, 1200),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.4, 1.0),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.3, 1.0),\n",
    "        \"subsample_freq\": trial.suggest_int(\"subsample_freq\", 1, 10),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 20),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 200, 500),\n",
    "    }\n",
    "    \n",
    "    # CV=10으로 튜닝\n",
    "    \n",
    "    seed = 91373\n",
    "    kfold = StratifiedKFold(n_splits=10, random_state = seed, shuffle = True) # Cross-validation cv=5\n",
    "    cv = np.zeros((train_x.shape[0], 3))\n",
    "\n",
    "    for n, (train_idx, val_idx) in enumerate(kfold.split(train_x, train_y)):\n",
    "\n",
    "        x_train, x_val = train_x.iloc[train_idx], train_x.iloc[val_idx]\n",
    "        y_train, y_val = train_y.iloc[train_idx], train_y.iloc[val_idx]\n",
    "\n",
    "        lgbmodel = LGBMClassifier(**params_lgb)\n",
    "                                                                                        # 진행상황 보고싶을때 None을 100으로\n",
    "        lgbmodel.fit(x_train, y_train, eval_set=[(x_val, y_val)], early_stopping_rounds=30, verbose=None) \n",
    "        cv[val_idx, :] = lgbmodel.predict_proba(x_val)\n",
    "    # print('multi_logloss:', log_loss(train_y, cv))\n",
    "\n",
    "    \n",
    "    return log_loss(train_y, cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-05-21 02:54:11,575]\u001b[0m A new study created in memory with name: lgbm_parameter_opt\u001b[0m\n",
      "\u001b[32m[I 2021-05-21 02:58:08,934]\u001b[0m Trial 0 finished with value: 0.6975400397809083 and parameters: {'learning_rate': 0.005247240713084175, 'reg_alpha': 0.9507635921035062, 'reg_lambda': 0.7322619478695936, 'max_depth': 21, 'num_leaves': 241, 'colsample_bytree': 0.49359671220172163, 'subsample': 0.3406585285177396, 'subsample_freq': 9, 'min_child_samples': 62, 'min_child_weight': 15, 'max_bin': 206}. Best is trial 0 with value: 0.6975400397809083.\u001b[0m\n",
      "\u001b[32m[I 2021-05-21 03:01:02,709]\u001b[0m Trial 1 finished with value: 0.6876954022262171 and parameters: {'learning_rate': 0.008819459112971965, 'reg_alpha': 0.8326101981596213, 'reg_lambda': 0.21312677156759788, 'max_depth': 12, 'num_leaves': 272, 'colsample_bytree': 0.5825453457757226, 'subsample': 0.6673295021425665, 'subsample_freq': 5, 'min_child_samples': 32, 'min_child_weight': 13, 'max_bin': 241}. Best is trial 1 with value: 0.6876954022262171.\u001b[0m\n",
      "\u001b[32m[I 2021-05-21 03:05:43,330]\u001b[0m Trial 2 finished with value: 0.6859506618700459 and parameters: {'learning_rate': 0.004752867891211309, 'reg_alpha': 0.366995481450398, 'reg_lambda': 0.4566139142328189, 'max_depth': 26, 'num_leaves': 291, 'colsample_bytree': 0.708540663048167, 'subsample': 0.7146901982034297, 'subsample_freq': 1, 'min_child_samples': 63, 'min_child_weight': 4, 'max_bin': 219}. Best is trial 2 with value: 0.6859506618700459.\u001b[0m\n",
      "\u001b[32m[I 2021-05-21 03:09:02,842]\u001b[0m Trial 3 finished with value: 0.686856245970775 and parameters: {'learning_rate': 0.008693313223519999, 'reg_alpha': 0.9656664010414848, 'reg_lambda': 0.8085889507683447, 'max_depth': 15, 'num_leaves': 175, 'colsample_bytree': 0.8105398159072941, 'subsample': 0.6081067456177209, 'subsample_freq': 2, 'min_child_samples': 52, 'min_child_weight': 1, 'max_bin': 473}. Best is trial 2 with value: 0.6859506618700459.\u001b[0m\n",
      "\u001b[32m[I 2021-05-21 03:16:07,706]\u001b[0m Trial 4 finished with value: 0.6886769806929607 and parameters: {'learning_rate': 0.0045526798896001015, 'reg_alpha': 0.662859762069628, 'reg_lambda': 0.31239936501332155, 'max_depth': 19, 'num_leaves': 685, 'colsample_bytree': 0.5109126733153162, 'subsample': 0.9787092394351908, 'subsample_freq': 8, 'min_child_samples': 95, 'min_child_weight': 18, 'max_bin': 379}. Best is trial 2 with value: 0.6859506618700459.\u001b[0m\n",
      "\u001b[32m[I 2021-05-21 03:18:52,224]\u001b[0m Trial 5 finished with value: 0.6908648710327179 and parameters: {'learning_rate': 0.0085312454101387, 'reg_alpha': 0.08940400954986757, 'reg_lambda': 0.19678687955672605, 'max_depth': 9, 'num_leaves': 433, 'colsample_bytree': 0.6332063738136893, 'subsample': 0.4899443222417271, 'subsample_freq': 9, 'min_child_samples': 39, 'min_child_weight': 6, 'max_bin': 363}. Best is trial 2 with value: 0.6859506618700459.\u001b[0m\n",
      "\u001b[32m[I 2021-05-21 03:24:57,944]\u001b[0m Trial 6 finished with value: 0.7017028572547303 and parameters: {'learning_rate': 0.003845545349848576, 'reg_alpha': 0.8023947837732857, 'reg_lambda': 0.07547609303609105, 'max_depth': 30, 'num_leaves': 942, 'colsample_bytree': 0.5192294089205034, 'subsample': 0.3038654819865217, 'subsample_freq': 9, 'min_child_samples': 72, 'min_child_weight': 15, 'max_bin': 432}. Best is trial 2 with value: 0.6859506618700459.\u001b[0m\n",
      "\u001b[32m[I 2021-05-21 03:32:36,612]\u001b[0m Trial 7 finished with value: 0.6943393416352936 and parameters: {'learning_rate': 0.003444267910404542, 'reg_alpha': 0.35910726281572836, 'reg_lambda': 0.11675319046560459, 'max_depth': 27, 'num_leaves': 772, 'colsample_bytree': 0.5985388149115896, 'subsample': 0.34449084520021656, 'subsample_freq': 4, 'min_child_samples': 36, 'min_child_weight': 15, 'max_bin': 391}. Best is trial 2 with value: 0.6859506618700459.\u001b[0m\n",
      "\u001b[32m[I 2021-05-21 03:35:26,623]\u001b[0m Trial 8 finished with value: 0.6862466302033807 and parameters: {'learning_rate': 0.008323276455457959, 'reg_alpha': 0.4727427102367873, 'reg_lambda': 0.1204746516923634, 'max_depth': 24, 'num_leaves': 929, 'colsample_bytree': 0.7367663185416977, 'subsample': 0.8396770259681927, 'subsample_freq': 5, 'min_child_samples': 55, 'min_child_weight': 9, 'max_bin': 207}. Best is trial 2 with value: 0.6859506618700459.\u001b[0m\n",
      "\u001b[32m[I 2021-05-21 03:41:10,020]\u001b[0m Trial 9 finished with value: 0.6927589370051462 and parameters: {'learning_rate': 0.0036473485619598267, 'reg_alpha': 0.032397756501047516, 'reg_lambda': 0.6367740008525166, 'max_depth': 15, 'num_leaves': 642, 'colsample_bytree': 0.9445398843556558, 'subsample': 0.4745045604042124, 'subsample_freq': 5, 'min_child_samples': 77, 'min_child_weight': 5, 'max_bin': 223}. Best is trial 2 with value: 0.6859506618700459.\u001b[0m\n",
      "\u001b[32m[I 2021-05-21 03:46:03,372]\u001b[0m Trial 10 finished with value: 0.6894350697280368 and parameters: {'learning_rate': 0.0069398113953232955, 'reg_alpha': 0.31279801925415696, 'reg_lambda': 0.4689337310003644, 'max_depth': 29, 'num_leaves': 1167, 'colsample_bytree': 0.8813199209081033, 'subsample': 0.8112568266449068, 'subsample_freq': 1, 'min_child_samples': 14, 'min_child_weight': 1, 'max_bin': 286}. Best is trial 2 with value: 0.6859506618700459.\u001b[0m\n",
      "\u001b[32m[I 2021-05-21 03:50:39,977]\u001b[0m Trial 11 finished with value: 0.6880112267342479 and parameters: {'learning_rate': 0.006704726837958126, 'reg_alpha': 0.4450864428065579, 'reg_lambda': 0.9891205939336012, 'max_depth': 24, 'num_leaves': 1129, 'colsample_bytree': 0.7525450633340443, 'subsample': 0.8989285558650519, 'subsample_freq': 3, 'min_child_samples': 88, 'min_child_weight': 8, 'max_bin': 284}. Best is trial 2 with value: 0.6859506618700459.\u001b[0m\n",
      "\u001b[32m[I 2021-05-21 03:55:15,326]\u001b[0m Trial 12 finished with value: 0.6861376633408557 and parameters: {'learning_rate': 0.005887730957796136, 'reg_alpha': 0.5760129559832933, 'reg_lambda': 0.4365886322509043, 'max_depth': 25, 'num_leaves': 925, 'colsample_bytree': 0.7142220854849797, 'subsample': 0.7640952235729825, 'subsample_freq': 7, 'min_child_samples': 54, 'min_child_weight': 10, 'max_bin': 275}. Best is trial 2 with value: 0.6859506618700459.\u001b[0m\n",
      "\u001b[32m[I 2021-05-21 03:59:12,108]\u001b[0m Trial 13 finished with value: 0.6859983986144663 and parameters: {'learning_rate': 0.005815339391123893, 'reg_alpha': 0.20419858409117847, 'reg_lambda': 0.44861250834557953, 'max_depth': 25, 'num_leaves': 456, 'colsample_bytree': 0.6800957377291021, 'subsample': 0.7172936738742088, 'subsample_freq': 7, 'min_child_samples': 69, 'min_child_weight': 4, 'max_bin': 291}. Best is trial 2 with value: 0.6859506618700459.\u001b[0m\n",
      "\u001b[32m[I 2021-05-21 04:04:45,829]\u001b[0m Trial 14 finished with value: 0.6870017569956032 and parameters: {'learning_rate': 0.004955873101679859, 'reg_alpha': 0.155569546257673, 'reg_lambda': 0.5555516571578584, 'max_depth': 22, 'num_leaves': 459, 'colsample_bytree': 0.6612528409747244, 'subsample': 0.686893223697214, 'subsample_freq': 7, 'min_child_samples': 77, 'min_child_weight': 4, 'max_bin': 318}. Best is trial 2 with value: 0.6859506618700459.\u001b[0m\n",
      "\u001b[32m[I 2021-05-21 04:10:27,618]\u001b[0m Trial 15 finished with value: 0.6890109199298236 and parameters: {'learning_rate': 0.006078976456720229, 'reg_alpha': 0.2204656990032958, 'reg_lambda': 0.36042069218745865, 'max_depth': 28, 'num_leaves': 406, 'colsample_bytree': 0.8346635810400218, 'subsample': 0.5506168543975675, 'subsample_freq': 7, 'min_child_samples': 67, 'min_child_weight': 2, 'max_bin': 324}. Best is trial 2 with value: 0.6859506618700459.\u001b[0m\n",
      "\u001b[32m[I 2021-05-21 04:19:53,284]\u001b[0m Trial 16 finished with value: 0.6866857787476592 and parameters: {'learning_rate': 0.004288262658035761, 'reg_alpha': 0.31112414309062236, 'reg_lambda': 0.5711333467406943, 'max_depth': 26, 'num_leaves': 115, 'colsample_bytree': 0.43831421341262305, 'subsample': 0.7393220143703185, 'subsample_freq': 1, 'min_child_samples': 87, 'min_child_weight': 3, 'max_bin': 256}. Best is trial 2 with value: 0.6859506618700459.\u001b[0m\n",
      "\u001b[32m[I 2021-05-21 04:26:23,242]\u001b[0m Trial 17 finished with value: 0.689493710796672 and parameters: {'learning_rate': 0.007358501601091283, 'reg_alpha': 0.21298531546469154, 'reg_lambda': 0.3629282154888011, 'max_depth': 21, 'num_leaves': 496, 'colsample_bytree': 0.7864992818849761, 'subsample': 0.9382974053334283, 'subsample_freq': 10, 'min_child_samples': 100, 'min_child_weight': 7, 'max_bin': 324}. Best is trial 2 with value: 0.6859506618700459.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-05-21 04:33:07,334]\u001b[0m Trial 18 finished with value: 0.6873762736875665 and parameters: {'learning_rate': 0.005607028558517419, 'reg_alpha': 0.39203300474468783, 'reg_lambda': 0.7096835447717555, 'max_depth': 18, 'num_leaves': 327, 'colsample_bytree': 0.6736248157743403, 'subsample': 0.5945502775024079, 'subsample_freq': 6, 'min_child_samples': 45, 'min_child_weight': 12, 'max_bin': 246}. Best is trial 2 with value: 0.6859506618700459.\u001b[0m\n",
      "\u001b[32m[I 2021-05-21 04:39:04,616]\u001b[0m Trial 19 finished with value: 0.6877142823510389 and parameters: {'learning_rate': 0.00633485576276626, 'reg_alpha': 0.5936144137185339, 'reg_lambda': 0.8791098387769989, 'max_depth': 30, 'num_leaves': 80, 'colsample_bytree': 0.8988239472783234, 'subsample': 0.7435827157131368, 'subsample_freq': 3, 'min_child_samples': 10, 'min_child_weight': 4, 'max_bin': 201}. Best is trial 2 with value: 0.6859506618700459.\u001b[0m\n",
      "\u001b[32m[I 2021-05-21 04:45:47,956]\u001b[0m Trial 20 finished with value: 0.685804887212405 and parameters: {'learning_rate': 0.007580195689342787, 'reg_alpha': 0.04408338554947866, 'reg_lambda': 0.2699383503171331, 'max_depth': 23, 'num_leaves': 603, 'colsample_bytree': 0.60186555280138, 'subsample': 0.8665067097580175, 'subsample_freq': 6, 'min_child_samples': 84, 'min_child_weight': 6, 'max_bin': 299}. Best is trial 20 with value: 0.685804887212405.\u001b[0m\n",
      "\u001b[32m[I 2021-05-21 04:52:30,325]\u001b[0m Trial 21 finished with value: 0.6862441049623805 and parameters: {'learning_rate': 0.007793304491992633, 'reg_alpha': 0.0015084083900648027, 'reg_lambda': 0.23996949087805025, 'max_depth': 23, 'num_leaves': 554, 'colsample_bytree': 0.581494495594857, 'subsample': 0.9065066552677854, 'subsample_freq': 6, 'min_child_samples': 83, 'min_child_weight': 6, 'max_bin': 309}. Best is trial 20 with value: 0.685804887212405.\u001b[0m\n",
      "\u001b[32m[I 2021-05-21 05:01:30,534]\u001b[0m Trial 22 finished with value: 0.6858866068198797 and parameters: {'learning_rate': 0.005362675155715463, 'reg_alpha': 0.13116360898013402, 'reg_lambda': 0.4248531400491573, 'max_depth': 27, 'num_leaves': 778, 'colsample_bytree': 0.6871434193118, 'subsample': 0.8367339969630806, 'subsample_freq': 8, 'min_child_samples': 62, 'min_child_weight': 3, 'max_bin': 355}. Best is trial 20 with value: 0.685804887212405.\u001b[0m\n",
      "\u001b[32m[I 2021-05-21 05:18:06,721]\u001b[0m Trial 23 finished with value: 0.6847747463144215 and parameters: {'learning_rate': 0.0030073139305987016, 'reg_alpha': 0.10169413299521202, 'reg_lambda': 0.3053539665728724, 'max_depth': 27, 'num_leaves': 811, 'colsample_bytree': 0.6237310860616897, 'subsample': 0.841844781362006, 'subsample_freq': 8, 'min_child_samples': 61, 'min_child_weight': 1, 'max_bin': 410}. Best is trial 23 with value: 0.6847747463144215.\u001b[0m\n",
      "\u001b[32m[I 2021-05-21 05:25:29,914]\u001b[0m Trial 24 finished with value: 0.6847488829223594 and parameters: {'learning_rate': 0.00788304775066344, 'reg_alpha': 0.10356271422975835, 'reg_lambda': 0.022280450201471153, 'max_depth': 28, 'num_leaves': 783, 'colsample_bytree': 0.6253487866621272, 'subsample': 0.8419146408998046, 'subsample_freq': 8, 'min_child_samples': 24, 'min_child_weight': 1, 'max_bin': 417}. Best is trial 24 with value: 0.6847488829223594.\u001b[0m\n",
      "\u001b[32m[I 2021-05-21 05:33:14,594]\u001b[0m Trial 25 finished with value: 0.685881350778602 and parameters: {'learning_rate': 0.007897406628321578, 'reg_alpha': 0.01602817352768411, 'reg_lambda': 0.026194097918517206, 'max_depth': 29, 'num_leaves': 816, 'colsample_bytree': 0.40635451172416, 'subsample': 0.9937905173476751, 'subsample_freq': 10, 'min_child_samples': 24, 'min_child_weight': 1, 'max_bin': 420}. Best is trial 24 with value: 0.6847488829223594.\u001b[0m\n",
      "\u001b[32m[I 2021-05-21 05:42:06,850]\u001b[0m Trial 26 finished with value: 0.6831866487716537 and parameters: {'learning_rate': 0.007466403440973404, 'reg_alpha': 0.07828170158996119, 'reg_lambda': 0.2859126903075814, 'max_depth': 20, 'num_leaves': 1022, 'colsample_bytree': 0.545709602695438, 'subsample': 0.8668821452546976, 'subsample_freq': 8, 'min_child_samples': 21, 'min_child_weight': 1, 'max_bin': 484}. Best is trial 26 with value: 0.6831866487716537.\u001b[0m\n",
      "\u001b[32m[I 2021-05-21 05:50:36,218]\u001b[0m Trial 27 finished with value: 0.6830397183576321 and parameters: {'learning_rate': 0.007096771024924204, 'reg_alpha': 0.09816068161134374, 'reg_lambda': 0.03579065283289935, 'max_depth': 19, 'num_leaves': 1010, 'colsample_bytree': 0.5461329199876218, 'subsample': 0.7999486081859046, 'subsample_freq': 8, 'min_child_samples': 22, 'min_child_weight': 1, 'max_bin': 495}. Best is trial 27 with value: 0.6830397183576321.\u001b[0m\n",
      "\u001b[32m[I 2021-05-21 05:59:20,333]\u001b[0m Trial 28 finished with value: 0.6825600948604507 and parameters: {'learning_rate': 0.007206166465934459, 'reg_alpha': 0.2683115974869285, 'reg_lambda': 0.011611525566326018, 'max_depth': 17, 'num_leaves': 1020, 'colsample_bytree': 0.4590367510054892, 'subsample': 0.7885689810897614, 'subsample_freq': 9, 'min_child_samples': 18, 'min_child_weight': 2, 'max_bin': 499}. Best is trial 28 with value: 0.6825600948604507.\u001b[0m\n",
      "\u001b[32m[I 2021-05-21 06:08:19,921]\u001b[0m Trial 29 finished with value: 0.6869072870261453 and parameters: {'learning_rate': 0.0070844858098238995, 'reg_alpha': 0.27026604737444493, 'reg_lambda': 0.15487435129161525, 'max_depth': 18, 'num_leaves': 1066, 'colsample_bytree': 0.4543037596850504, 'subsample': 0.7846557527879655, 'subsample_freq': 10, 'min_child_samples': 5, 'min_child_weight': 20, 'max_bin': 494}. Best is trial 28 with value: 0.6825600948604507.\u001b[0m\n",
      "\u001b[32m[I 2021-05-21 06:17:23,393]\u001b[0m Trial 30 finished with value: 0.6835958692401566 and parameters: {'learning_rate': 0.006388312942394135, 'reg_alpha': 0.16560358482477144, 'reg_lambda': 0.010138335608287006, 'max_depth': 15, 'num_leaves': 1094, 'colsample_bytree': 0.5430997164343814, 'subsample': 0.9516917607699625, 'subsample_freq': 9, 'min_child_samples': 20, 'min_child_weight': 2, 'max_bin': 453}. Best is trial 28 with value: 0.6825600948604507.\u001b[0m\n",
      "\u001b[32m[I 2021-05-21 06:26:28,971]\u001b[0m Trial 31 finished with value: 0.6834520320661686 and parameters: {'learning_rate': 0.006532756681979381, 'reg_alpha': 0.17853038109120367, 'reg_lambda': 0.02656534428407696, 'max_depth': 15, 'num_leaves': 1045, 'colsample_bytree': 0.5385413538831594, 'subsample': 0.9491570894389942, 'subsample_freq': 9, 'min_child_samples': 19, 'min_child_weight': 2, 'max_bin': 460}. Best is trial 28 with value: 0.6825600948604507.\u001b[0m\n",
      "\u001b[32m[I 2021-05-21 06:34:45,749]\u001b[0m Trial 32 finished with value: 0.6826924023356907 and parameters: {'learning_rate': 0.007220821286330562, 'reg_alpha': 0.2652984406339838, 'reg_lambda': 0.08698403037044383, 'max_depth': 20, 'num_leaves': 1025, 'colsample_bytree': 0.4810665595884468, 'subsample': 0.876369623952996, 'subsample_freq': 9, 'min_child_samples': 29, 'min_child_weight': 2, 'max_bin': 497}. Best is trial 28 with value: 0.6825600948604507.\u001b[0m\n",
      "\u001b[32m[I 2021-05-21 06:43:25,795]\u001b[0m Trial 33 finished with value: 0.6829095900731879 and parameters: {'learning_rate': 0.007130634136351928, 'reg_alpha': 0.26437435328321196, 'reg_lambda': 0.07600943568949123, 'max_depth': 20, 'num_leaves': 984, 'colsample_bytree': 0.4808135525902175, 'subsample': 0.8921007214935543, 'subsample_freq': 10, 'min_child_samples': 29, 'min_child_weight': 3, 'max_bin': 500}. Best is trial 28 with value: 0.6825600948604507.\u001b[0m\n",
      "\u001b[32m[I 2021-05-21 06:51:35,471]\u001b[0m Trial 34 finished with value: 0.6823484517798757 and parameters: {'learning_rate': 0.007053758735632195, 'reg_alpha': 0.2593745281653904, 'reg_lambda': 0.08236221360922927, 'max_depth': 17, 'num_leaves': 980, 'colsample_bytree': 0.4706943980164826, 'subsample': 0.787536052758319, 'subsample_freq': 10, 'min_child_samples': 30, 'min_child_weight': 3, 'max_bin': 500}. Best is trial 34 with value: 0.6823484517798757.\u001b[0m\n",
      "\u001b[32m[I 2021-05-21 06:58:55,003]\u001b[0m Trial 35 finished with value: 0.6828479812076919 and parameters: {'learning_rate': 0.008142677707443209, 'reg_alpha': 0.27545274112913665, 'reg_lambda': 0.16546165384779166, 'max_depth': 13, 'num_leaves': 1191, 'colsample_bytree': 0.46950873793955766, 'subsample': 0.9143171246533828, 'subsample_freq': 10, 'min_child_samples': 30, 'min_child_weight': 3, 'max_bin': 499}. Best is trial 34 with value: 0.6823484517798757.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-05-21 07:04:50,663]\u001b[0m Trial 36 finished with value: 0.683687356966122 and parameters: {'learning_rate': 0.008987419746013069, 'reg_alpha': 0.4097294340448544, 'reg_lambda': 0.17672740298947529, 'max_depth': 12, 'num_leaves': 1196, 'colsample_bytree': 0.4005872728926337, 'subsample': 0.6627583649992329, 'subsample_freq': 10, 'min_child_samples': 43, 'min_child_weight': 5, 'max_bin': 473}. Best is trial 34 with value: 0.6823484517798757.\u001b[0m\n",
      "\u001b[32m[I 2021-05-21 07:11:38,114]\u001b[0m Trial 37 finished with value: 0.6827213449666762 and parameters: {'learning_rate': 0.008319962724563788, 'reg_alpha': 0.5284861720650198, 'reg_lambda': 0.09777440965658712, 'max_depth': 13, 'num_leaves': 858, 'colsample_bytree': 0.4739138892194339, 'subsample': 0.914579070918308, 'subsample_freq': 9, 'min_child_samples': 32, 'min_child_weight': 5, 'max_bin': 452}. Best is trial 34 with value: 0.6823484517798757.\u001b[0m\n",
      "\u001b[32m[I 2021-05-21 07:19:35,450]\u001b[0m Trial 38 finished with value: 0.6834076680095325 and parameters: {'learning_rate': 0.008669513220944579, 'reg_alpha': 0.5326503900498775, 'reg_lambda': 0.0868085243129482, 'max_depth': 17, 'num_leaves': 853, 'colsample_bytree': 0.4290747711500254, 'subsample': 0.9803131058035075, 'subsample_freq': 9, 'min_child_samples': 14, 'min_child_weight': 7, 'max_bin': 451}. Best is trial 34 with value: 0.6823484517798757.\u001b[0m\n",
      "\u001b[32m[I 2021-05-21 07:25:07,422]\u001b[0m Trial 39 finished with value: 0.6891759940733808 and parameters: {'learning_rate': 0.008293732302706287, 'reg_alpha': 0.6694142809203891, 'reg_lambda': 0.19627952757870382, 'max_depth': 8, 'num_leaves': 883, 'colsample_bytree': 0.5061226269002752, 'subsample': 0.6296099346434098, 'subsample_freq': 9, 'min_child_samples': 35, 'min_child_weight': 5, 'max_bin': 475}. Best is trial 34 with value: 0.6823484517798757.\u001b[0m\n",
      "\u001b[32m[I 2021-05-21 07:32:43,675]\u001b[0m Trial 40 finished with value: 0.6855351410575011 and parameters: {'learning_rate': 0.006864386012335794, 'reg_alpha': 0.7493397925361569, 'reg_lambda': 0.12418376960332886, 'max_depth': 13, 'num_leaves': 714, 'colsample_bytree': 0.4867501648254156, 'subsample': 0.6949998749821082, 'subsample_freq': 9, 'min_child_samples': 47, 'min_child_weight': 12, 'max_bin': 444}. Best is trial 34 with value: 0.6823484517798757.\u001b[0m\n",
      "\u001b[32m[I 2021-05-21 07:40:05,144]\u001b[0m Trial 41 finished with value: 0.6843963600513615 and parameters: {'learning_rate': 0.008089626587755375, 'reg_alpha': 0.3072298404578203, 'reg_lambda': 0.13213342895992947, 'max_depth': 10, 'num_leaves': 1115, 'colsample_bytree': 0.4653321513889707, 'subsample': 0.9087663865742386, 'subsample_freq': 10, 'min_child_samples': 30, 'min_child_weight': 3, 'max_bin': 484}. Best is trial 34 with value: 0.6823484517798757.\u001b[0m\n",
      "\u001b[32m[I 2021-05-21 07:46:56,309]\u001b[0m Trial 42 finished with value: 0.6834621919666601 and parameters: {'learning_rate': 0.008457502415467725, 'reg_alpha': 0.3421841510316108, 'reg_lambda': 0.2279112902960692, 'max_depth': 13, 'num_leaves': 950, 'colsample_bytree': 0.42485455056918703, 'subsample': 0.9371152596751712, 'subsample_freq': 10, 'min_child_samples': 39, 'min_child_weight': 2, 'max_bin': 463}. Best is trial 34 with value: 0.6823484517798757.\u001b[0m\n",
      "\u001b[32m[I 2021-05-21 07:53:50,327]\u001b[0m Trial 43 finished with value: 0.6822046404624154 and parameters: {'learning_rate': 0.008964104825823405, 'reg_alpha': 0.4499590552446543, 'reg_lambda': 0.05979914903878847, 'max_depth': 16, 'num_leaves': 1169, 'colsample_bytree': 0.4527040535891135, 'subsample': 0.7802084890324582, 'subsample_freq': 9, 'min_child_samples': 28, 'min_child_weight': 5, 'max_bin': 498}. Best is trial 43 with value: 0.6822046404624154.\u001b[0m\n",
      "\u001b[32m[I 2021-05-21 08:01:46,128]\u001b[0m Trial 44 finished with value: 0.682730008332004 and parameters: {'learning_rate': 0.007571196294028347, 'reg_alpha': 0.5118487754396126, 'reg_lambda': 0.07535656875366373, 'max_depth': 16, 'num_leaves': 1152, 'colsample_bytree': 0.5074614365676878, 'subsample': 0.7841706496482079, 'subsample_freq': 9, 'min_child_samples': 27, 'min_child_weight': 8, 'max_bin': 483}. Best is trial 43 with value: 0.6822046404624154.\u001b[0m\n",
      "\u001b[32m[I 2021-05-21 08:07:46,519]\u001b[0m Trial 45 finished with value: 0.6822978725273158 and parameters: {'learning_rate': 0.008981927891095778, 'reg_alpha': 0.4391609407611585, 'reg_lambda': 0.0027277261310326498, 'max_depth': 16, 'num_leaves': 880, 'colsample_bytree': 0.4090077806199986, 'subsample': 0.7565418998022788, 'subsample_freq': 9, 'min_child_samples': 35, 'min_child_weight': 5, 'max_bin': 436}. Best is trial 43 with value: 0.6822046404624154.\u001b[0m\n",
      "\u001b[32m[I 2021-05-21 08:14:44,545]\u001b[0m Trial 46 finished with value: 0.6826090987632617 and parameters: {'learning_rate': 0.008948208320608761, 'reg_alpha': 0.4388215363161289, 'reg_lambda': 0.05959994134959983, 'max_depth': 17, 'num_leaves': 994, 'colsample_bytree': 0.41730701029126355, 'subsample': 0.7534998423647274, 'subsample_freq': 9, 'min_child_samples': 14, 'min_child_weight': 7, 'max_bin': 388}. Best is trial 43 with value: 0.6822046404624154.\u001b[0m\n",
      "\u001b[32m[I 2021-05-21 08:21:20,714]\u001b[0m Trial 47 finished with value: 0.682766518799939 and parameters: {'learning_rate': 0.008958760547645477, 'reg_alpha': 0.44815748745001677, 'reg_lambda': 0.007603942091451717, 'max_depth': 17, 'num_leaves': 956, 'colsample_bytree': 0.4039648575132601, 'subsample': 0.7543636877231549, 'subsample_freq': 8, 'min_child_samples': 7, 'min_child_weight': 9, 'max_bin': 384}. Best is trial 43 with value: 0.6822046404624154.\u001b[0m\n",
      "\u001b[32m[I 2021-05-21 08:28:23,243]\u001b[0m Trial 48 finished with value: 0.6824658254001859 and parameters: {'learning_rate': 0.00874775164062699, 'reg_alpha': 0.4745163722001452, 'reg_lambda': 0.043843828378668945, 'max_depth': 16, 'num_leaves': 1089, 'colsample_bytree': 0.45001838261714655, 'subsample': 0.7111832425994218, 'subsample_freq': 7, 'min_child_samples': 13, 'min_child_weight': 8, 'max_bin': 399}. Best is trial 43 with value: 0.6822046404624154.\u001b[0m\n",
      "\u001b[32m[I 2021-05-21 08:34:46,618]\u001b[0m Trial 49 finished with value: 0.6841438198484906 and parameters: {'learning_rate': 0.008646155188240088, 'reg_alpha': 0.5956780618754134, 'reg_lambda': 0.0016951293343230658, 'max_depth': 14, 'num_leaves': 1074, 'colsample_bytree': 0.4497934800866031, 'subsample': 0.7135839506016087, 'subsample_freq': 7, 'min_child_samples': 36, 'min_child_weight': 10, 'max_bin': 368}. Best is trial 43 with value: 0.6822046404624154.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best hyperparameters are:\n",
      " {'learning_rate': 0.008964104825823405, 'reg_alpha': None, 'reg_lambda': None, 'max_depth': -1, 'num_leaves': 1169, 'colsample_bytree': 0.4527040535891135, 'subsample': 0.7802084890324582, 'subsample_freq': 9, 'min_child_samples': 28, 'min_child_weight': 5, 'max_bin': 498, 'objective': 'multiclass', 'n_estimators': 10000, 'lambda_l1': 0.4499590552446543, 'lambda_l2': 0.05979914903878847}\n"
     ]
    }
   ],
   "source": [
    "# sampler = TPESampler(seed=42)\n",
    "# study = optuna.create_study(study_name=\"lgbm_parameter_opt\", direction=\"minimize\", sampler=sampler)\n",
    "# study.optimize(lgb_objective, n_trials=50)\n",
    "\n",
    "# lgb_best_hyperparams = study.best_trial.params\n",
    "# lgb_base_hyperparams = {'objective':'multiclass', 'n_estimators':10000, 'max_depth':-1,\n",
    "#                         'lambda_l1':lgb_best_hyperparams['reg_alpha'],\n",
    "#                         'lambda_l2':lgb_best_hyperparams['reg_lambda'],\n",
    "#                         'reg_alpha':None, 'reg_lambda':None}\n",
    "# lgb_best_hyperparams.update(lgb_base_hyperparams)\n",
    "# print(\"The best hyperparameters are:\\n\", lgb_best_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## optuna 결과\n",
    "# lgb_best_hyperparams = {'learning_rate': 0.008964104825823405, 'reg_alpha': None, 'reg_lambda': None, 'max_depth': -1,\n",
    "#                         'num_leaves': 1169, 'colsample_bytree': 0.4527040535891135, 'subsample': 0.7802084890324582, \n",
    "#                         'subsample_freq': 9, 'min_child_samples': 28, 'min_child_weight': 5, 'max_bin': 498, \n",
    "#                         'objective': 'multiclass', 'n_estimators': 10000, 'lambda_l1': 0.4499590552446543, \n",
    "#                         'lambda_l2': 0.05979914903878847}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyperParameter Tuning (hyperopt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperopt의 metric함수를 StratifiedKFold(cv=5)로 구하기\n",
    "def lgb_score(params):\n",
    "    seed = 91373\n",
    "    print(\"Training with params:\", params)\n",
    "    kfold = StratifiedKFold(n_splits=10, random_state = seed, shuffle = True) # Cross-validation cv=5\n",
    "    cv = np.zeros((train_x.shape[0], 3))\n",
    "\n",
    "    for n, (train_idx, val_idx) in enumerate(kfold.split(train_x, train_y)):\n",
    "\n",
    "        x_train, x_val = train_x.iloc[train_idx], train_x.iloc[val_idx]\n",
    "        y_train, y_val = train_y.iloc[train_idx], train_y.iloc[val_idx]\n",
    "\n",
    "        lgbmodel = LGBMClassifier(**params)\n",
    "                                                                                        # 진행상황 보고싶을때 None을 100으로\n",
    "        lgbmodel.fit(x_train, y_train, eval_set=[(x_val, y_val)], early_stopping_rounds=30, verbose=None) \n",
    "        cv[val_idx, :] = lgbmodel.predict_proba(x_val)\n",
    "    print('multi_logloss:', log_loss(train_y, cv))\n",
    "    return {'loss': log_loss(train_y, cv), 'status': STATUS_OK}\n",
    "\n",
    "    # 시드를 3개로 평균내고 싶을 때 아래 주석 해제\n",
    "# def lgb_score(params):\n",
    "#     print(\"Training with params:\", params)\n",
    "#     lucky_seeds=np.random.randint(1, 10000, 3) # 랜덤으로 시드 3개 생성\n",
    "#     score_list = []\n",
    "#     for i, seed in enumerate(lucky_seeds):\n",
    "#         kfold = StratifiedKFold(n_splits=5, random_state = seed, shuffle = True) # Cross-validation cv=5\n",
    "#         cv = np.zeros((train_x.shape[0], 3))\n",
    "\n",
    "#         for n, (train_idx, val_idx) in enumerate(kfold.split(train_x, train_y)):\n",
    "\n",
    "#             x_train, x_val = train_x.iloc[train_idx], train_x.iloc[val_idx]\n",
    "#             y_train, y_val = train_y.iloc[train_idx], train_y.iloc[val_idx]\n",
    "\n",
    "#             lgbmodel = LGBMClassifier(**params)\n",
    "\n",
    "#             lgbmodel.fit(x_train, y_train, eval_set=[(x_val, y_val)], early_stopping_rounds=30, verbose=None) \n",
    "#             cv[val_idx, :] = lgbmodel.predict_proba(x_val)\n",
    "#         print(f'seed{seed}_multi_logloss:', log_loss(train_y, cv))\n",
    "#         score_list.append(log_loss(train_y, cv))\n",
    "#     return {'loss': np.mean(score_list), 'status': STATUS_OK}\n",
    "\n",
    "# Hyperopt의 범위를 지정해주고 max_evals만큼 반복한 후 최적의 파라미터를 반환\n",
    "def lgb_optimize(random_state=0):\n",
    "\n",
    "    space = {\n",
    "        'learning_rate': hp.quniform('learning_rate', 0.003, 0.009, 0.001),\n",
    "        #'learning_rate' : 0.004,\n",
    "        'num_leaves': scope.int(hp.quniform('num_leaves', 300, 1200, 50)),\n",
    "        #'num_leaves' : 1000,\n",
    "        'min_data_in_leaf' : scope.int(hp.quniform('min_data_in_leaf', 10, 40, 1)),\n",
    "        'min_child_weight': hp.quniform('min_child_weight', 1, 20, 0.001),\n",
    "        #'min_child_weight' : 2,\n",
    "        'subsample': hp.quniform('subsample', 0.5, 1, 0.05),\n",
    "        #'subsample' : 1,\n",
    "        'colsample_bytree': hp.quniform('colsample_bytree', 0.2, 1, 0.01),\n",
    "        #'colsample_bytree' : 0.6,\n",
    "        'reg_alpha': hp.quniform('reg_alpha', 0.01, 1, 0.01),\n",
    "        #'reg_alpha' : 0.94,\n",
    "        'reg_lambda': hp.quniform('reg_lambda', 0.01, 1, 0.01),\n",
    "        #'reg_lambda' : 0.98,\n",
    "        'max_depth': scope.int(hp.quniform('max_depth', 8, 30, 1)),\n",
    "        #'max_depth' : -1,\n",
    "        'n_estimators' : 5000,\n",
    "        'objective' : 'multiclass',\n",
    "        'num_class' : 3,\n",
    "        'seed': 0,\n",
    "    }\n",
    "    # Use the fmin function from Hyperopt to find the best hyperparameters\n",
    "    best = fmin(lgb_score, space, algo=tpe.suggest, \n",
    "                # trials=trials, \n",
    "                max_evals=50)\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgb_best_hyperparams = lgb_optimize()\n",
    "# lgb_base_hyperparams = {'objective':'multiclass', 'num_class':3, 'n_estimators':5000, \n",
    "#                         'num_leaves':int(lgb_best_hyperparams['num_leaves']),\n",
    "#                         'lambda_l1': lgb_best_hyperparams['reg_alpha'], 'lambda_l2': lgb_best_hyperparams['reg_lambda'],\n",
    "#                         'reg_alpha': None, 'reg_lambda': None, 'min_child_samples': None, 'seed':0}\n",
    "# lgb_best_hyperparams.update(lgb_base_hyperparams)\n",
    "# print(\"The best hyperparameters are:\", lgb_best_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lucky_seeds=[100,2019,91373] # Lucky seed 늘려가면서 하기\n",
    "for i, seed in enumerate(lucky_seeds):\n",
    "\n",
    "    kfold = StratifiedKFold(n_splits=10, random_state = seed, shuffle = True) # CV 늘려가면서 하기\n",
    "    cv=np.zeros((train_x.shape[0], 3))\n",
    "    pred_test = np.zeros((test_x.shape[0], 3), dtype=float)\n",
    "    \n",
    "    for n, (train_idx, val_idx) in enumerate(kfold.split(train_x, train_y)):\n",
    "\n",
    "        x_train, x_val = train_x.iloc[train_idx], train_x.iloc[val_idx]\n",
    "        y_train, y_val = train_y.iloc[train_idx], train_y.iloc[val_idx]\n",
    "\n",
    "        lgbmodel = LGBMClassifier(**lgb_best_hyperparams)\n",
    "        \n",
    "        # 직접 파라미터 넣고싶을땐 아래 코드 주석 해제\n",
    "#         lgbmodel = LGBMClassifier(learning_rate=0.004, objective='multiclass', n_estimators=10000, num_leaves=1000, \n",
    "#                                   max_depth=-1, min_child_weight=2, colsample_bytree=0.6, reg_alpha=0.94, reg_lambda=0.98,\n",
    "#                                    n_jobs=-1, random_state=seed)\n",
    "                                                                                    # 진행상황 보고싶을때 None을 100으로\n",
    "        lgbmodel.fit(x_train, y_train, eval_set=[(x_val, y_val)], early_stopping_rounds=30, verbose=None) \n",
    "        \n",
    "        cv[val_idx,:] = lgbmodel.predict_proba(x_val)\n",
    "        #print(f'fold{n+1} multi_logloss: {log_loss(y_val, cv[val_idx, :])}') # Fold마다 점수 체크하려면 주석 해제\n",
    "        pred_test += lgbmodel.predict_proba(test_x) / 10 # CV 바꾸면 이 숫자도 똑같이 바꿔야함\n",
    "    pred_dict['lgb'+str(seed)] = cv\n",
    "    pred_test_dict['lgb'+str(seed)] = pred_test\n",
    "        \n",
    "    print('multi_logloss :', log_loss(train_y, cv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lgbmodels_path = os.listdir('./pred_pkl/')\n",
    "lgbmodels_list = [x for x in lgbmodels_path if x.endswith(\"lgb.pkl\")]\n",
    "assert len(lgbmodels_list) == 15\n",
    "lgb_preds = np.zeros((test_x.shape[0], 3))\n",
    "\n",
    "for m in lgbmodels_list:\n",
    "    lgbmodel = joblib.load('./pred_pkl/'+m)\n",
    "    lgb_preds_proba = lgbmodel.predict_proba(test)\n",
    "    lgb_preds += lgb_preds_proba/15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원핫인코딩된 feature로 만들어주기 **꼭 밑에 코드 실행하고 XGBoost랑 Randomforest 돌리기!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One Hot Encoding Completed\n"
     ]
    }
   ],
   "source": [
    "train_x_oh = train_x_raw.copy()\n",
    "test_x_oh = test_x_raw.copy()\n",
    "\n",
    "object_col = []\n",
    "for col in train_x_oh.columns:\n",
    "    if (train_x_oh[col].dtype.name == 'category'):\n",
    "        object_col.append(col)   \n",
    "enc = OneHotEncoder()\n",
    "enc.fit(train_x_oh.loc[:,object_col])\n",
    "\n",
    "train_onehot_df = pd.DataFrame(enc.transform(train_x_oh.loc[:,object_col]).toarray(), \n",
    "             columns=enc.get_feature_names(object_col))\n",
    "train_x_oh.drop(object_col, axis=1, inplace=True)\n",
    "train_x_oh = pd.concat([train_x_oh, train_onehot_df], axis=1)    \n",
    "\n",
    "test_onehot_df = pd.DataFrame(enc.transform(test_x_oh.loc[:,object_col]).toarray(), \n",
    "             columns=enc.get_feature_names(object_col))\n",
    "test_x_oh.drop(object_col, axis=1, inplace=True)\n",
    "test_x_oh = pd.concat([test_x_oh, test_onehot_df], axis=1)\n",
    "\n",
    "train_x = train_x_oh.copy()\n",
    "test_x = test_x_oh.copy()\n",
    "\n",
    "print('One Hot Encoding Completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyperParameter Tuning (optuna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_objective(trial: Trial) -> float:\n",
    "    params_xgb = {\n",
    "        \"random_state\": 91373,\n",
    "        \"verbose\": None,\n",
    "        \"num_class\": 3,\n",
    "        \"objective\": \"multi:softprob\",\n",
    "        \"eval_metric\": \"mlogloss\",\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.003, 0.009),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-3, 1),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-3, 1),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 8, 80),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.3, 1.0),\n",
    "        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.3, 1.0),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.3, 1.0),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0.3, 1.0),\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 20),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 200, 500),\n",
    "    }\n",
    "    \n",
    "    # CV=10으로 튜닝\n",
    "    \n",
    "    seed = 91373\n",
    "    kfold = StratifiedKFold(n_splits=10, random_state = seed, shuffle = True) # Cross-validation cv=5\n",
    "    cv = np.zeros((train_x.shape[0], 3))\n",
    "\n",
    "    for n, (train_idx, val_idx) in enumerate(kfold.split(train_x, train_y)):\n",
    "\n",
    "        x_train, x_val = train_x.iloc[train_idx], train_x.iloc[val_idx]\n",
    "        y_train, y_val = train_y.iloc[train_idx], train_y.iloc[val_idx]\n",
    "        \n",
    "        dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "        dvalid = xgb.DMatrix(x_val, label=y_val)\n",
    "        watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n",
    "                                                                                            # 진행상황 보고싶을때 None을 100으로\n",
    "        xgbmodel = xgb.train(params_xgb, dtrain, 100000, watchlist, early_stopping_rounds=30, verbose_eval=None)\n",
    "\n",
    "        cv[val_idx, :] = xgbmodel.predict(dvalid)\n",
    "        #print(f'fold{n+1} multi_logloss: {log_loss(y_val, cv[val_idx, :])}') # Fold마다 점수 체크하려면 주석 해제\n",
    "    print('multi_logloss:', log_loss(train_y, cv))\n",
    "\n",
    "    \n",
    "    return log_loss(train_y, cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-05-21 08:34:46,948]\u001b[0m A new study created in memory with name: xgbm_parameter_opt\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi_logloss: 0.7059662480866681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-05-21 09:41:31,682]\u001b[0m Trial 0 finished with value: 0.7059662480866681 and parameters: {'learning_rate': 0.005247240713084175, 'reg_alpha': 0.9507635921035062, 'reg_lambda': 0.7322619478695936, 'max_depth': 21, 'colsample_bytree': 0.40921304830970556, 'colsample_bylevel': 0.40919616423534183, 'subsample': 0.3406585285177396, 'gamma': 0.9063233020424546, 'min_child_weight': 13, 'max_bin': 413}. Best is trial 0 with value: 0.7059662480866681.\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-e6977207687f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msampler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTPESampler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mstudy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstudy_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"xgbm_parameter_opt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdirection\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"minimize\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msampler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mstudy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxgb_objective\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mxgb_best_hyperparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstudy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_trial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\optuna\\study.py\u001b[0m in \u001b[0;36moptimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    398\u001b[0m             )\n\u001b[0;32m    399\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 400\u001b[1;33m         _optimize(\n\u001b[0m\u001b[0;32m    401\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\optuna\\_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m             _optimize_sequential(\n\u001b[0m\u001b[0;32m     67\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\optuna\\_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m             \u001b[0mtrial\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m             \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\optuna\\_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m         \u001b[0mvalue_or_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    218\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m         \u001b[1;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-20-037f16d3d369>\u001b[0m in \u001b[0;36mxgb_objective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mwatchlist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdvalid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'valid'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m                                                                                             \u001b[1;31m# 진행상황 보고싶을때 None을 100으로\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[0mxgbmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams_xgb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwatchlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose_eval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0mcv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgbmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdvalid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks)\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[0mBooster\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtrained\u001b[0m \u001b[0mbooster\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m     \"\"\"\n\u001b[1;32m--> 189\u001b[1;33m     bst = _train_internal(params, dtrain,\n\u001b[0m\u001b[0;32m    190\u001b[0m                           \u001b[0mnum_boost_round\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_boost_round\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m                           \u001b[0mevals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks, evals_result, maximize, verbose_eval, early_stopping_rounds)\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbefore_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m         \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mafter_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   1494\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1495\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1496\u001b[1;33m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,\n\u001b[0m\u001b[0;32m   1497\u001b[0m                                                     \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1498\u001b[0m                                                     dtrain.handle))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sampler = TPESampler(seed=42)\n",
    "study = optuna.create_study(study_name=\"xgbm_parameter_opt\", direction=\"minimize\", sampler=sampler)\n",
    "study.optimize(xgb_objective, n_trials=50)\n",
    "\n",
    "xgb_best_hyperparams = study.best_trial.params\n",
    "xgb_base_hyperparams = {'objective':'multi:softprob', \"num_class\": 3, \"eval_metric\": \"mlogloss\", \"random_state\": 91373}\n",
    "xgb_best_hyperparams.update(xgb_base_hyperparams)\n",
    "print(\"The best hyperparameters are:\\n\", xgb_best_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyperParameter Tuning (hyperopt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperopt의 metric함수를 StratifiedKFold(cv=5)로 구하기\n",
    "def xgb_score(params):\n",
    "    seed = 91373\n",
    "    print(\"Training with params:\", params)\n",
    "    kfold = StratifiedKFold(n_splits=10, random_state = seed, shuffle = True) # Cross-validation cv=5\n",
    "    cv = np.zeros((train_x.shape[0], 3))\n",
    "\n",
    "    for n, (train_idx, val_idx) in enumerate(kfold.split(train_x, train_y)):\n",
    "\n",
    "        x_train, x_val = train_x.iloc[train_idx], train_x.iloc[val_idx]\n",
    "        y_train, y_val = train_y.iloc[train_idx], train_y.iloc[val_idx]\n",
    "\n",
    "        dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "        dvalid = xgb.DMatrix(x_val, label=y_val)\n",
    "        watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n",
    "                                                                                    # 진행상황 보고싶을때 None을 100으로\n",
    "        xgbmodel = xgb.train(params, dtrain, 100000, watchlist, early_stopping_rounds=30, verbose_eval=None)\n",
    "        cv[val_idx, :] = xgbmodel.predict(dvalid)\n",
    "    print('multi_logloss:', log_loss(train_y, cv))\n",
    "    return {'loss': log_loss(train_y, cv), 'status': STATUS_OK}\n",
    "\n",
    "    # 시드를 3개로 평균내고 싶을 때 아래 주석 해제\n",
    "# def xgb_score(params):\n",
    "#     print(\"Training with params:\", params)\n",
    "#     np.random.seed(0)\n",
    "#     lucky_seeds=np.random.randint(1, 10000, 3) # 랜덤으로 시드 3개 생성\n",
    "#     score_list = []\n",
    "#     for i, seed in enumerate(lucky_seeds):\n",
    "#         kfold = StratifiedKFold(n_splits=5, random_state = seed, shuffle = True) # Cross-validation cv=5\n",
    "#         cv = np.zeros((train_x.shape[0], 3))\n",
    "\n",
    "#         for n, (train_idx, val_idx) in enumerate(kfold.split(train_x, train_y)):\n",
    "\n",
    "#             x_train, x_val = train_x.iloc[train_idx], train_x.iloc[val_idx]\n",
    "#             y_train, y_val = train_y.iloc[train_idx], train_y.iloc[val_idx]\n",
    "\n",
    "#             dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "#             dvalid = xgb.DMatrix(x_val, label=y_val)\n",
    "#             watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n",
    "#                                                                                         # 진행상황 보고싶을때 None을 100으로\n",
    "#             xgbmodel = xgb.train(params, dtrain, 100000, watchlist, early_stopping_rounds=30, verbose_eval=None)\n",
    "#             cv[val_idx, :] = xgbmodel.predict(dvalid)\n",
    "#         print(f'seed{seed}_multi_logloss:', log_loss(train_y, cv))\n",
    "#         score_list.append(log_loss(train_y, cv))\n",
    "#     return {'loss': np.mean(score_list), 'status': STATUS_OK}\n",
    "\n",
    "# Hyperopt의 범위를 지정해주고 max_evals만큼 반복한 후 최적의 파라미터를 반환\n",
    "def xgb_optimize(random_state=0):\n",
    "    \n",
    "    space = {\n",
    "        'eta': hp.quniform('eta', 0.003, 0.009, 0.001),\n",
    "        #'eta' : 0.005,\n",
    "        'max_depth':  hp.choice('max_depth', np.arange(8, 30, dtype=int)),\n",
    "        #'max_depth' : 100,\n",
    "        'num_leaves':  hp.choice('num_leaves', np.arange(64, 1200, dtype=int)),\n",
    "        'min_child_weight': hp.quniform('min_child_weight', 1, 20, 1),\n",
    "        #'min_child_weight': 4,\n",
    "        'subsample': hp.quniform('subsample', 0.3, 1, 0.05),\n",
    "        #'subsample': 0.72,\n",
    "        'gamma': hp.quniform('gamma', 0.5, 1, 0.05),\n",
    "        #'gamma': 0.7,\n",
    "        'colsample_bytree': hp.quniform('colsample_bytree', 0.1, 1, 0.05),\n",
    "        #'colsample_bytree': 0.45,\n",
    "        'colsample_bylevel': hp.quniform('colsample_bylevel', 0.1, 1, 0.05),\n",
    "        #'colsample_bylevel': 0.15,\n",
    "        'alpha' :  hp.quniform('alpha', 0.01, 1, 0.01),\n",
    "        'lambda' :  hp.quniform('lambda', 0.01, 1, 0.01),\n",
    "        'max_delta_step': scope.int(hp.quniform('max_delta_step', 0, 10, 1)),\n",
    "        'eval_metric': 'mlogloss',\n",
    "        'objective' : 'multi:softprob',\n",
    "        'num_class' : 3,\n",
    "        'seed': 0,\n",
    "    }\n",
    "    # Use the fmin function from Hyperopt to find the best hyperparameters\n",
    "    best = fmin(xgb_score, space, algo=tpe.suggest, \n",
    "                # trials=trials, \n",
    "                max_evals=50)\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb_best_hyperparams = xgb_optimize()\n",
    "# xgb_base_hyperparams = {'objective':'multi:softprob', 'eval_metric': 'mlogloss', 'num_class':3,\n",
    "#                         'max_delta_step':int(xgb_best_hyperparams['max_delta_step']), 'seed':0}\n",
    "# xgb_best_hyperparams.update(xgb_base_hyperparams)\n",
    "# print(\"The best hyperparameters are:\", xgb_best_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lucky_seeds=[0, 100, 91373] # 늘려가면서\n",
    "xgtest = xgb.DMatrix(test_x)\n",
    "for i, seed in enumerate(lucky_seeds):\n",
    "\n",
    "    kfold = StratifiedKFold(n_splits=10, random_state = seed, shuffle = True) # 늘려가면서\n",
    "    cv = np.zeros((train.shape[0], 3))\n",
    "    pred_test = np.zeros((test_x.shape[0], 3), dtype=float)\n",
    "\n",
    "    for n, (train_idx, val_idx) in enumerate(kfold.split(train_x, train_y)):\n",
    "        \n",
    "        x_train, x_val = train_x.iloc[train_idx], train_x.iloc[val_idx]\n",
    "        y_train, y_val = train_y.iloc[train_idx], train_y.iloc[val_idx]\n",
    "        \n",
    "        dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "        dvalid = xgb.DMatrix(x_val, label=y_val)\n",
    "        watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n",
    "        \n",
    "        # 직접 파라미터 넣고싶을땐 아래 코드 주석 해제\n",
    "#         xgb_best_hyperparams = {'colsample_bylevel': 0.15, 'colsample_bytree': 0.45, 'eta': 0.005, 'eval_metric': 'mlogloss', \n",
    "#          'gamma': 0.7, 'max_depth': 100, 'min_child_weight': 4, 'num_class': 3, \n",
    "#          'objective': 'multi:softprob', 'seed': 0, 'subsample': 0.72}\n",
    "                                                                                            # 진행상황 보고싶을때 None을 100으로\n",
    "        rfmodel = xgb.train(xgb_best_hyperparams, dtrain, 100000, watchlist, early_stopping_rounds=30, verbose_eval=None)\n",
    "\n",
    "        cv[val_idx, :] = xgbmodel.predict(dvalid)\n",
    "        #print(f'fold{n+1} multi_logloss: {log_loss(y_val, cv[val_idx, :])}') # Fold마다 점수 체크하려면 주석 해제\n",
    "        pred_test += xgbmodel.predict(xgtest) / 10 # CV 바꾸면 이 숫자도 똑같이 바꿔야함\n",
    "        \n",
    "    pred_dict['xgb'+str(seed)] = cv\n",
    "    pred_test_dict['xgb'+str(seed)] = pred_test\n",
    "    print('multi_logloss:', log_loss(train_y, cv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xgbmodels_path = os.listdir('./pred_pkl/')\n",
    "xgbmodels_list = [x for x in xgbmodels_path if x.endswith(\"xgb.pkl\")]\n",
    "assert len(xgbmodels_list) == 15\n",
    "xgb_preds = np.zeros((test_x.shape[0], 3))\n",
    "xgtest = xgb.DMatrix(test_X)\n",
    "\n",
    "for m in xgbmodels_list:\n",
    "    xgbmodel = joblib.load('./pred_pkl/'+m)\n",
    "    xgb_preds_proba = xgbmodel.predict_proba(xgtest\n",
    "       xgb_preds += xgb_preds_proba/15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_objective(trial: Trial) -> float:\n",
    "    params_rf = {\n",
    "        \"random_state\": 91373,\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 500, 1500),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 10, 100),\n",
    "        \"max_features\": trial.suggest_float(\"max_features\", 0.15, 1.0),\n",
    "        \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 1, 20),\n",
    "        \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 20),\n",
    "        \"max_samples\": trial.suggest_float(\"max_samples\", 0.5, 1),\n",
    "        \"criterion\": trial.suggest_categorical(\"criterion\", [\"gini\", \"entropy\"]),\n",
    "        'n_jobs': -1, \n",
    "    }\n",
    "    \n",
    "    # CV=10으로 튜닝\n",
    "    \n",
    "    seed = 91373\n",
    "    kfold = StratifiedKFold(n_splits=10, random_state = seed, shuffle = True) # Cross-validation cv=5\n",
    "    cv = np.zeros((train_x.shape[0], 3))\n",
    "\n",
    "    for n, (train_idx, val_idx) in enumerate(kfold.split(train_x, train_y)):\n",
    "\n",
    "        x_train, x_val = train_x.iloc[train_idx], train_x.iloc[val_idx]\n",
    "        y_train, y_val = train_y.iloc[train_idx], train_y.iloc[val_idx]\n",
    "\n",
    "        rfmodel = RandomForestClassifier(**params_rf)\n",
    "                                                                                        # 진행상황 보고싶을때 None을 100으로\n",
    "        rfmodel.fit(x_train, y_train) \n",
    "        cv[val_idx, :] = rfmodel.predict_proba(x_val)\n",
    "    # print('multi_logloss:', log_loss(train_y, cv))\n",
    "\n",
    "    \n",
    "    return log_loss(train_y, cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = TPESampler(seed=42)\n",
    "study = optuna.create_study(study_name=\"rf_parameter_opt\", direction=\"minimize\", sampler=sampler)\n",
    "study.optimize(rf_objective, n_trials=50)\n",
    "\n",
    "rf_best_hyperparams = study.best_trial.params\n",
    "rf_base_hyperparams = {'bootstrap': True, 'n_jobs': -1, \"random_state\": 91373}\n",
    "rf_best_hyperparams.update(rf_base_hyperparams)\n",
    "print(\"The best hyperparameters are:\\n\", rf_best_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 seeds, 10 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lucky_seeds=[42,2019,91373] # 늘려가면서\n",
    "for i, seed in enumerate(lucky_seeds):\n",
    "\n",
    "    kfold = StratifiedKFold(n_splits=10, random_state = seed, shuffle = True) # 늘려가면서\n",
    "    cv = np.zeros((train_x.shape[0], 3))\n",
    "    pred_test = np.zeros((test_x.shape[0], 3), dtype=float)\n",
    "    \n",
    "    for n, (train_idx, val_idx) in enumerate(kfold.split(train_x, train_y)):\n",
    "        \n",
    "        x_train, x_val = train_x.iloc[train_idx], train_x.iloc[val_idx]\n",
    "        y_train, y_val = train_y.iloc[train_idx], train_y.iloc[val_idx]\n",
    "\n",
    "        rfmodel = RandomForestClassifier(**rf_best_hyperparams)\n",
    "        \n",
    "        rfmodel.fit(x_train, y_train)\n",
    "        #joblib.dump(rfmodel, f'./pred_pkl/RF_{n+1}_fold_{seed}_seed_rf.pkl')\n",
    "        \n",
    "        cv[val_idx, :] = rfmodel.predict_proba(x_val)      \n",
    "        # print(f'fold{n+1} multi_logloss: {log_loss(y_val, cv[val_idx, :])}')\n",
    "        pred_test += rfmodel.predict_proba(test_x) / 10 # CV 바꾸면 이 숫자도 똑같이 바꿔야함\n",
    "        \n",
    "    pred_dict['rf'+str(seed)] = cv\n",
    "    pred_test_dict['rf'+str(seed)] = pred_test\n",
    "    print('multi_logloss :', log_loss(train_y, cv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rfmodels_path = os.listdir('./pred_pkl/')\n",
    "rfmodels_list = [x for x in rfmodels_path if x.endswith(\"rf.pkl\")]\n",
    "assert len(rfmodels_list) == 15\n",
    "rf_preds = np.zeros((test_x.shape[0], 3))\n",
    "\n",
    "for m in rfmodels_list:\n",
    "    rfmodel = joblib.load('./pred_pkl/'+m)\n",
    "    rf_preds_proba = rfmodel.predict_proba(test_x)\n",
    "    rf_preds += rf_preds_proba/15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) Catboost (성능X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_objective(trial: Trial) -> float:\n",
    "    cat_params = {\n",
    "        'loss_function': 'MultiClass',\n",
    "        'bootstrap_type': trial.suggest_categorical('bootstrap_type', ['Bayesian', 'Bernoulli', 'Poisson']),\n",
    "        'task_type': 'GPU',\n",
    "        'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-3, 10.0),\n",
    "        'max_bin': trial.suggest_int('max_bin', 200, 400),\n",
    "        #'rsm': trial.suggest_uniform('rsm', 0.3, 1.0),\n",
    "        'subsample': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n",
    "        'learning_rate': trial.suggest_uniform('learning_rate', 0.006, 0.018),\n",
    "        'n_estimators':  25000,\n",
    "        'max_depth': trial.suggest_categorical('max_depth', [7,10,14,16]),\n",
    "        'random_state': trial.suggest_categorical('random_state', [24, 48,2020]),\n",
    "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 300),\n",
    "    }\n",
    "    \n",
    "    # CV=10으로 튜닝\n",
    "    \n",
    "    seed = 91373\n",
    "    kfold = StratifiedKFold(n_splits=10, random_state = seed, shuffle = True)\n",
    "    cv = np.zeros((train_x.shape[0], 3))\n",
    "\n",
    "    for n, (train_idx, val_idx) in enumerate(kfold.split(train_x, train_y)):\n",
    "\n",
    "        x_train, x_val = train_x.iloc[train_idx], train_x.iloc[val_idx]\n",
    "        y_train, y_val = train_y.iloc[train_idx], train_y.iloc[val_idx]\n",
    "        \n",
    "        catmodel = CatBoostClassifier(**cat_params)                                       # 진행상황 보고싶을때 False를 100으로\n",
    "        catmodel.fit(x_train, y_train, eval_set=[(x_val,y_val)], early_stopping_rounds=30, verbose=False)\n",
    "\n",
    "        cv[val_idx, :] = catmodel.predict_proba(x_val)\n",
    "        #print(f'fold{n+1} multi_logloss: {log_loss(y_val, cv[val_idx, :])}') # Fold마다 점수 체크하려면 주석 해제\n",
    "    print('multi_logloss:', log_loss(train_y, cv))\n",
    "\n",
    "    \n",
    "    return log_loss(train_y, cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-05-21 19:29:59,470]\u001b[0m A new study created in memory with name: cat_parameter_opt\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-ff4b93c340cb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msampler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTPESampler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mstudy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstudy_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"cat_parameter_opt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdirection\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"minimize\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msampler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mstudy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcat_objective\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mcat_best_hyperparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstudy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_trial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\optuna\\study.py\u001b[0m in \u001b[0;36moptimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    398\u001b[0m             )\n\u001b[0;32m    399\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 400\u001b[1;33m         _optimize(\n\u001b[0m\u001b[0;32m    401\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\optuna\\_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m             _optimize_sequential(\n\u001b[0m\u001b[0;32m     67\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\optuna\\_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m             \u001b[0mtrial\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m             \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\optuna\\_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m         \u001b[0mvalue_or_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    218\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m         \u001b[1;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-26-dbf62b55981d>\u001b[0m in \u001b[0;36mcat_objective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mcatmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCatBoostClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mcat_params\u001b[0m\u001b[1;33m)\u001b[0m                                       \u001b[1;31m# 진행상황 보고싶을때 False를 100으로\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0mcatmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mcv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcatmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\catboost\\core.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, cat_features, text_features, embedding_features, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model)\u001b[0m\n\u001b[0;32m   4537\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_is_classification_objective\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss_function'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4538\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4539\u001b[1;33m         self._fit(X, y, cat_features, text_features, embedding_features, None, sample_weight, None, None, None, None, baseline, use_best_model,\n\u001b[0m\u001b[0;32m   4540\u001b[0m                   \u001b[0meval_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogging_level\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumn_description\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose_eval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric_period\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4541\u001b[0m                   silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\catboost\\core.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, cat_features, text_features, embedding_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model)\u001b[0m\n\u001b[0;32m   1916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1917\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mlog_fixup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplot_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_get_train_dir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1918\u001b[1;33m             self._train(\n\u001b[0m\u001b[0;32m   1919\u001b[0m                 \u001b[0mtrain_pool\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1920\u001b[0m                 \u001b[0mtrain_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"eval_sets\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\catboost\\core.py\u001b[0m in \u001b[0;36m_train\u001b[1;34m(self, train_pool, test_pool, params, allow_clear_pool, init_model)\u001b[0m\n\u001b[0;32m   1364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1365\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_pool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_pool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_clear_pool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1366\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_object\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_pool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_pool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_clear_pool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_object\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0minit_model\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1367\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_trained_model_attributes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1368\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._CatBoost._train\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._CatBoost._train\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sampler = TPESampler(seed=42)\n",
    "study = optuna.create_study(study_name=\"cat_parameter_opt\", direction=\"minimize\", sampler=sampler)\n",
    "study.optimize(cat_objective, n_trials=50)\n",
    "\n",
    "cat_best_hyperparams = study.best_trial.params\n",
    "cat_base_hyperparams = {'loss_function': 'MultiClass', 'n_estimators': 100000, 'task_type': 'GPU'}\n",
    "cat_best_hyperparams.update(cat_base_hyperparams)\n",
    "print(\"The best hyperparameters are:\\n\", cat_best_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lucky_seeds=[42,2019,91373]\n",
    "for i, seed in enumerate(lucky_seeds):\n",
    "\n",
    "    kfold = KFold(n_splits=10, random_state = seed, shuffle = True)\n",
    "    cv = np.zeros((train.shape[0], 3))\n",
    "    pred_test = np.zeros((test_x.shape[0], 3), dtype=float)\n",
    "    \n",
    "    for n, (train_idx, val_idx) in enumerate(kfold.split(train)):\n",
    "        \n",
    "        x_train, x_val = train_x.iloc[train_idx], train_x.iloc[val_idx]\n",
    "        y_train, y_val = train_y.iloc[train_idx], train_y.iloc[val_idx]\n",
    "        _train = Pool(x_train, label=y_train)\n",
    "        _valid = Pool(x_val, label=y_val)\n",
    "\n",
    "        catmodel =  CatBoostClassifier(**cat_best_hyperparams)\n",
    "        \n",
    "        catmodel.fit(_train, eval_set=_valid, use_best_model=True, verbose=2000)\n",
    "        #joblib.dump(rfmodel, f'./pred_pkl/RF_{n+1}_fold_{seed}_seed_rf.pkl')\n",
    "        \n",
    "        cv[val_idx, :] = catmodel.predict_proba(x_val)        \n",
    "        pred_test += catmodel.predict_proba(test_x) / 10\n",
    "        \n",
    "    pred_dict['cat'+str(i+1)] = cv\n",
    "    pred_test_dict['cat'+str(i+1)] = pred_test\n",
    "    print('multi_logloss :', log_loss(true, cv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) Stacking (AutoLGB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 27features = 3seed(42, 2019, 91373) x 3model(lgb, xgb, rf) x 3class(0, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pred_dict_1={}\n",
    "new_pred_test_dict_1={}\n",
    "for i in range(len(pred_dict)):\n",
    "    if log_loss(train_y, list(pred_dict.values())[i])<0.68:\n",
    "        new_pred_dict_1[list(pred_dict.keys())[i]]=list(pred_dict.values())[i]\n",
    "        new_pred_test_dict_1[list(pred_test_dict.keys())[i]]=list(pred_test_dict.values())[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pred_dict_2={}\n",
    "new_pred_test_dict_2={}\n",
    "for i in range(len(pred_dict)):\n",
    "    if log_loss(train_y, list(pred_dict.values())[i])<0.69:\n",
    "        new_pred_dict_2[list(pred_dict.keys())[i]]=list(pred_dict.values())[i]\n",
    "        new_pred_test_dict_2[list(pred_test_dict.keys())[i]]=list(pred_test_dict.values())[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./pkl/new_pred_dict_1.pickle', 'wb') as fw:\n",
    "    pickle.dump(new_pred_dict_1, fw)\n",
    "    \n",
    "with open('./pkl/new_pred_test_dict_1.pickle', 'wb') as fw:\n",
    "    pickle.dump(new_pred_test_dict_1, fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./pkl/new_pred_dict_2.pickle', 'wb') as fw:\n",
    "    pickle.dump(new_pred_dict_2, fw)\n",
    "    \n",
    "with open('./pkl/new_pred_test_dict_2.pickle', 'wb') as fw:\n",
    "    pickle.dump(new_pred_test_dict_2, fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./pkl/new_pred_dict_1.pickle', 'rb') as fw:\n",
    "    new_pred_dict_1 = pickle.load(fw)\n",
    "\n",
    "with open('./pkl/new_pred_test_dict_2.pickle', 'rb') as fw:\n",
    "    new_pred_test_dict_2 = pickle.load(fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./pkl/new_pred_dict_2.pickle', 'rb') as fw:\n",
    "    new_pred_dict_2 = pickle.load(fw)\n",
    "\n",
    "with open('./pkl/new_pred_test_dict_2.pickle', 'rb') as fw:\n",
    "    new_pred_test_dict_2 = pickle.load(fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(np.hstack([x for _, x in new_pred_dict_2.items()]))\n",
    "X_test = pd.DataFrame(np.hstack([x for _, x in new_pred_test_dict_2.items()]))\n",
    "\n",
    "pred = np.zeros((X_train.shape[0], 3), dtype=float)\n",
    "pred_test = np.zeros((X_test.shape[0], 3), dtype=float)\n",
    "#kfold = KFold(n_splits=5, random_state = seed, shuffle = True)\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "for i_cv, (i_trn, i_val) in enumerate(cv.split(X_train, train_y)):\n",
    "    if i_cv == 0:\n",
    "        clf = AutoLGB(objective='multiclass', metric='multi_logloss', params={'num_class': 3}, \n",
    "                      feature_selection=False, n_est=10000)\n",
    "        clf.tune(X_train.iloc[i_trn], train_y[i_trn])\n",
    "        n_best = clf.n_best\n",
    "        features = clf.features\n",
    "        params = clf.params\n",
    "        print(f'best iteration: {n_best}')\n",
    "        print(f'selected features ({len(features)}): {features}')        \n",
    "        print(params)\n",
    "        clf.fit(X_train.iloc[i_trn], train_y[i_trn])\n",
    "    else:\n",
    "        train_data = lgb.Dataset(X_train[features].iloc[i_trn], label=train_y[i_trn])\n",
    "        clf = lgb.train(params, train_data, n_best, verbose_eval=100)\n",
    "    \n",
    "    pred[i_val] = clf.predict(X_train[features].iloc[i_val])\n",
    "    pred_test += clf.predict(X_test[features]) / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'CV Log Loss: {log_loss(train_y, pred):.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack1_test = pred_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking (XGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_objective(trial: Trial) -> float:\n",
    "    params_xgb = {\n",
    "        \"random_state\": 91373,\n",
    "        \"verbose\": None,\n",
    "        \"num_class\": 3,\n",
    "        \"objective\": \"multi:softprob\",\n",
    "        \"eval_metric\": \"mlogloss\",\n",
    "        \"tree_method\": \"gpu_hist\",\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.003, 0.009),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-3, 1),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-3, 1),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 8, 30),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.3, 1.0),\n",
    "        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.3, 1.0),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.3, 1.0),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0.3, 1.0),\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 20),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 200, 500),\n",
    "    }\n",
    "    \n",
    "    # CV=10으로 튜닝\n",
    "    \n",
    "    seed = 91373\n",
    "    kfold = StratifiedKFold(n_splits=10, random_state = seed, shuffle = True) # Cross-validation cv=5\n",
    "    cv = np.zeros((train_x.shape[0], 3))\n",
    "\n",
    "    for n, (train_idx, val_idx) in enumerate(kfold.split(train_x, train_y)):\n",
    "\n",
    "        x_train, x_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_train, y_val = train_y.iloc[train_idx], train_y.iloc[val_idx]\n",
    "        \n",
    "        dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "        dvalid = xgb.DMatrix(x_val, label=y_val)\n",
    "        watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n",
    "                                                                                            # 진행상황 보고싶을때 None을 100으로\n",
    "        stack_xgbmodel = xgb.train(params_xgb, dtrain, 100000, watchlist, early_stopping_rounds=30, verbose_eval=None)\n",
    "\n",
    "        cv[val_idx, :] = stack_xgbmodel.predict(dvalid)\n",
    "        #print(f'fold{n+1} multi_logloss: {log_loss(y_val, cv[val_idx, :])}') # Fold마다 점수 체크하려면 주석 해제\n",
    "    print('multi_logloss:', log_loss(train_y, cv))\n",
    "\n",
    "    return log_loss(train_y, cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = TPESampler(seed=42)\n",
    "study = optuna.create_study(study_name=\"stack_parameter_opt\", direction=\"minimize\", sampler=sampler)\n",
    "study.optimize(stack_objective, n_trials=50)\n",
    "\n",
    "stack_best_hyperparams = study.best_trial.params\n",
    "stack_base_hyperparams = {'objective':'multi:softprob', \"num_class\": 3, \"eval_metric\": \"mlogloss\", \n",
    "                         \"tree_method\": \"gpu_hist\", \"random_state\": 91373}\n",
    "stack_best_hyperparams.update(stack_base_hyperparams)\n",
    "print(\"The best hyperparameters are:\\n\", stack_best_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.zeros((X_train.shape[0], 3), dtype=float)\n",
    "pred_test = np.zeros((X_test.shape[0], 3), dtype=float)\n",
    "kfold = StratifiedKFold(n_splits=10, random_state = 91373, shuffle = True)\n",
    "\n",
    "for n, (train_idx, val_idx) in enumerate(kfold.split(X_train, train_y)):\n",
    "    x_train, x_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_train, y_val = train_y.iloc[train_idx], train_y.iloc[val_idx]\n",
    "\n",
    "    dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "    dvalid = xgb.DMatrix(x_val, label=y_val)\n",
    "    watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n",
    "                                                                                        # 진행상황 보고싶을때 None을 100으로\n",
    "    stack_xgbmodel = xgb.train(params_xgb, dtrain, 100000, watchlist, early_stopping_rounds=30, verbose_eval=None)\n",
    "    \n",
    "    pred[val_idx] = stack_xgbmodel.predict(X_train.iloc[val_idx])\n",
    "    pred_test += stack_xgbmodel.predict(X_test) / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'CV Log Loss: {log_loss(train_y, pred):.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack2_test = pred_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 결과 제출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = sample_submission.copy()\n",
    "submission.iloc[:, 1:] = pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optuna 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_contour(study, params=['learning_rate',\n",
    "#                             'max_depth',\n",
    "#                             'num_leaves',\n",
    "#                             'colsample_bytree',\n",
    "#                             'subsample',\n",
    "#                             'min_child_weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_optimization_history(study) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_parallel_coordinate(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_slice(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optuna.visualization.plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForest GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {'max_depth': [55, 60, 65] # 튜닝할 파라미터 삽입\n",
    "#             }\n",
    "\n",
    "# rf_clf = RandomForestClassifier(random_state = 0, n_estimators = 1000, \n",
    "#                                 min_samples_leaf=2, min_samples_split=2,\n",
    "#                                 criterion='entropy', n_jobs = -1)\n",
    "# grid_cv = GridSearchCV(rf_clf, param_grid = params, cv = 5, n_jobs = -1)\n",
    "# grid_cv.fit(df_train, y)\n",
    "\n",
    "# print('최적 하이퍼 파라미터: ', grid_cv.best_params_)\n",
    "# print('최고 예측 정확도: {:.4f}'.format(grid_cv.best_score_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
