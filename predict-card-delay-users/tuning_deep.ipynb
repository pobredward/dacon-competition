{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just to Get Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory optimization from 15.34MB to  4.04MB (73.7% reduction)\n",
      "Memory optimization from  5.80MB to  1.53MB (73.7% reduction)\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import joblib\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from itertools import combinations\n",
    "from eli5.permutation_importance import get_score_importances\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "from sklearn import cluster\n",
    "\n",
    "import optuna\n",
    "from optuna import Trial\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.visualization import plot_contour, plot_optimization_history\n",
    "from optuna.visualization import plot_parallel_coordinate, plot_slice, plot_param_importances\n",
    "from hyperopt.pyll.base import scope\n",
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "from catboost import Pool, CatBoostClassifier\n",
    "from kaggler.model import AutoLGB\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, GridSearchCV, StratifiedShuffleSplit, train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, log_loss\n",
    "import random\n",
    "\n",
    "train = pd.read_csv('data/train.csv').drop(['index', 'FLAG_MOBIL'], axis=1).fillna('NAN')\n",
    "test = pd.read_csv('data/test.csv').drop(['index', 'FLAG_MOBIL'], axis=1).fillna('NAN')\n",
    "sample_submission = pd.read_csv('data/sample_submission.csv')\n",
    "\n",
    "# train데이터와 test데이터 변수를 함께 조정하기 위해 병합\n",
    "merge_data = pd.concat([train, test], axis = 0)\n",
    "\n",
    "# DAYS_BIRTH\n",
    "merge_data['DAYS_BIRTH_month']=np.floor((-merge_data['DAYS_BIRTH'])/30)-(\n",
    "    (np.floor((-merge_data['DAYS_BIRTH'])/30)/12).astype(int)*12)\n",
    "merge_data['DAYS_BIRTH_week']=np.floor((-merge_data['DAYS_BIRTH'])/7)-(\n",
    "    (np.floor((-merge_data['DAYS_BIRTH'])/7)/4).astype(int)*4)\n",
    "\n",
    "# DAYS_EMPLOYED\n",
    "merge_data['DAYS_EMPLOYED_month']=np.floor((-merge_data['DAYS_EMPLOYED'])/30)-(\n",
    "    (np.floor((-merge_data['DAYS_EMPLOYED'])/30)/12).astype(int)*12)\n",
    "merge_data['DAYS_EMPLOYED_week']=np.floor((-merge_data['DAYS_EMPLOYED'])/7)-(\n",
    "    (np.floor((-merge_data['DAYS_EMPLOYED'])/7)/4).astype(int)*4)\n",
    "\n",
    "# before_EMPLOYED\n",
    "merge_data['before_EMPLOYED']=merge_data['DAYS_BIRTH']-merge_data['DAYS_EMPLOYED']\n",
    "merge_data['before_EMPLOYED_month']=np.floor((-merge_data['before_EMPLOYED'])/30)-(\n",
    "    (np.floor((-merge_data['before_EMPLOYED'])/30)/12).astype(int)*12)\n",
    "merge_data['before_EMPLOYED_week']=np.floor((-merge_data['before_EMPLOYED'])/7)-(\n",
    "    (np.floor((-merge_data['before_EMPLOYED'])/7)/4).astype(int)*4)\n",
    "\n",
    "# DAYS_BIRTH / Income\n",
    "merge_data['DAYS_BIRTH_month/income_total'] = merge_data['DAYS_BIRTH_month'] / merge_data['income_total']\n",
    "merge_data['DAYS_BIRTH_week/income_total'] = merge_data['DAYS_BIRTH_week'] / merge_data['income_total']\n",
    "\n",
    "# DAYS_EMPLOYED / Income\n",
    "merge_data['DAYS_EMPLOYED_month/income_total'] = merge_data['DAYS_EMPLOYED_month'] / merge_data['income_total']\n",
    "merge_data['DAYS_EMPLOYED_week/income_total'] = merge_data['DAYS_EMPLOYED_week'] / merge_data['income_total']\n",
    "\n",
    "# before_EMPLOYED / Income\n",
    "merge_data['before_EMPLOYED/income_total'] = merge_data['before_EMPLOYED'] / merge_data['income_total']\n",
    "merge_data['before_EMPLOYED_month/income_total'] = merge_data['before_EMPLOYED_month'] / merge_data['income_total']\n",
    "merge_data['before_EMPLOYED_week/income_total'] = merge_data['before_EMPLOYED_week'] / merge_data['income_total']\n",
    "\n",
    "# Income / Family\n",
    "merge_data['income_total/family_size'] = merge_data['income_total'] / merge_data['family_size']\n",
    "\n",
    "merge_data['child_num/income_total'] = merge_data['child_num'] / merge_data['income_total']\n",
    "merge_data['family_size/income_total'] = merge_data['family_size'] / merge_data['income_total']\n",
    "merge_data['DAYS_BIRTH/income_total'] = merge_data['DAYS_BIRTH'] / merge_data['income_total']\n",
    "merge_data['DAYS_EMPLOYED/income_total'] = merge_data['DAYS_EMPLOYED'] / merge_data['income_total']\n",
    "merge_data['DAYS_EMPLOYED/DAYS_BIRTH'] =  merge_data['DAYS_EMPLOYED'] / merge_data['DAYS_BIRTH']\n",
    "\n",
    "# Income skewed-data\n",
    "merge_data['income_total'] = np.log1p(merge_data['income_total'])\n",
    "# merge_data['log_income_total'] = np.log(merge_data['income_total'])\n",
    "# merge_data['sqrt_income_total'] = np.sqrt(merge_data['income_total'])\n",
    "# merge_data['boxcox_income_total'] = stats.boxcox(merge_data['income_total'])[0]\n",
    "\n",
    "merge_data = merge_data.fillna(-999)\n",
    "train = merge_data[merge_data['credit'] != -999]\n",
    "test = merge_data[merge_data['credit'] == -999]\n",
    "test.drop('credit', axis = 1, inplace = True)\n",
    "\n",
    "train_cols = list(train.columns); train_cols.remove('credit'); train_cols.append('credit')\n",
    "train = train[train_cols]\n",
    "\n",
    "train_x = train.drop(['credit'], axis=1) # 데이터 나누기\n",
    "train_y = train['credit']\n",
    "test_x = test.copy()\n",
    "\n",
    "object_col = []\n",
    "for col in train_x.columns:\n",
    "    if (train_x[col].dtype == 'object'):\n",
    "        object_col.append(col)   \n",
    "enc = OneHotEncoder()\n",
    "enc.fit(train_x.loc[:,object_col])\n",
    "\n",
    "train_onehot_df = pd.DataFrame(enc.transform(train_x.loc[:,object_col]).toarray(), \n",
    "             columns=enc.get_feature_names(object_col))\n",
    "train_x.drop(object_col, axis=1, inplace=True)\n",
    "train_x = pd.concat([train_x, train_onehot_df], axis=1)    \n",
    "\n",
    "test_onehot_df = pd.DataFrame(enc.transform(test_x.loc[:,object_col]).toarray(), \n",
    "             columns=enc.get_feature_names(object_col))\n",
    "test_x.drop(object_col, axis=1, inplace=True)\n",
    "test_x = pd.concat([test_x, test_onehot_df], axis=1)\n",
    "\n",
    "def reduce_mem_usage(data):\n",
    "    numerics = ['int8', 'int16', 'int32', 'int64', 'float32', 'float64']\n",
    "    start_memory = data.memory_usage().sum() / 1024**2    \n",
    "    for col in data.columns:\n",
    "        col_type = data[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = data[col].min()\n",
    "            c_max = data[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    data[col] = data[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    data[col] = data[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    data[col] = data[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    data[col] = data[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    data[col] = data[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    data[col] = data[col].astype(np.float32)\n",
    "                else:\n",
    "                    data[col] = data[col].astype(np.float64)    \n",
    "    end_memory = data.memory_usage().sum() / 1024**2\n",
    "    print('Memory optimization from {:5.2f}MB to {:5.2f}MB ({:.1f}% reduction)'\n",
    "          .format(start_memory, end_memory, 100 * (start_memory - end_memory) / start_memory))\n",
    "    return data\n",
    "train_x = reduce_mem_usage(train_x)\n",
    "test_x = reduce_mem_usage(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_tabnet.multitask import TabNetMultiTaskClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch_tabnet\n",
      "  Cloning https://github.com/dreamquark-ai/tabnet.git (to revision develop) to c:\\users\\archi\\appdata\\local\\temp\\pip-install-qiobt6tx\\pytorch-tabnet_96e11b6826c043a38bf0e4d66ce564ac\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Requirement already satisfied: torch<2.0,>=1.2 in c:\\users\\archi\\anaconda3\\lib\\site-packages (from pytorch_tabnet) (1.7.1)\n",
      "Requirement already satisfied: scipy>1.4 in c:\\users\\archi\\anaconda3\\lib\\site-packages (from pytorch_tabnet) (1.5.2)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.36 in c:\\users\\archi\\anaconda3\\lib\\site-packages (from pytorch_tabnet) (4.50.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17 in c:\\users\\archi\\anaconda3\\lib\\site-packages (from pytorch_tabnet) (1.19.2)\n",
      "Requirement already satisfied: scikit_learn>0.21 in c:\\users\\archi\\anaconda3\\lib\\site-packages (from pytorch_tabnet) (0.23.2)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\archi\\anaconda3\\lib\\site-packages (from scikit_learn>0.21->pytorch_tabnet) (0.17.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\archi\\anaconda3\\lib\\site-packages (from scikit_learn>0.21->pytorch_tabnet) (2.1.0)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\archi\\anaconda3\\lib\\site-packages (from torch<2.0,>=1.2->pytorch_tabnet) (3.7.4.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install  \"git+https://github.com/dreamquark-ai/tabnet.git@develop#egg=pytorch_tabnet\" --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cat_idxs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-9363bee3b9a8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m clf = TabNetMultiTaskClassifier(n_steps=1,\n\u001b[1;32m----> 2\u001b[1;33m                                 \u001b[0mcat_idxs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcat_idxs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m                                 \u001b[0mcat_dims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcat_dims\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                                 \u001b[0mcat_emb_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                                 \u001b[0moptimizer_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cat_idxs' is not defined"
     ]
    }
   ],
   "source": [
    "clf = TabNetMultiTaskClassifier(n_steps=1,\n",
    "                                cat_idxs=cat_idxs,\n",
    "                                cat_dims=cat_dims,\n",
    "                                cat_emb_dim=1,\n",
    "                                optimizer_fn=torch.optim.Adam,\n",
    "                                optimizer_params=dict(lr=2e-2),\n",
    "                                scheduler_params={\"step_size\":50,\n",
    "                                                  \"gamma\":0.9},\n",
    "                                scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "                                mask_type='entmax', \n",
    "                                lambda_sparse=0, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
