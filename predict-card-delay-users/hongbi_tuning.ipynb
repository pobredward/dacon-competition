{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "monetary-insertion",
   "metadata": {},
   "source": [
    "# Just to Get Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "optional-manchester",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category Encoding Completed\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import joblib\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from itertools import combinations\n",
    "from eli5.permutation_importance import get_score_importances\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "from sklearn import cluster\n",
    "\n",
    "import optuna\n",
    "from optuna import Trial\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.visualization import plot_contour, plot_optimization_history\n",
    "from optuna.visualization import plot_parallel_coordinate, plot_slice, plot_param_importances\n",
    "from hyperopt.pyll.base import scope\n",
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "from catboost import Pool, CatBoostClassifier\n",
    "from kaggler.model import AutoLGB\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, GridSearchCV, StratifiedShuffleSplit, train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, log_loss\n",
    "import random\n",
    "\n",
    "train = pd.read_csv('data/train.csv').drop(['index', 'FLAG_MOBIL'], axis=1).fillna('NAN')\n",
    "test = pd.read_csv('data/test.csv').drop(['index', 'FLAG_MOBIL'], axis=1).fillna('NAN')\n",
    "sample_submission = pd.read_csv('data/sample_submission.csv')\n",
    "\n",
    "# train데이터와 test데이터 변수를 함께 조정하기 위해 병합\n",
    "merge_data = pd.concat([train, test], axis = 0)\n",
    "\n",
    "# DAYS_BIRTH\n",
    "merge_data['DAYS_BIRTH_month']=np.floor((-merge_data['DAYS_BIRTH'])/30)-(\n",
    "    (np.floor((-merge_data['DAYS_BIRTH'])/30)/12).astype(int)*12)\n",
    "merge_data['DAYS_BIRTH_week']=np.floor((-merge_data['DAYS_BIRTH'])/7)-(\n",
    "    (np.floor((-merge_data['DAYS_BIRTH'])/7)/4).astype(int)*4)\n",
    "\n",
    "# DAYS_EMPLOYED\n",
    "merge_data['DAYS_EMPLOYED_month']=np.floor((-merge_data['DAYS_EMPLOYED'])/30)-(\n",
    "    (np.floor((-merge_data['DAYS_EMPLOYED'])/30)/12).astype(int)*12)\n",
    "merge_data['DAYS_EMPLOYED_week']=np.floor((-merge_data['DAYS_EMPLOYED'])/7)-(\n",
    "    (np.floor((-merge_data['DAYS_EMPLOYED'])/7)/4).astype(int)*4)\n",
    "\n",
    "# before_EMPLOYED\n",
    "merge_data['before_EMPLOYED']=merge_data['DAYS_BIRTH']-merge_data['DAYS_EMPLOYED']\n",
    "merge_data['before_EMPLOYED_month']=np.floor((-merge_data['before_EMPLOYED'])/30)-(\n",
    "    (np.floor((-merge_data['before_EMPLOYED'])/30)/12).astype(int)*12)\n",
    "merge_data['before_EMPLOYED_week']=np.floor((-merge_data['before_EMPLOYED'])/7)-(\n",
    "    (np.floor((-merge_data['before_EMPLOYED'])/7)/4).astype(int)*4)\n",
    "\n",
    "# DAYS_BIRTH / Income\n",
    "merge_data['DAYS_BIRTH_month/income_total'] = merge_data['DAYS_BIRTH_month'] / merge_data['income_total']\n",
    "merge_data['DAYS_BIRTH_week/income_total'] = merge_data['DAYS_BIRTH_week'] / merge_data['income_total']\n",
    "\n",
    "# DAYS_EMPLOYED / Income\n",
    "merge_data['DAYS_EMPLOYED_month/income_total'] = merge_data['DAYS_EMPLOYED_month'] / merge_data['income_total']\n",
    "merge_data['DAYS_EMPLOYED_week/income_total'] = merge_data['DAYS_EMPLOYED_week'] / merge_data['income_total']\n",
    "\n",
    "# before_EMPLOYED / Income\n",
    "merge_data['before_EMPLOYED/income_total'] = merge_data['before_EMPLOYED'] / merge_data['income_total']\n",
    "merge_data['before_EMPLOYED_month/income_total'] = merge_data['before_EMPLOYED_month'] / merge_data['income_total']\n",
    "merge_data['before_EMPLOYED_week/income_total'] = merge_data['before_EMPLOYED_week'] / merge_data['income_total']\n",
    "\n",
    "# Income / Family\n",
    "merge_data['income_total/family_size'] = merge_data['income_total'] / merge_data['family_size']\n",
    "\n",
    "merge_data['child_num/income_total'] = merge_data['child_num'] / merge_data['income_total']\n",
    "merge_data['family_size/income_total'] = merge_data['family_size'] / merge_data['income_total']\n",
    "merge_data['DAYS_BIRTH/income_total'] = merge_data['DAYS_BIRTH'] / merge_data['income_total']\n",
    "merge_data['DAYS_EMPLOYED/income_total'] = merge_data['DAYS_EMPLOYED'] / merge_data['income_total']\n",
    "merge_data['DAYS_EMPLOYED/DAYS_BIRTH'] =  merge_data['DAYS_EMPLOYED'] / merge_data['DAYS_BIRTH']\n",
    "\n",
    "# Income skewed-data\n",
    "merge_data['income_total'] = np.log1p(merge_data['income_total'])\n",
    "# merge_data['log_income_total'] = np.log(merge_data['income_total'])\n",
    "# merge_data['sqrt_income_total'] = np.sqrt(merge_data['income_total'])\n",
    "# merge_data['boxcox_income_total'] = stats.boxcox(merge_data['income_total'])[0]\n",
    "\n",
    "merge_data = merge_data.fillna(-999)\n",
    "train = merge_data[merge_data['credit'] != -999]\n",
    "test = merge_data[merge_data['credit'] == -999]\n",
    "test.drop('credit', axis = 1, inplace = True)\n",
    "\n",
    "train_cols = list(train.columns); train_cols.remove('credit'); train_cols.append('credit')\n",
    "train = train[train_cols]\n",
    "\n",
    "train_cat = train.copy()\n",
    "test_cat = test.copy()\n",
    "\n",
    "for col in train_cat.columns:\n",
    "    if train_cat[col].dtypes=='object':\n",
    "        train_cat[col] =  train_cat[col].astype('category')\n",
    "        test_cat[col] =  test_cat[col].astype('category')\n",
    "    \n",
    "print('Category Encoding Completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "pretty-selection",
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_lgbmodel(train, verbose=True):\n",
    "    \n",
    "    train_x = train.drop(['credit'], axis=1)\n",
    "    train_y = train['credit']\n",
    "    \n",
    "    np.random.seed(0)\n",
    "    lucky_seeds=np.random.randint(1, 10000, 5)\n",
    "    score_list = []\n",
    "    \n",
    "    for i, seed in enumerate(lucky_seeds):\n",
    "\n",
    "        kfold = StratifiedKFold(n_splits=5, random_state = seed, shuffle = True) # CV 늘려가면서 하기\n",
    "        cv=np.zeros((train_x.shape[0], 3))\n",
    "\n",
    "        for n, (train_idx, val_idx) in enumerate(kfold.split(train_x, train_y)):\n",
    "\n",
    "            x_train, x_val = train_x.iloc[train_idx], train_x.iloc[val_idx]\n",
    "            y_train, y_val = train_y.iloc[train_idx], train_y.iloc[val_idx]\n",
    "\n",
    "            lgbmodel = LGBMClassifier(objective='multiclass', n_estimators=10000, random_state=seed)\n",
    "            lgbmodel.fit(x_train, y_train, eval_set=[(x_val, y_val)], early_stopping_rounds=30, verbose=None) \n",
    "\n",
    "            cv[val_idx,:] = lgbmodel.predict_proba(x_val)\n",
    "        if verbose==True:\n",
    "            print(f'multi_logloss: {log_loss(train_y, cv):.4f}')\n",
    "        score_list.append(log_loss(train_y, cv))\n",
    "    print(f'Average Logloss: {np.mean(score_list):.4f}')\n",
    "    return np.mean(score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ruled-constitutional",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Logloss: 0.7107\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba3fb5b6100f4c4fa22450e5112f17d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster: 3 Average Logloss: 0.7106\n",
      "cluster: 4 Average Logloss: 0.7110\n",
      "cluster: 5 Average Logloss: 0.7109\n",
      "cluster: 6 Average Logloss: 0.7106\n",
      "cluster: 7 Average Logloss: 0.7116\n",
      "cluster: 8 Average Logloss: 0.7106\n",
      "cluster: 9 Average Logloss: 0.7105\n",
      "cluster: 10 Average Logloss: 0.7106\n",
      "cluster: 11 Average Logloss: 0.7107\n"
     ]
    }
   ],
   "source": [
    "# income_type, occyp_type, income_total을 이용하여 클러스터링하기 위해 따로 원 핫 인코딩\n",
    "kmeans_train = train[['occyp_type', 'income_type', 'income_total']]\n",
    "kmeans_test = test[['occyp_type', 'income_type', 'income_total']]\n",
    "object_col = []\n",
    "for col in kmeans_train.columns:\n",
    "    if kmeans_train[col].dtype == 'object':\n",
    "        object_col.append(col)\n",
    "        \n",
    "enc = OneHotEncoder()\n",
    "enc.fit(kmeans_train.loc[:,object_col])\n",
    "\n",
    "train_onehot_df = pd.DataFrame(enc.transform(kmeans_train.loc[:,object_col]).toarray(), \n",
    "             columns=enc.get_feature_names(object_col))\n",
    "kmeans_train.drop(object_col, axis=1, inplace=True)\n",
    "kmeans_train = pd.concat([kmeans_train, train_onehot_df], axis=1)\n",
    "\n",
    "test_onehot_df = pd.DataFrame(enc.transform(kmeans_test.loc[:,object_col]).toarray(),\n",
    "             columns=enc.get_feature_names(object_col))\n",
    "kmeans_test.drop(object_col, axis=1, inplace=True)\n",
    "kmeans_test = pd.concat([kmeans_test, test_onehot_df], axis=1)\n",
    "\n",
    "base_lgbmodel(train_cat, verbose=False)\n",
    "# n_clusters를 3부터 10까지 진행하여 하나씩 성능 체크\n",
    "score_list = {}\n",
    "k_means_train_total_df = pd.DataFrame()\n",
    "k_means_test_total_df = pd.DataFrame()\n",
    "for i in tqdm(range(3, 12)):\n",
    "    train_cat = train.copy()\n",
    "    test_cat = test.copy()\n",
    "\n",
    "    for col in train_cat.columns:\n",
    "        if train_cat[col].dtypes=='object':\n",
    "            train_cat[col] =  train_cat[col].astype('category')\n",
    "            test_cat[col] =  test_cat[col].astype('category')\n",
    "\n",
    "    # n_cluster를 늘려가며 클러스터링 진행\n",
    "    k_means_train_df = pd.DataFrame()\n",
    "    k_means_test_df = pd.DataFrame()\n",
    "    k_means = cluster.KMeans(n_clusters=i)\n",
    "    k_means.fit(kmeans_train)\n",
    "    k_means_train_df = pd.concat([k_means_train_df, pd.DataFrame(k_means.labels_, columns=[f'cluster_{i}'])], axis=1)\n",
    "    k_means_train_total_df = pd.concat([k_means_train_total_df, pd.DataFrame(k_means.labels_, columns=[f'cluster_{i}'])], axis=1)\n",
    "    k_means.fit(kmeans_test)\n",
    "    k_means_test_df = pd.concat([k_means_test_df, pd.DataFrame(k_means.labels_, columns=[f'cluster_{i}'])], axis=1)\n",
    "    k_means_test_total_df = pd.concat([k_means_test_total_df, pd.DataFrame(k_means.labels_, columns=[f'cluster_{i}'])], axis=1)\n",
    "\n",
    "    train_cat = pd.concat([train_cat, k_means_train_df], axis=1)\n",
    "    test_cat = pd.concat([test_cat, k_means_test_df], axis=1)\n",
    "\n",
    "    # 클러스터링 결과를 category 타입으로 변경\n",
    "    for col in train_cat.columns:\n",
    "        if train_cat[col].dtypes=='int32':\n",
    "            train_cat[col] =  train_cat[col].astype('category')\n",
    "            test_cat[col] =  test_cat[col].astype('category')\n",
    "    print(f'cluster: {i}', end=' ')\n",
    "    score_list[f'cluster_{i}'] = base_lgbmodel(train_cat, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "preceding-panel",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cluster_9']\n",
      "Average Logloss: 0.7105\n",
      "0.7105158199710967\n"
     ]
    }
   ],
   "source": [
    "# 성능이 좋아지는 클러스터링 개수로만 피처 추출\n",
    "\n",
    "train_cat = train.copy()\n",
    "test_cat = test.copy()\n",
    "\n",
    "for col in train_cat.columns:\n",
    "    if train_cat[col].dtypes=='object':\n",
    "        train_cat[col] =  train_cat[col].astype('category')\n",
    "        test_cat[col] =  test_cat[col].astype('category')\n",
    "        \n",
    "n = 1 # 성능이 좋아지는 클러스터 개수별로 정렬한 후 앞의 n개만 추출\n",
    "train_cat = pd.concat([train_cat, k_means_train_total_df.loc[:, sorted(score_list, key=lambda x: score_list[x])[:n]]], axis=1)\n",
    "test_cat = pd.concat([test_cat, k_means_test_total_df.loc[:, sorted(score_list, key=lambda x: score_list[x])[:n]]], axis=1)\n",
    "\n",
    "for col in train_cat.columns:\n",
    "    if train_cat[col].dtypes=='int32':\n",
    "        train_cat[col] =  train_cat[col].astype('category')\n",
    "        test_cat[col] =  test_cat[col].astype('category')\n",
    "        \n",
    "print(k_means_train_total_df.loc[:, sorted(score_list, key=lambda x: score_list[x])[:n]].columns.tolist())\n",
    "print(base_lgbmodel(train_cat, verbose=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "answering-mistake",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Encoding Completed\n"
     ]
    }
   ],
   "source": [
    "train_x = train_cat.drop(['credit'], axis=1) # 데이터 나누기\n",
    "train_y = train_cat['credit']\n",
    "test_x = test_cat.copy()\n",
    "\n",
    "train_x_raw = train_x.copy() # 카테고리 인코딩 된 데이터 저장해두기\n",
    "test_x_raw = test_x.copy()\n",
    "\n",
    "train_x_oh = train_x_raw.copy()\n",
    "test_x_oh = test_x_raw.copy()\n",
    "\n",
    "enc = LabelEncoder()\n",
    "for col in train_x_oh.columns:\n",
    "    if (train_x_oh[col].dtype.name=='category'):\n",
    "        train_x_oh[col] = enc.fit_transform(train_x_oh[col])\n",
    "        test_x_oh[col] = enc.fit_transform(test_x_oh[col])\n",
    "\n",
    "train_x = train_x_oh.copy()\n",
    "test_x = test_x_oh.copy()\n",
    "        \n",
    "print('Label Encoding Completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "representative-liechtenstein",
   "metadata": {},
   "source": [
    "# 3. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "digital-history",
   "metadata": {},
   "source": [
    "## (2) XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "robust-auditor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_objective(trial: Trial) -> float:\n",
    "    params_xgb = {\n",
    "        \"random_state\": 91373,\n",
    "        \"verbose\": None,\n",
    "        \"num_class\": 3,\n",
    "        \"objective\": \"multi:softprob\",\n",
    "        \"eval_metric\": \"mlogloss\",\n",
    "        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 1e-1),\n",
    "        \"reg_alpha\": trial.suggest_uniform(\"reg_alpha\", 0.1, 1.0), # default=0\n",
    "        \"reg_lambda\": trial.suggest_uniform(\"reg_lambda\", 0.1, 1.0), # default=1\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 8, 100),\n",
    "        \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0.3, 1.0), # default=0\n",
    "        \"colsample_bylevel\": trial.suggest_uniform(\"colsample_bylevel\", 0.3, 1.0),\n",
    "        \"subsample\": trial.suggest_uniform(\"subsample\", 0.3, 1.0), # default=1\n",
    "        \"gamma\": trial.suggest_uniform(\"gamma\", 0.0, 1.0), # default=0\n",
    "        \"min_child_weight\": trial.suggest_uniform(\"min_child_weight\", 1, 10), # default=1\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 200, 500),\n",
    "    }\n",
    "    \n",
    "    # CV=10으로 튜닝\n",
    "    \n",
    "    seed = 91373\n",
    "    kfold = StratifiedKFold(n_splits=5, random_state = seed, shuffle = True) # Cross-validation cv=5\n",
    "    cv = np.zeros((train_x.shape[0], 3))\n",
    "\n",
    "    for n, (train_idx, val_idx) in enumerate(kfold.split(train_x, train_y)):\n",
    "\n",
    "        x_train, x_val = train_x.iloc[train_idx], train_x.iloc[val_idx]\n",
    "        y_train, y_val = train_y.iloc[train_idx], train_y.iloc[val_idx]\n",
    "        \n",
    "        dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "        dvalid = xgb.DMatrix(x_val, label=y_val)\n",
    "        watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n",
    "                                                                                            # 진행상황 보고싶을때 None을 100으로\n",
    "        xgbmodel = xgb.train(params_xgb, dtrain, 100000, watchlist, early_stopping_rounds=30, verbose_eval=None)\n",
    "\n",
    "        cv[val_idx, :] = xgbmodel.predict(dvalid)\n",
    "        #print(f'fold{n+1} multi_logloss: {log_loss(y_val, cv[val_idx, :])}') # Fold마다 점수 체크하려면 주석 해제\n",
    "    print('multi_logloss:', log_loss(train_y, cv))\n",
    "\n",
    "    \n",
    "    return log_loss(train_y, cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlimited-wildlife",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-05-21 22:24:27,860]\u001b[0m A new study created in memory with name: xgbm_parameter_opt\u001b[0m\n",
      "\u001b[32m[I 2021-05-21 22:35:12,258]\u001b[0m Trial 0 finished with value: 0.6929546781435697 and parameters: {'learning_rate': 0.005611516415334507, 'reg_alpha': 0.9556428757689246, 'reg_lambda': 0.7587945476302645, 'max_depth': 63, 'colsample_bytree': 0.40921304830970556, 'colsample_bylevel': 0.40919616423534183, 'subsample': 0.3406585285177396, 'gamma': 0.8661761457749352, 'min_child_weight': 6.41003510568888, 'max_bin': 413}. Best is trial 0 with value: 0.6929546781435697.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi_logloss: 0.6929546781435697\n",
      "multi_logloss: 0.6880207372839158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-05-21 23:38:18,507]\u001b[0m Trial 1 finished with value: 0.6880207372839158 and parameters: {'learning_rate': 0.0010994335574766201, 'reg_alpha': 0.9729188669457949, 'reg_lambda': 0.8491983767203796, 'max_depth': 27, 'colsample_bytree': 0.42727747704497043, 'colsample_bylevel': 0.4283831568974037, 'subsample': 0.5129695700716763, 'gamma': 0.5247564316322378, 'min_child_weight': 4.887505167779041, 'max_bin': 287}. Best is trial 1 with value: 0.6880207372839158.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi_logloss: 0.6912587633821059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-05-21 23:45:31,217]\u001b[0m Trial 2 finished with value: 0.6912587633821059 and parameters: {'learning_rate': 0.01673808578875214, 'reg_alpha': 0.22554447458683766, 'reg_lambda': 0.3629301836816964, 'max_depth': 42, 'colsample_bytree': 0.619248988951925, 'colsample_bylevel': 0.8496231729751094, 'subsample': 0.4397716475108518, 'gamma': 0.5142344384136116, 'min_child_weight': 6.331731119758382, 'max_bin': 213}. Best is trial 1 with value: 0.6880207372839158.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi_logloss: 0.6956150873940341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-05-21 23:52:23,930]\u001b[0m Trial 3 finished with value: 0.6956150873940341 and parameters: {'learning_rate': 0.016409286730647923, 'reg_alpha': 0.2534717113185624, 'reg_lambda': 0.1585464336867516, 'max_depth': 96, 'colsample_bytree': 0.9759424231521916, 'colsample_bylevel': 0.8658781436815228, 'subsample': 0.5132296384213595, 'gamma': 0.09767211400638387, 'min_child_weight': 7.158097238609412, 'max_bin': 332}. Best is trial 1 with value: 0.6880207372839158.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi_logloss: 0.6887046047523031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-05-22 00:42:31,317]\u001b[0m Trial 4 finished with value: 0.6887046047523031 and parameters: {'learning_rate': 0.0017541893487450805, 'reg_alpha': 0.5456592191001431, 'reg_lambda': 0.13094966900369656, 'max_depth': 92, 'colsample_bytree': 0.48114598712001183, 'colsample_bylevel': 0.7637655990477874, 'subsample': 0.5181977532625877, 'gamma': 0.5200680211778108, 'min_child_weight': 5.920392514089517, 'max_bin': 255}. Best is trial 1 with value: 0.6880207372839158.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi_logloss: 0.7041641754710253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-05-22 00:44:30,411]\u001b[0m Trial 5 finished with value: 0.7041641754710253 and parameters: {'learning_rate': 0.08692991511139551, 'reg_alpha': 0.7976195410250031, 'reg_lambda': 0.9455490474077702, 'max_depth': 91, 'colsample_bytree': 0.7185299851677596, 'colsample_bylevel': 0.9453119645161818, 'subsample': 0.36194475143634364, 'gamma': 0.1959828624191452, 'min_child_weight': 1.4070456001948426, 'max_bin': 297}. Best is trial 1 with value: 0.6880207372839158.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi_logloss: 0.6881634903126366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-05-22 00:59:58,491]\u001b[0m Trial 6 finished with value: 0.6881634903126366 and parameters: {'learning_rate': 0.0059890036722543005, 'reg_alpha': 0.34421412859650635, 'reg_lambda': 0.8458637582367364, 'max_depth': 41, 'colsample_bytree': 0.49665415678116653, 'colsample_bylevel': 0.6798872582107739, 'subsample': 0.3986469574823338, 'gamma': 0.8021969807540397, 'min_child_weight': 1.6709557931179373, 'max_bin': 497}. Best is trial 1 with value: 0.6880207372839158.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi_logloss: 0.7011356461978966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-05-22 01:04:22,837]\u001b[0m Trial 7 finished with value: 0.7011356461978966 and parameters: {'learning_rate': 0.03503398491158688, 'reg_alpha': 0.2788441133807552, 'reg_lambda': 0.10496990541124217, 'max_depth': 83, 'colsample_bytree': 0.7948001406933319, 'colsample_bylevel': 0.810305017628691, 'subsample': 0.8398892426801621, 'gamma': 0.07404465173409036, 'min_child_weight': 4.226191556898454, 'max_bin': 234}. Best is trial 1 with value: 0.6880207372839158.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi_logloss: 0.6919789569259247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-05-22 01:06:43,876]\u001b[0m Trial 8 finished with value: 0.6919789569259247 and parameters: {'learning_rate': 0.05323617594751501, 'reg_alpha': 0.6609683141448022, 'reg_lambda': 0.39780822236738433, 'max_depth': 13, 'colsample_bytree': 0.5176876252009635, 'colsample_bylevel': 0.5276283254187228, 'subsample': 0.8107243248366449, 'gamma': 0.6375574713552131, 'min_child_weight': 8.98491468318694, 'max_bin': 342}. Best is trial 1 with value: 0.6880207372839158.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi_logloss: 0.6911392137495636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-05-22 02:14:47,398]\u001b[0m Trial 9 finished with value: 0.6911392137495636 and parameters: {'learning_rate': 0.0017345566642360942, 'reg_alpha': 0.7419203085006955, 'reg_lambda': 0.7847065437552077, 'max_depth': 60, 'colsample_bytree': 0.8396770259681927, 'colsample_bylevel': 0.6456569174550735, 'subsample': 0.6659129805673958, 'gamma': 0.42754101835854963, 'min_child_weight': 1.2287721406968566, 'max_bin': 232}. Best is trial 1 with value: 0.6880207372839158.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi_logloss: 0.6884152315747006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-05-22 03:37:58,211]\u001b[0m Trial 10 finished with value: 0.6884152315747006 and parameters: {'learning_rate': 0.0011861280274792088, 'reg_alpha': 0.9394437550391957, 'reg_lambda': 0.6220267779791122, 'max_depth': 9, 'colsample_bytree': 0.3183939067172853, 'colsample_bylevel': 0.31683962344795524, 'subsample': 0.6866426081502623, 'gamma': 0.28905925958373585, 'min_child_weight': 3.7017252053363916, 'max_bin': 456}. Best is trial 1 with value: 0.6880207372839158.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi_logloss: 0.6879423131853242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-05-22 04:01:16,554]\u001b[0m Trial 11 finished with value: 0.6879423131853242 and parameters: {'learning_rate': 0.004206532980963756, 'reg_alpha': 0.4514536883096462, 'reg_lambda': 0.9929500483908029, 'max_depth': 35, 'colsample_bytree': 0.30780184898590235, 'colsample_bylevel': 0.5812711164046102, 'subsample': 0.5433061416984076, 'gamma': 0.8902749106451219, 'min_child_weight': 3.1409011104154554, 'max_bin': 500}. Best is trial 11 with value: 0.6879423131853242.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi_logloss: 0.6871078953920429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-05-22 04:32:32,573]\u001b[0m Trial 12 finished with value: 0.6871078953920429 and parameters: {'learning_rate': 0.003489284542034327, 'reg_alpha': 0.4613924070000139, 'reg_lambda': 0.9815577829160963, 'max_depth': 26, 'colsample_bytree': 0.3023022043705436, 'colsample_bylevel': 0.49566527844569797, 'subsample': 0.5705347594156813, 'gamma': 0.9965752411747821, 'min_child_weight': 3.5609422007512035, 'max_bin': 400}. Best is trial 12 with value: 0.6871078953920429.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi_logloss: 0.6870282626719132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-05-22 05:05:12,679]\u001b[0m Trial 13 finished with value: 0.6870282626719132 and parameters: {'learning_rate': 0.004044758550598604, 'reg_alpha': 0.4704732458166859, 'reg_lambda': 0.9491590094936627, 'max_depth': 25, 'colsample_bytree': 0.3217328044396752, 'colsample_bylevel': 0.5573812769299912, 'subsample': 0.9824454271977694, 'gamma': 0.9457622119062251, 'min_child_weight': 3.0072345852011932, 'max_bin': 405}. Best is trial 13 with value: 0.6870282626719132.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi_logloss: 0.6861771694808935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-05-22 05:36:02,238]\u001b[0m Trial 14 finished with value: 0.6861771694808935 and parameters: {'learning_rate': 0.002959825478357582, 'reg_alpha': 0.11607982947588863, 'reg_lambda': 0.9795398190355702, 'max_depth': 18, 'colsample_bytree': 0.30103021381501593, 'colsample_bylevel': 0.4747661585014905, 'subsample': 0.9659280483713507, 'gamma': 0.9839251109984457, 'min_child_weight': 2.820300947268634, 'max_bin': 414}. Best is trial 14 with value: 0.6861771694808935.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "sampler = TPESampler(seed=42)\n",
    "xgb_study = optuna.create_study(study_name=\"xgbm_parameter_opt\", direction=\"minimize\", sampler=sampler)\n",
    "xgb_study.optimize(xgb_objective, n_trials=30)\n",
    "\n",
    "xgb_best_hyperparams = xgb_study.best_trial.params\n",
    "xgb_base_hyperparams = {'objective':'multi:softprob', \"num_class\": 3, \"eval_metric\": \"mlogloss\", \"random_state\": 91373}\n",
    "xgb_best_hyperparams.update(xgb_base_hyperparams)\n",
    "print(\"The best hyperparameters are:\\n\", xgb_best_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preliminary-procedure",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 나오자마자 복붙해서 카톡방에 올려서 따로 저장해두기!! (파라미터값, logg값 둘 다)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "positive-journey",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_optimization_history(xgb_study) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "false-breeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_param_importances(xgb_study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "periodic-diamond",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_slice(xgb_study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raised-simulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위 그래프들은 지우지 말고 그대로 커밋해주기 (결과 보고 범위 다시 지정해야함)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordered-finding",
   "metadata": {},
   "source": [
    "## (4) Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "municipal-princess",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_objective(trial: Trial) -> float:\n",
    "    cat_params = {\n",
    "        'loss_function': 'MultiClass',\n",
    "        'bootstrap_type': trial.suggest_categorical('bootstrap_type', ['Bayesian', 'Bernoulli', 'MVS']), # Poisson은 gpu만\n",
    "        'l2_leaf_reg': trial.suggest_loguniform(\"l2_leaf_reg\", 1e-3, 100),\n",
    "        'random_strength': trial.suggest_loguniform(\"random_strength\", 1e-2, 100),\n",
    "        'max_bin': trial.suggest_int('max_bin', 150, 500), # border_counts\n",
    "        'colsample_bylevel': trial.suggest_uniform('colsample_bylevel', 0.01, 1.0), # rsm # gpu not support\n",
    "        'learning_rate':trial.suggest_loguniform(\"learning_rate\", 1e-3, 1e-1),\n",
    "        'n_estimators':  50000,\n",
    "        'depth': trial.suggest_int('depth', 5, 15),\n",
    "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 100), # min_child_samples\n",
    "     }\n",
    "    if cat_params[\"bootstrap_type\"] == \"Bayesian\":\n",
    "        cat_params[\"bagging_temperature\"] = trial.suggest_loguniform('bagging_temperature', 1e-2, 100)\n",
    "    else:\n",
    "        cat_params[\"subsample\"] = trial.suggest_uniform(\"subsample\", 0.1, 1.0)\n",
    "        \n",
    "    \n",
    "    \n",
    "    # CV=10으로 튜닝\n",
    "    \n",
    "    seed = 91373\n",
    "    kfold = StratifiedKFold(n_splits=5, random_state = seed, shuffle = True)\n",
    "    cv = np.zeros((train_x.shape[0], 3))\n",
    "\n",
    "    for n, (train_idx, val_idx) in enumerate(kfold.split(train_x, train_y)):\n",
    "\n",
    "        x_train, x_val = train_x.iloc[train_idx], train_x.iloc[val_idx]\n",
    "        y_train, y_val = train_y.iloc[train_idx], train_y.iloc[val_idx]\n",
    "        \n",
    "        catmodel = CatBoostClassifier(**cat_params)                                       # 진행상황 보고싶을때 False를 1000으로\n",
    "        catmodel.fit(x_train, y_train, eval_set=[(x_val,y_val)], early_stopping_rounds=30, verbose=False)\n",
    "\n",
    "        cv[val_idx, :] = catmodel.predict_proba(x_val)\n",
    "        #print(f'fold{n+1} multi_logloss: {log_loss(y_val, cv[val_idx, :])}') # Fold마다 점수 체크하려면 주석 해제\n",
    "    #print('multi_logloss:', log_loss(train_y, cv))\n",
    "\n",
    "    \n",
    "    return log_loss(train_y, cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experimental-proof",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = TPESampler(seed=42)\n",
    "cat_study = optuna.create_study(study_name=\"cat_parameter_opt\", direction=\"minimize\", sampler=sampler)\n",
    "cat_study.optimize(cat_objective, n_trials=30)\n",
    "\n",
    "cat_best_hyperparams = cat_study.best_trial.params\n",
    "cat_base_hyperparams = {'loss_function': 'MultiClass', 'n_estimators': 50000}\n",
    "cat_best_hyperparams.update(cat_base_hyperparams)\n",
    "print(\"The best hyperparameters are:\\n\", cat_best_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lyric-mayor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 나오자마자 복붙해서 카톡방에 올려서 따로 저장해두기!! (파라미터값, logg값 둘 다)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excessive-mixer",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_optimization_history(cat_study) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informative-quantum",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_param_importances(cat_study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "killing-calibration",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_slice(cat_study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alien-duplicate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위 그래프들은 지우지 말고 그대로 커밋해주기 (결과 보고 범위 다시 지정해야함)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
