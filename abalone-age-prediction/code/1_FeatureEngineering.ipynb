{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e3babe3",
   "metadata": {},
   "source": [
    "# 1. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3afc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Library\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler, QuantileTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e408836",
   "metadata": {},
   "source": [
    "# 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bcc759",
   "metadata": {},
   "source": [
    "## (1) Define Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70d0759",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler1 = StandardScaler()\n",
    "scaler2 = MinMaxScaler()\n",
    "scaler3 = QuantileTransformer()\n",
    "def feature_num_scaler(train_df, test_df):\n",
    "    for num_col in num_cols:\n",
    "        \n",
    "        scaler1.fit(train[[num_col]])\n",
    "        train_df[num_col+'#scaler1'] = scaler1.transform(train[[num_col]])\n",
    "        test_df[num_col+'#scaler1'] = scaler1.transform(test_df[[num_col]])\n",
    "        train_df[num_col+'#scaler1'] = train_df[num_col+'#scaler1'].replace(0.0, 0.001)\n",
    "        test_df[num_col+'#scaler1'] = test_df[num_col+'#scaler1'].replace(0.0, 0.001)\n",
    "        \n",
    "        scaler2.fit(train[[num_col]])\n",
    "        train_df[num_col+'#scaler2'] = scaler2.transform(train[[num_col]])\n",
    "        test_df[num_col+'#scaler2'] = scaler2.transform(test_df[[num_col]])\n",
    "        train_df[num_col+'#scaler2'] = train_df[num_col+'#scaler2'].replace(0.0, 0.001)\n",
    "        test_df[num_col+'#scaler2'] = test_df[num_col+'#scaler2'].replace(0.0, 0.001)\n",
    "        \n",
    "        scaler3.fit(train[[num_col]])\n",
    "        train_df[num_col+'#scaler3'] = scaler3.transform(train[[num_col]])\n",
    "        test_df[num_col+'#scaler3'] = scaler3.transform(test_df[[num_col]])\n",
    "        train_df[num_col+'#scaler3'] = train_df[num_col+'#scaler3'].replace(0.0, 0.001)\n",
    "        test_df[num_col+'#scaler3'] = test_df[num_col+'#scaler3'].replace(0.0, 0.001)\n",
    "        \n",
    "        train_df[num_col+'#log2'] = np.log2(train_df[num_col])\n",
    "        test_df[num_col+'#log2'] = np.log2(test_df[num_col])\n",
    "        train_df[num_col+'#log2'] = train_df[num_col+'#log2'].replace(0.0, 0.006)\n",
    "        test_df[num_col+'#log2'] = test_df[num_col+'#log2'].replace(0.0, 0.006)\n",
    "        \n",
    "        train_df[num_col+'#log'] = np.log(train_df[num_col])\n",
    "        test_df[num_col+'#log'] = np.log(test_df[num_col])\n",
    "        train_df[num_col+'#log'] = train_df[num_col+'#log'].replace(0.0, 0.004)\n",
    "        test_df[num_col+'#log'] = test_df[num_col+'#log'].replace(0.0, 0.004)\n",
    "        \n",
    "        train_df[num_col+'#log10'] = np.log10(train_df[num_col])\n",
    "        test_df[num_col+'#log10'] = np.log10(test_df[num_col])\n",
    "        train_df[num_col+'#log10'] = train_df[num_col+'#log10'].replace(0.0, 0.002)\n",
    "        test_df[num_col+'#log10'] = test_df[num_col+'#log10'].replace(0.0, 0.002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5995747",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_cat_generation(df):\n",
    "\n",
    "    for cat_col in cat_cols:\n",
    "        for num_col in num_cols:        \n",
    "            new_name = cat_col + \"#mean#\" + num_col\n",
    "            grouped = df.groupby(cat_col)[num_col].mean()\n",
    "            df[new_name] = df[cat_col].map(grouped)\n",
    "\n",
    "            new_name = cat_col + \"#std#\" + num_col\n",
    "            grouped = df.groupby(cat_col)[num_col].std(ddof = 1)\n",
    "            df[new_name] = df[cat_col].map(grouped)\n",
    "\n",
    "            new_name = cat_col + \"#var#\" + num_col\n",
    "            grouped = df.groupby(cat_col)[num_col].var(ddof = 1)\n",
    "            df[new_name] = df[cat_col].map(grouped)\n",
    "\n",
    "            new_name = cat_col + \"#max#\" + num_col\n",
    "            grouped = df.groupby(cat_col)[num_col].max()\n",
    "            df[new_name] = df[cat_col].map(grouped)\n",
    "\n",
    "            new_name = cat_col + \"#min#\" + num_col\n",
    "            grouped = df.groupby(cat_col)[num_col].min()\n",
    "            df[new_name] = df[cat_col].map(grouped)\n",
    "\n",
    "            new_name = cat_col + \"#ptp#\" + num_col\n",
    "            grouped = df.groupby(cat_col)[num_col].agg(np.ptp)\n",
    "            df[new_name] = df[cat_col].map(grouped)\n",
    "\n",
    "            new_name = cat_col + \"#median\" + num_col\n",
    "            grouped = df.groupby(cat_col)[num_col].median()\n",
    "            df[new_name] = df[cat_col].map(grouped)\n",
    "\n",
    "            new_name = cat_col + \"#skew\" + num_col\n",
    "            grouped = df.groupby(cat_col)[num_col].skew()\n",
    "            df[new_name] = df[cat_col].map(grouped)\n",
    "\n",
    "            new_name = cat_col + \"#percentile_10\" + num_col\n",
    "            grouped = df.groupby(cat_col)[num_col].agg(lambda x: np.percentile(x, 10))\n",
    "            df[new_name] = df[cat_col].map(grouped)\n",
    "\n",
    "            new_name = cat_col + \"#percentile_50\" + num_col\n",
    "            grouped = df.groupby(cat_col)[num_col].agg(lambda x: np.percentile(x, 50))\n",
    "            df[new_name] = df[cat_col].map(grouped)\n",
    "\n",
    "            new_name = cat_col + \"#percentile_90\" + num_col\n",
    "            grouped = df.groupby(cat_col)[num_col].agg(lambda x: np.percentile(x, 90))\n",
    "            df[new_name] = df[cat_col].map(grouped)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105ced5e",
   "metadata": {},
   "source": [
    "## (2) Feature 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02851cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../data/train.csv\")\n",
    "test = pd.read_csv(\"../data/test.csv\")\n",
    "submission = pd.read_csv(\"../data/sample_submission.csv\")\n",
    "\n",
    "train = train.drop('id', axis=1)\n",
    "test = test.drop('id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0d9c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 무게 - (껍질 무게 + 껍질이 아닌 무게) Feature 생성\n",
    "train['Water Weight'] = train['Whole Weight'] - (train['Shucked Weight'] + train['Shell Weight'])\n",
    "test['Water Weight'] = test['Whole Weight'] - (test['Shucked Weight'] + test['Shell Weight'])\n",
    "\n",
    "# 0.005보다 낮은 수는 0.005로 대체\n",
    "train.loc[train[(train['Water Weight']<0.0005)].index, \"Water Weight\"] = 0.0005\n",
    "test.loc[test[(test['Water Weight']<0.0005)].index, \"Water Weight\"] = 0.0005\n",
    "\n",
    "# 0.01 값을 0.0으로 대체\n",
    "train = train.replace(0.0, 0.01)\n",
    "test = test.replace(0.0, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6623958",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = []\n",
    "num_cols = []\n",
    "for col in train.columns:\n",
    "    if train[col].dtypes=='object':\n",
    "        cat_cols.append(col)\n",
    "    elif train[col].dtypes=='float64':\n",
    "        num_cols.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abd89cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_num_scaler(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9267acfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = []\n",
    "num_cols = []\n",
    "for col in train.columns:\n",
    "    if train[col].dtypes=='object':\n",
    "        cat_cols.append(col)\n",
    "    elif train[col].dtypes=='float64':\n",
    "        num_cols.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dfe9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for num_col_first in num_cols:\n",
    "    for num_col_second in num_cols:\n",
    "        if (num_col_first != num_col_second):\n",
    "            train[num_col_first+'/'+num_col_second] = train[num_col_first] / train[num_col_second]\n",
    "            train[num_col_first+'*'+num_col_second] = train[num_col_first] * train[num_col_second]\n",
    "            test[num_col_first+'/'+num_col_second] = test[num_col_first] / test[num_col_second]\n",
    "            test[num_col_first+'*'+num_col_second] = test[num_col_first] * test[num_col_second]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aed0c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cat_generation(train)\n",
    "feature_cat_generation(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45474f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('../data/train_f1.csv', index=False)\n",
    "test.to_csv('../data/test_f1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34236804",
   "metadata": {},
   "source": [
    "## (3) Feature 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce09e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../data/train.csv\")\n",
    "test = pd.read_csv(\"../data/test.csv\")\n",
    "submission = pd.read_csv(\"../data/sample_submission.csv\")\n",
    "\n",
    "train = train.drop('id', axis=1)\n",
    "test = test.drop('id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cec6f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Weight Ratio'] = train['Shucked Weight'] / train['Whole Weight']\n",
    "test['Weight Ratio'] = test['Shucked Weight'] / test['Whole Weight']\n",
    "\n",
    "train['Foreign Body'] = train['Whole Weight'] - (train['Shucked Weight'] + train['Viscra Weight'] + train['Shell Weight'])\n",
    "test['Foreign Body'] = test['Whole Weight'] - (test['Shucked Weight'] + test['Viscra Weight'] + test['Shell Weight'])\n",
    "train.loc[train[(train['Foreign Body']<0.0005)].index, \"Foreign Body\"] = 0.0005\n",
    "test.loc[test[(test['Foreign Body']<0.0005)].index, \"Foreign Body\"] = 0.0005\n",
    "\n",
    "train = train.replace(0.0, 0.01)\n",
    "test = test.replace(0.0, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19206f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = []\n",
    "num_cols = []\n",
    "for col in train.columns:\n",
    "    if train[col].dtypes=='object':\n",
    "        cat_cols.append(col)\n",
    "    elif train[col].dtypes=='float64':\n",
    "        num_cols.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8290bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_num_scaler(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf5565a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = []\n",
    "num_cols = []\n",
    "for col in train.columns:\n",
    "    if train[col].dtypes=='object':\n",
    "        cat_cols.append(col)\n",
    "    elif train[col].dtypes=='float64':\n",
    "        num_cols.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7b1c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "for num_col_first in num_cols:\n",
    "    for num_col_second in num_cols:\n",
    "        if (num_col_first != num_col_second):\n",
    "            train[num_col_first+'/'+num_col_second] = train[num_col_first] / train[num_col_second]\n",
    "            train[num_col_first+'*'+num_col_second] = train[num_col_first] * train[num_col_second]\n",
    "            test[num_col_first+'/'+num_col_second] = test[num_col_first] / test[num_col_second]\n",
    "            test[num_col_first+'*'+num_col_second] = test[num_col_first] * test[num_col_second]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b823c1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cat_generation(train)\n",
    "feature_cat_generation(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32e8dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('../data/train_f2.csv', index=False)\n",
    "test.to_csv('../data/test_f2.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
