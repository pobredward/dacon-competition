{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95d1e511",
   "metadata": {},
   "source": [
    "# 전복 나이 예측 경진대회\n",
    "\n",
    "https://github.com/pobredward/dacon-competition/tree/main/abalone-age-prediction <br>\n",
    "모든 코드는 깃허브에 업로드하였고, 각각의 코드를 파일 한 개로 모을 시간이 마땅치 않아 nbviewer 링크로 업로드합니다.\n",
    "\n",
    "#### 1. 파생 변수 약 6000여개와 8000여개를 각각 생성하여 두 가지의 데이터프레임을 만들었습니다.\n",
    "https://nbviewer.org/github/pobredward/dacon-competition/blob/main/abalone-age-prediction/code/1_FeatureEngineering.ipynb\n",
    "\n",
    "#### 2. 얻어낸 데이터프레임 두 개를 따로따로 모델에 학습시켰습니다.\n",
    "(LGB1) https://nbviewer.org/github/pobredward/dacon-competition/blob/main/abalone-age-prediction/code/2_Modeling_lightgbm-f1.ipynb<br>\n",
    "(LGB2) https://nbviewer.org/github/pobredward/dacon-competition/blob/main/abalone-age-prediction/code/2_Modeling_lightgbm-f2.ipynb<br>\n",
    "(XGB1) https://nbviewer.org/github/pobredward/dacon-competition/blob/main/abalone-age-prediction/code/2_Modeling_xgboost-f1.ipynb<br>\n",
    "(XGB2) https://nbviewer.org/github/pobredward/dacon-competition/blob/main/abalone-age-prediction/code/X_Modeling_xgboost-f2.ipynb<br>\n",
    "(CAT1) https://nbviewer.org/github/pobredward/dacon-competition/blob/main/abalone-age-prediction/code/X_Modeling_catboost-f1.ipynb<br>\n",
    "(CAT2) https://nbviewer.org/github/pobredward/dacon-competition/blob/main/abalone-age-prediction/code/X_Modeling_catboost-f2.ipynb<br>\n",
    "(NN) https://nbviewer.org/github/pobredward/dacon-competition/blob/main/abalone-age-prediction/code/2_Modeling_nn.ipynb\n",
    "\n",
    "#### 3. 총 2번의 Stacking을 거친 후 Blending하였습니다. Blending 과정은 seed가 바뀔 때마다 permutation으로 적절한 값을 노가다로 찾아야 함.\n",
    "(STACK1) https://nbviewer.org/github/pobredward/dacon-competition/blob/main/abalone-age-prediction/code/3_stacking.ipynb<br>\n",
    "(STACK2) https://nbviewer.org/github/pobredward/dacon-competition/blob/main/abalone-age-prediction/code/3_stacking2.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198979db",
   "metadata": {},
   "source": [
    "![structure.png](../image/structure.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3babe3",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09c17e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Library\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from itertools import permutations, combinations\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# HP Tuning\n",
    "import optuna\n",
    "from optuna import Trial\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.visualization import plot_contour, plot_optimization_history, plot_parallel_coordinate, plot_slice, plot_param_importances\n",
    "\n",
    "# Modeling\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf83377",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dir(dir):\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "        print(\"Created Directory :\", dir)\n",
    "    else:\n",
    "        print(\"Directory already existed :\", dir)\n",
    "create_dir(\"../pickle\")\n",
    "create_dir(\"../model\")\n",
    "create_dir(\"../submission\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c700011b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../data/train.csv\")\n",
    "test = pd.read_csv(\"../data/test.csv\")\n",
    "submission = pd.read_csv(\"../data/sample_submission.csv\")\n",
    "\n",
    "train = train.drop([\"id\"], axis=1)\n",
    "test = test.drop([\"id\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b95a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_train = train.shape[0] # 주어진 train data의 row 수\n",
    "rows_test = test.shape[0] # 주어진 test data의 row 수\n",
    "num_trial = 100 # 파라미터 튜닝을 몇 번 진행하는지의 수\n",
    "splits_hp = 5 # 파라미터 튜닝을 진행할 때의 kfold 수\n",
    "splits_tr = 15 # 모델 트레이닝을 진행할 때의 kfold 수\n",
    "basic_seed = 42 # default seed\n",
    "num_seed_tr = 10 # 트레이닝 seed 개수\n",
    "sel_seed = 3 # 선택할 seed 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6f0c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_dict(model, pred_dict, pred_test_dict):\n",
    "    pred_dict_local = {}\n",
    "    for key, value in pred_dict.items():\n",
    "        if model in key:\n",
    "            pred_dict_local[key]=value\n",
    "\n",
    "    pred_test_dict_local = {}\n",
    "    for key, value in pred_test_dict.items():\n",
    "        if model in key:\n",
    "            pred_test_dict_local[key]=value\n",
    "\n",
    "    pred_dict_new_local = dict(sorted(\n",
    "        pred_dict_local.items(), \n",
    "        key=lambda x:mean_absolute_error((train_y), list(x[1])), reverse=False)[:sel_seed])\n",
    "    pred_test_dict_new_local = {}\n",
    "    for key, value in pred_dict_new_local.items():\n",
    "        pred_test_dict_new_local[key]=pred_test_dict_local[key]\n",
    "        \n",
    "    return pred_dict_new_local, pred_test_dict_new_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3587025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dict(model, pred_dict, pred_test_dict):\n",
    "    with open('../pickle/pred_dict_'+model+'.pickle', 'wb') as fw:\n",
    "        pickle.dump(pred_dict, fw)\n",
    "    with open('../pickle/pred_test_dict_'+model+'.pickle', 'wb') as fw:\n",
    "        pickle.dump(pred_test_dict, fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df06f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dict(model):\n",
    "    with open('../pickle/pred_dict_'+model+'.pickle', 'rb') as fw:\n",
    "        pred_dict_new_local = pickle.load(fw)\n",
    "    with open('../pickle/pred_test_dict_'+model+'.pickle', 'rb') as fw:\n",
    "        pred_test_dict_new_local = pickle.load(fw)\n",
    "    return pred_dict_new_local, pred_test_dict_new_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b14fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dict = {}\n",
    "pred_test_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312683f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dict_lgb, pred_test_dict_lgb = load_dict('lgb')\n",
    "pred_dict_lgb2, pred_test_dict_lgb2 = load_dict('lgb2')\n",
    "pred_dict_xgb, pred_test_dict_xgb = load_dict('xgb')\n",
    "pred_dict_cat, pred_test_dict_cat = load_dict('cat')\n",
    "pred_dict_cat2, pred_test_dict_cat2 = load_dict('cat2')\n",
    "pred_dict_nn, pred_test_dict_nn = load_dict('nn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7254e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dict_total = {**pred_dict_lgb, **pred_dict_cat,\n",
    "                   **pred_dict_lgb2, **pred_dict_xgb, **pred_dict_cat2, **pred_dict_nn}\n",
    "pred_test_dict_total = {**pred_test_dict_lgb, **pred_test_dict_cat, \n",
    "                        **pred_test_dict_lgb2, **pred_test_dict_xgb, **pred_test_dict_cat2, **pred_test_dict_nn}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3d4a67",
   "metadata": {},
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b98db1d",
   "metadata": {},
   "source": [
    "## (2) HP Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6acdc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(np.vstack([x for _, x in pred_dict_total.items()]).T)\n",
    "X_test = pd.DataFrame(np.vstack([x for _, x in pred_test_dict_total.items()]).T)\n",
    "train_y = train['Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb8e583",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_objective(trial: Trial) -> float:\n",
    "    score_hp = []\n",
    "    for seed_hp in [0]:\n",
    "        params_xgb = {\n",
    "            \"random_state\": basic_seed,\n",
    "            \"verbose\": None,\n",
    "            \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 1e-1), # eta, default=0.3, range=[0,1]\n",
    "            \"gamma\": trial.suggest_loguniform(\"gamma\", 1e-2, 1e+2), # min_split_loss, default=0, range=[0,∞]\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 4, 12), # default=5, range=[0,∞]\n",
    "            \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10), #default=1\n",
    "            \"max_delta_step\" : trial.suggest_int(\"max_delta_step\", 0, 10), #default=0\n",
    "            \"subsample\": trial.suggest_uniform(\"subsample\", 0.2, 1.0), # default=1, range=(0,1]\n",
    "            \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0.2, 1.0), # default=1, range=(0,1]\n",
    "            \"colsample_bylevel\": trial.suggest_uniform(\"colsample_bylevel\", 0.2, 1.0), # default=1, range=(0,1]\n",
    "            \"colsample_bynode\": trial.suggest_uniform(\"colsample_bynode\", 0.2, 1.0), # default=1, range=(0,1]\n",
    "            \"reg_alpha\": trial.suggest_loguniform(\"reg_alpha\", 1e-2, 1e+2), # default=0, range=[0,∞]\n",
    "            \"reg_lambda\": trial.suggest_loguniform(\"reg_lambda\", 1e-2, 1e+2), # default=1, range=[0,∞]\n",
    "            \"max_bin\": trial.suggest_int(\"max_bin\", 100, 500),\n",
    "        }\n",
    "\n",
    "        kfold = StratifiedKFold(n_splits=splits_hp, random_state=seed_hp, shuffle=True)\n",
    "        cv = np.zeros(rows_train)\n",
    "\n",
    "        for n, (train_idx, val_idx) in enumerate(kfold.split(X_train, train_y)):\n",
    "\n",
    "            x_train, x_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "            y_train, y_val = train_y.iloc[train_idx], train_y.iloc[val_idx]\n",
    "\n",
    "            dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "            dvalid = xgb.DMatrix(x_val, label=y_val)\n",
    "            watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n",
    "                                                                                               \n",
    "            stack_xgbmodel = xgb.train(params_xgb, dtrain, 100000, watchlist, early_stopping_rounds=30, verbose_eval=None)\n",
    "            cv[val_idx] = stack_xgbmodel.predict(dvalid)\n",
    "\n",
    "        score_hp.append(mean_absolute_error(train_y, cv))\n",
    "    \n",
    "    np.mean(score_hp)\n",
    "    return np.mean(score_hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6617baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = TPESampler(seed=basic_seed)\n",
    "stack_study = optuna.create_study(study_name=\"stack_parameter_opt\", direction=\"minimize\", sampler=sampler)\n",
    "stack_study.optimize(stack_objective, n_trials=num_trial)\n",
    "\n",
    "stack_best_hyperparams = stack_study.best_trial.params\n",
    "stack_base_hyperparams = {\"random_state\": basic_seed}\n",
    "stack_best_hyperparams.update(stack_base_hyperparams)\n",
    "print(\"The best hyperparameters are:\\n\", stack_best_hyperparams)\n",
    "\n",
    "with open('../pickle/stack_best_hyperparams2.pickle', 'wb') as fw:\n",
    "    pickle.dump(stack_best_hyperparams, fw)\n",
    "print(\"The best hyperparameters are:\\n\", stack_best_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b06cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.matplotlib.plot_param_importances(stack_study);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e71b736",
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.matplotlib.plot_slice(stack_study);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609578e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../pickle/stack_best_hyperparams2.pickle', 'rb') as fw:\n",
    "    stack_best_hyperparams = pickle.load(fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b31379a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_best_hyperparams = stack_study.best_trial.params\n",
    "stack_base_hyperparams = {\"random_state\": basic_seed}\n",
    "stack_best_hyperparams.update(stack_base_hyperparams)\n",
    "print(\"The best hyperparameters are:\\n\", stack_best_hyperparams)\n",
    "\n",
    "with open('../pickle/stack_best_hyperparams2.pickle', 'wb') as fw:\n",
    "    pickle.dump(stack_best_hyperparams, fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa3b835",
   "metadata": {},
   "outputs": [],
   "source": [
    "lucky_seeds = np.random.randint(0, 1000, num_seed_tr)\n",
    "\n",
    "for i, seed in enumerate(lucky_seeds):\n",
    "\n",
    "    kfold = StratifiedKFold(n_splits=splits_tr, random_state=seed, shuffle=True)\n",
    "    cv = np.zeros(rows_train)\n",
    "    pred_test = np.zeros(rows_test)\n",
    "\n",
    "    for n, (train_idx, val_idx) in enumerate(kfold.split(X_train, train_y)):\n",
    "        x_train, x_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_train, y_val = train_y.iloc[train_idx], train_y.iloc[val_idx]\n",
    "\n",
    "        dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "        dvalid = xgb.DMatrix(x_val, label=y_val)\n",
    "        watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n",
    "                                                                                           \n",
    "        stack_xgbmodel = xgb.train(stack_best_hyperparams, dtrain, 100000, watchlist, early_stopping_rounds=30, verbose_eval=None)\n",
    "\n",
    "        cv[val_idx] = stack_xgbmodel.predict(dvalid)\n",
    "        pred_test += stack_xgbmodel.predict(xgb.DMatrix(X_test)) / splits_tr\n",
    "        \n",
    "    pred_dict['stack'+str(seed)] = cv\n",
    "    pred_test_dict['stack'+str(seed)] = pred_test\n",
    "    print(f'seed {seed}', 'mean_absolute_error :', mean_absolute_error(train_y, cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf94c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dict_stack2, pred_test_dict_stack2 = sort_dict('stack', pred_dict, pred_test_dict)\n",
    "save_dict('stack2', pred_dict_stack2, pred_test_dict_stack2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ffa56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.zeros(rows_train)\n",
    "for _, value in pred_dict_stack2.items():\n",
    "    pred += value\n",
    "pred /= len(pred_dict_stack2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f248bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'CV mean_absolute_error: {mean_absolute_error(train_y, np.round(pred)):.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb2504d",
   "metadata": {},
   "source": [
    "# 4. Blending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83060bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dict_stack1, pred_test_dict_stack1 = load_dict('stack')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c7595f",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate = np.arange(0, 600)\n",
    "# candidate = [0, 1, 2, 3, 4, 5, 10, 140, 520]\n",
    "permute = permutations(candidate, 7)\n",
    "score = {}\n",
    "for i in tqdm(list(permute)):\n",
    "    pred_permute = (\n",
    "                    sum(pred_dict_lgb.values())/sel_seed * i[0] +\n",
    "                    sum(pred_dict_lgb2.values())/sel_seed * i[1] +\n",
    "#                     sum(pred_dict_xgb.values())/sel_seed * i[2] +\n",
    "                    sum(pred_dict_cat.values())/sel_seed * i[2] +\n",
    "                    sum(pred_dict_cat2.values())/sel_seed * i[3] +\n",
    "                    sum(pred_dict_nn.values())/sel_seed * i[4] +\n",
    "                    sum(pred_dict_stack1.values())/sel_seed * i[5] + \n",
    "                    sum(pred_dict_stack2.values())/sel_seed * i[6]\n",
    "                   )\n",
    "    score[i] = mean_absolute_error(train_y, pred_permute/sum(i))\n",
    "\n",
    "score = dict(sorted(score.items(), key=lambda x: x[1], reverse=False)[:5])\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffc927c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = (sum(pred_dict_lgb.values())/sel_seed * list(score.keys())[0][0] +\n",
    "        sum(pred_dict_lgb2.values())/sel_seed * list(score.keys())[0][1] +\n",
    "#         sum(pred_dict_xgb.values())/sel_seed * list(score.keys())[0][2] +\n",
    "        sum(pred_dict_cat.values())/sel_seed * list(score.keys())[0][2] +\n",
    "        sum(pred_dict_cat2.values())/sel_seed * list(score.keys())[0][3] +\n",
    "        sum(pred_dict_nn.values())/sel_seed * list(score.keys())[0][4] +\n",
    "        sum(pred_dict_stack1.values())/sel_seed * list(score.keys())[0][5] +\n",
    "        sum(pred_dict_stack2.values())/sel_seed * list(score.keys())[0][6]\n",
    "       ) / sum(list(score.keys())[0])\n",
    "mean_absolute_error(train_y, np.round(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1f7136",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = (sum(pred_test_dict_lgb.values())/3 * list(score.keys())[0][0] +\n",
    "             sum(pred_test_dict_lgb2.values())/3 * list(score.keys())[0][1] +\n",
    "#              sum(pred_test_dict_xgb.values())/3 * list(score.keys())[0][2] +\n",
    "             sum(pred_test_dict_cat.values())/3 * list(score.keys())[0][2] +\n",
    "             sum(pred_test_dict_cat2.values())/3 * list(score.keys())[0][3] +\n",
    "             sum(pred_test_dict_nn.values())/3 * list(score.keys())[0][4] +\n",
    "             sum(pred_test_dict_stack1.values())/3 * list(score.keys())[0][5] +\n",
    "             sum(pred_test_dict_stack2.values())/3 * list(score.keys())[0][6]\n",
    "            ) / sum(list(score.keys())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9215f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.Target = np.round(pred_test).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b979f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_name = '20220401'\n",
    "submission_number = '3'\n",
    "submission.to_csv(f'../submission/{submission_name}-{submission_number}.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
