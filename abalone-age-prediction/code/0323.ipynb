{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "038a6c13",
   "metadata": {},
   "source": [
    "Lasso, Ridge, Randomforest, ElasticNet, GradientBoostingRegressor, LGBM(dart), XGB(otherBoosting), AdaBoostRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3babe3",
   "metadata": {},
   "source": [
    "# 1. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d3afc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Library\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from itertools import permutations, combinations\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler, QuantileTransformer\n",
    "\n",
    "# HP Tuning\n",
    "import optuna\n",
    "from optuna import Trial\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.visualization import plot_contour, plot_optimization_history\n",
    "from optuna.visualization import plot_parallel_coordinate, plot_slice, plot_param_importances\n",
    "\n",
    "# Modeling\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from lightgbm import LGBMRegressor\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebf83377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already existed : ../pickle\n",
      "Directory already existed : ../model\n",
      "Directory already existed : ../submission\n"
     ]
    }
   ],
   "source": [
    "def create_dir(dir):\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "        print(\"Created Directory :\", dir)\n",
    "    else:\n",
    "        print(\"Directory already existed :\", dir)\n",
    "create_dir(\"../pickle\")\n",
    "create_dir(\"../model\")\n",
    "create_dir(\"../submission\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c700011b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../data/train.csv\")\n",
    "test = pd.read_csv(\"../data/test.csv\")\n",
    "submission = pd.read_csv(\"../data/sample_submission.csv\")\n",
    "\n",
    "train = train.drop('id', axis=1)\n",
    "test = test.drop('id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20b95a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_train = train.shape[0] # 주어진 train data의 row 수\n",
    "rows_test = test.shape[0] # 주어진 test data의 row 수\n",
    "num_trial = 100 # 파라미터 튜닝을 몇 번 진행하는지의 수\n",
    "splits_hp = 5 # 파라미터 튜닝을 진행할 때의 kfold 수\n",
    "splits_tr = 15 # 모델 트레이닝을 진행할 때의 kfold 수\n",
    "basic_seed = 42 # default seed\n",
    "num_seed_hp = 3 # 파라미터 튜닝 seed 개수\n",
    "num_seed_tr = 10 # 트레이닝 seed 개수\n",
    "sel_seed = 3 # 선택할 seed 개수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e408836",
   "metadata": {},
   "source": [
    "# 2. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7dbb35da",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.replace(0.0, 0.1)\n",
    "test = test.replace(0.0, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b70d0759",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler1 = StandardScaler()\n",
    "scaler2 = MinMaxScaler()\n",
    "scaler3 = QuantileTransformer()\n",
    "def feature_num_scaler(train_df, test_df):\n",
    "    for num_col in num_cols:\n",
    "        \n",
    "        scaler1.fit(train[[num_col]])\n",
    "        train_df[num_col+'#scaler1'] = scaler1.transform(train[[num_col]])\n",
    "        test_df[num_col+'#scaler1'] = scaler1.transform(test_df[[num_col]])\n",
    "        \n",
    "        scaler2.fit(train[[num_col]])\n",
    "        train_df[num_col+'#scaler2'] = scaler2.transform(train[[num_col]])\n",
    "        test_df[num_col+'#scaler2'] = scaler2.transform(test_df[[num_col]])\n",
    "        \n",
    "        scaler3.fit(train[[num_col]])\n",
    "        train_df[num_col+'#scaler3'] = scaler3.transform(train[[num_col]])\n",
    "        test_df[num_col+'#scaler3'] = scaler3.transform(test_df[[num_col]])\n",
    "        \n",
    "        train_df[num_col+'#log'] = np.log(train_df[num_col])\n",
    "        test_df[num_col+'#log'] = np.log(test_df[num_col])\n",
    "        \n",
    "        train_df[num_col+'#log2'] = np.log2(train_df[num_col])\n",
    "        test_df[num_col+'#log2'] = np.log2(test_df[num_col])\n",
    "        \n",
    "        train_df[num_col+'#log10'] = np.log10(train_df[num_col])\n",
    "        test_df[num_col+'#log10'] = np.log10(test_df[num_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5995747",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_cat_generation(df):\n",
    "\n",
    "    for cat_col in cat_cols:\n",
    "        for num_col in num_cols:        \n",
    "            new_name = cat_col + \"#mean#\" + num_col\n",
    "            grouped = df.groupby(cat_col)[num_col].mean()\n",
    "            df[new_name] = df[cat_col].map(grouped)\n",
    "\n",
    "            new_name = cat_col + \"#std#\" + num_col\n",
    "            grouped = df.groupby(cat_col)[num_col].std(ddof = 1)\n",
    "            df[new_name] = df[cat_col].map(grouped)\n",
    "\n",
    "            new_name = cat_col + \"#var#\" + num_col\n",
    "            grouped = df.groupby(cat_col)[num_col].var(ddof = 1)\n",
    "            df[new_name] = df[cat_col].map(grouped)\n",
    "\n",
    "            new_name = cat_col + \"#max#\" + num_col\n",
    "            grouped = df.groupby(cat_col)[num_col].max()\n",
    "            df[new_name] = df[cat_col].map(grouped)\n",
    "\n",
    "            new_name = cat_col + \"#min#\" + num_col\n",
    "            grouped = df.groupby(cat_col)[num_col].min()\n",
    "            df[new_name] = df[cat_col].map(grouped)\n",
    "\n",
    "            new_name = cat_col + \"#ptp#\" + num_col\n",
    "            grouped = df.groupby(cat_col)[num_col].agg(np.ptp)\n",
    "            df[new_name] = df[cat_col].map(grouped)\n",
    "\n",
    "            new_name = cat_col + \"#median\" + num_col\n",
    "            grouped = df.groupby(cat_col)[num_col].median()\n",
    "            df[new_name] = df[cat_col].map(grouped)\n",
    "\n",
    "            new_name = cat_col + \"#skew\" + num_col\n",
    "            grouped = df.groupby(cat_col)[num_col].skew()\n",
    "            df[new_name] = df[cat_col].map(grouped)\n",
    "\n",
    "            new_name = cat_col + \"#percentile_10\" + num_col\n",
    "            grouped = df.groupby(cat_col)[num_col].agg(lambda x: np.percentile(x, 10))\n",
    "            df[new_name] = df[cat_col].map(grouped)\n",
    "\n",
    "            new_name = cat_col + \"#percentile_60\" + num_col\n",
    "            grouped = df.groupby(cat_col)[num_col].agg(lambda x: np.percentile(x, 60))\n",
    "            df[new_name] = df[cat_col].map(grouped)\n",
    "\n",
    "            new_name = cat_col + \"#percentile_90\" + num_col\n",
    "            grouped = df.groupby(cat_col)[num_col].agg(lambda x: np.percentile(x, 90))\n",
    "            df[new_name] = df[cat_col].map(grouped)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6623958",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = []\n",
    "num_cols = []\n",
    "for col in train.columns:\n",
    "    if train[col].dtypes=='object':\n",
    "        cat_cols.append(col)\n",
    "    elif train[col].dtypes=='float64':\n",
    "        num_cols.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1abd89cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_num_scaler(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fca44778",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.replace(0.0, 0.01)\n",
    "test = test.replace(0.0, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9267acfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = []\n",
    "num_cols = []\n",
    "for col in train.columns:\n",
    "    if train[col].dtypes=='object':\n",
    "        cat_cols.append(col)\n",
    "    elif train[col].dtypes=='float64':\n",
    "        num_cols.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77dfe9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for num_col_first in num_cols:\n",
    "    for num_col_second in num_cols:\n",
    "        if (num_col_first != num_col_second):\n",
    "            train[num_col_first+'/'+num_col_second] = train[num_col_first] / train[num_col_second]\n",
    "            train[num_col_first+'*'+num_col_second] = train[num_col_first] * train[num_col_second]\n",
    "            test[num_col_first+'/'+num_col_second] = test[num_col_first] / test[num_col_second]\n",
    "            test[num_col_first+'*'+num_col_second] = test[num_col_first] * test[num_col_second]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fbdcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for num_col_first in num_cols:\n",
    "#     for num_col_second in num_cols:\n",
    "#         if ((num_col_first != num_col_second)&\n",
    "#             (((train[num_col_second]<=0.001)&(train[num_col_second]>=-0.001)).sum()==0)&\n",
    "#             (((test[num_col_second]<=0.001)&(test[num_col_second]>=-0.001)).sum()==0)):\n",
    "#             train[num_col_first+'/'+num_col_second] = train[num_col_first] / train[num_col_second]\n",
    "#             train[num_col_first+'*'+num_col_second] = train[num_col_first] * train[num_col_second]\n",
    "#             test[num_col_first+'/'+num_col_second] = test[num_col_first] / test[num_col_second]\n",
    "#             test[num_col_first+'*'+num_col_second] = test[num_col_first] * test[num_col_second]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0aed0c68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Lenght</th>\n",
       "      <th>Diameter</th>\n",
       "      <th>Height</th>\n",
       "      <th>Whole Weight</th>\n",
       "      <th>Shucked Weight</th>\n",
       "      <th>Viscra Weight</th>\n",
       "      <th>Shell Weight</th>\n",
       "      <th>Lenght#scaler1</th>\n",
       "      <th>Lenght#scaler2</th>\n",
       "      <th>...</th>\n",
       "      <th>Gender#std#Shell Weight#log10</th>\n",
       "      <th>Gender#var#Shell Weight#log10</th>\n",
       "      <th>Gender#max#Shell Weight#log10</th>\n",
       "      <th>Gender#min#Shell Weight#log10</th>\n",
       "      <th>Gender#ptp#Shell Weight#log10</th>\n",
       "      <th>Gender#medianShell Weight#log10</th>\n",
       "      <th>Gender#skewShell Weight#log10</th>\n",
       "      <th>Gender#percentile_10Shell Weight#log10</th>\n",
       "      <th>Gender#percentile_60Shell Weight#log10</th>\n",
       "      <th>Gender#percentile_90Shell Weight#log10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F</td>\n",
       "      <td>0.595</td>\n",
       "      <td>0.470</td>\n",
       "      <td>0.155</td>\n",
       "      <td>1.1210</td>\n",
       "      <td>0.4515</td>\n",
       "      <td>0.1780</td>\n",
       "      <td>0.1550</td>\n",
       "      <td>0.600176</td>\n",
       "      <td>0.723881</td>\n",
       "      <td>...</td>\n",
       "      <td>0.203919</td>\n",
       "      <td>0.041583</td>\n",
       "      <td>-0.070581</td>\n",
       "      <td>-1.602060</td>\n",
       "      <td>1.531479</td>\n",
       "      <td>-0.530178</td>\n",
       "      <td>-0.960625</td>\n",
       "      <td>-0.836841</td>\n",
       "      <td>-0.494850</td>\n",
       "      <td>-0.338662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "      <td>0.580</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.9270</td>\n",
       "      <td>0.2760</td>\n",
       "      <td>0.1815</td>\n",
       "      <td>0.3600</td>\n",
       "      <td>0.475366</td>\n",
       "      <td>0.701493</td>\n",
       "      <td>...</td>\n",
       "      <td>0.270111</td>\n",
       "      <td>0.072960</td>\n",
       "      <td>-0.047208</td>\n",
       "      <td>-2.301030</td>\n",
       "      <td>2.253822</td>\n",
       "      <td>-0.559091</td>\n",
       "      <td>-1.900897</td>\n",
       "      <td>-0.939302</td>\n",
       "      <td>-0.510042</td>\n",
       "      <td>-0.349694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.0970</td>\n",
       "      <td>0.0415</td>\n",
       "      <td>0.0190</td>\n",
       "      <td>0.0305</td>\n",
       "      <td>-2.187241</td>\n",
       "      <td>0.223881</td>\n",
       "      <td>...</td>\n",
       "      <td>0.374580</td>\n",
       "      <td>0.140310</td>\n",
       "      <td>-0.275724</td>\n",
       "      <td>-2.823909</td>\n",
       "      <td>2.548185</td>\n",
       "      <td>-0.945004</td>\n",
       "      <td>-1.136631</td>\n",
       "      <td>-1.522879</td>\n",
       "      <td>-0.866142</td>\n",
       "      <td>-0.610834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M</td>\n",
       "      <td>0.590</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.130</td>\n",
       "      <td>1.1020</td>\n",
       "      <td>0.4550</td>\n",
       "      <td>0.2055</td>\n",
       "      <td>0.3300</td>\n",
       "      <td>0.558572</td>\n",
       "      <td>0.716418</td>\n",
       "      <td>...</td>\n",
       "      <td>0.270111</td>\n",
       "      <td>0.072960</td>\n",
       "      <td>-0.047208</td>\n",
       "      <td>-2.301030</td>\n",
       "      <td>2.253822</td>\n",
       "      <td>-0.559091</td>\n",
       "      <td>-1.900897</td>\n",
       "      <td>-0.939302</td>\n",
       "      <td>-0.510042</td>\n",
       "      <td>-0.349694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>F</td>\n",
       "      <td>0.595</td>\n",
       "      <td>0.465</td>\n",
       "      <td>0.140</td>\n",
       "      <td>1.1130</td>\n",
       "      <td>0.5175</td>\n",
       "      <td>0.2440</td>\n",
       "      <td>0.3050</td>\n",
       "      <td>0.600176</td>\n",
       "      <td>0.723881</td>\n",
       "      <td>...</td>\n",
       "      <td>0.203919</td>\n",
       "      <td>0.041583</td>\n",
       "      <td>-0.070581</td>\n",
       "      <td>-1.602060</td>\n",
       "      <td>1.531479</td>\n",
       "      <td>-0.530178</td>\n",
       "      <td>-0.960625</td>\n",
       "      <td>-0.836841</td>\n",
       "      <td>-0.494850</td>\n",
       "      <td>-0.338662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2919</th>\n",
       "      <td>I</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.0340</td>\n",
       "      <td>0.0120</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>-2.936099</td>\n",
       "      <td>0.089552</td>\n",
       "      <td>...</td>\n",
       "      <td>0.374580</td>\n",
       "      <td>0.140310</td>\n",
       "      <td>-0.275724</td>\n",
       "      <td>-2.823909</td>\n",
       "      <td>2.548185</td>\n",
       "      <td>-0.945004</td>\n",
       "      <td>-1.136631</td>\n",
       "      <td>-1.522879</td>\n",
       "      <td>-0.866142</td>\n",
       "      <td>-0.610834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2920</th>\n",
       "      <td>I</td>\n",
       "      <td>0.435</td>\n",
       "      <td>0.345</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.4180</td>\n",
       "      <td>0.2220</td>\n",
       "      <td>0.0735</td>\n",
       "      <td>0.1060</td>\n",
       "      <td>-0.731128</td>\n",
       "      <td>0.485075</td>\n",
       "      <td>...</td>\n",
       "      <td>0.374580</td>\n",
       "      <td>0.140310</td>\n",
       "      <td>-0.275724</td>\n",
       "      <td>-2.823909</td>\n",
       "      <td>2.548185</td>\n",
       "      <td>-0.945004</td>\n",
       "      <td>-1.136631</td>\n",
       "      <td>-1.522879</td>\n",
       "      <td>-0.866142</td>\n",
       "      <td>-0.610834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2921</th>\n",
       "      <td>I</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.7940</td>\n",
       "      <td>0.3815</td>\n",
       "      <td>0.1415</td>\n",
       "      <td>0.2450</td>\n",
       "      <td>0.392159</td>\n",
       "      <td>0.686567</td>\n",
       "      <td>...</td>\n",
       "      <td>0.374580</td>\n",
       "      <td>0.140310</td>\n",
       "      <td>-0.275724</td>\n",
       "      <td>-2.823909</td>\n",
       "      <td>2.548185</td>\n",
       "      <td>-0.945004</td>\n",
       "      <td>-1.136631</td>\n",
       "      <td>-1.522879</td>\n",
       "      <td>-0.866142</td>\n",
       "      <td>-0.610834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2922</th>\n",
       "      <td>I</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.4885</td>\n",
       "      <td>0.1930</td>\n",
       "      <td>0.1050</td>\n",
       "      <td>0.1550</td>\n",
       "      <td>-0.523112</td>\n",
       "      <td>0.522388</td>\n",
       "      <td>...</td>\n",
       "      <td>0.374580</td>\n",
       "      <td>0.140310</td>\n",
       "      <td>-0.275724</td>\n",
       "      <td>-2.823909</td>\n",
       "      <td>2.548185</td>\n",
       "      <td>-0.945004</td>\n",
       "      <td>-1.136631</td>\n",
       "      <td>-1.522879</td>\n",
       "      <td>-0.866142</td>\n",
       "      <td>-0.610834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2923</th>\n",
       "      <td>F</td>\n",
       "      <td>0.565</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.9150</td>\n",
       "      <td>0.3540</td>\n",
       "      <td>0.1935</td>\n",
       "      <td>0.3200</td>\n",
       "      <td>0.350556</td>\n",
       "      <td>0.679104</td>\n",
       "      <td>...</td>\n",
       "      <td>0.203919</td>\n",
       "      <td>0.041583</td>\n",
       "      <td>-0.070581</td>\n",
       "      <td>-1.602060</td>\n",
       "      <td>1.531479</td>\n",
       "      <td>-0.530178</td>\n",
       "      <td>-0.960625</td>\n",
       "      <td>-0.836841</td>\n",
       "      <td>-0.494850</td>\n",
       "      <td>-0.338662</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2924 rows × 5293 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Gender  Lenght  Diameter  Height  Whole Weight  Shucked Weight  \\\n",
       "0         F   0.595     0.470   0.155        1.1210          0.4515   \n",
       "1         M   0.580     0.450   0.150        0.9270          0.2760   \n",
       "2         I   0.260     0.205   0.070        0.0970          0.0415   \n",
       "3         M   0.590     0.460   0.130        1.1020          0.4550   \n",
       "4         F   0.595     0.465   0.140        1.1130          0.5175   \n",
       "...     ...     ...       ...     ...           ...             ...   \n",
       "2919      I   0.170     0.105   0.035        0.0340          0.0120   \n",
       "2920      I   0.435     0.345   0.115        0.4180          0.2220   \n",
       "2921      I   0.570     0.450   0.135        0.7940          0.3815   \n",
       "2922      I   0.460     0.350   0.120        0.4885          0.1930   \n",
       "2923      F   0.565     0.440   0.160        0.9150          0.3540   \n",
       "\n",
       "      Viscra Weight  Shell Weight  Lenght#scaler1  Lenght#scaler2  ...  \\\n",
       "0            0.1780        0.1550        0.600176        0.723881  ...   \n",
       "1            0.1815        0.3600        0.475366        0.701493  ...   \n",
       "2            0.0190        0.0305       -2.187241        0.223881  ...   \n",
       "3            0.2055        0.3300        0.558572        0.716418  ...   \n",
       "4            0.2440        0.3050        0.600176        0.723881  ...   \n",
       "...             ...           ...             ...             ...  ...   \n",
       "2919         0.0085        0.0050       -2.936099        0.089552  ...   \n",
       "2920         0.0735        0.1060       -0.731128        0.485075  ...   \n",
       "2921         0.1415        0.2450        0.392159        0.686567  ...   \n",
       "2922         0.1050        0.1550       -0.523112        0.522388  ...   \n",
       "2923         0.1935        0.3200        0.350556        0.679104  ...   \n",
       "\n",
       "      Gender#std#Shell Weight#log10  Gender#var#Shell Weight#log10  \\\n",
       "0                          0.203919                       0.041583   \n",
       "1                          0.270111                       0.072960   \n",
       "2                          0.374580                       0.140310   \n",
       "3                          0.270111                       0.072960   \n",
       "4                          0.203919                       0.041583   \n",
       "...                             ...                            ...   \n",
       "2919                       0.374580                       0.140310   \n",
       "2920                       0.374580                       0.140310   \n",
       "2921                       0.374580                       0.140310   \n",
       "2922                       0.374580                       0.140310   \n",
       "2923                       0.203919                       0.041583   \n",
       "\n",
       "      Gender#max#Shell Weight#log10  Gender#min#Shell Weight#log10  \\\n",
       "0                         -0.070581                      -1.602060   \n",
       "1                         -0.047208                      -2.301030   \n",
       "2                         -0.275724                      -2.823909   \n",
       "3                         -0.047208                      -2.301030   \n",
       "4                         -0.070581                      -1.602060   \n",
       "...                             ...                            ...   \n",
       "2919                      -0.275724                      -2.823909   \n",
       "2920                      -0.275724                      -2.823909   \n",
       "2921                      -0.275724                      -2.823909   \n",
       "2922                      -0.275724                      -2.823909   \n",
       "2923                      -0.070581                      -1.602060   \n",
       "\n",
       "      Gender#ptp#Shell Weight#log10  Gender#medianShell Weight#log10  \\\n",
       "0                          1.531479                        -0.530178   \n",
       "1                          2.253822                        -0.559091   \n",
       "2                          2.548185                        -0.945004   \n",
       "3                          2.253822                        -0.559091   \n",
       "4                          1.531479                        -0.530178   \n",
       "...                             ...                              ...   \n",
       "2919                       2.548185                        -0.945004   \n",
       "2920                       2.548185                        -0.945004   \n",
       "2921                       2.548185                        -0.945004   \n",
       "2922                       2.548185                        -0.945004   \n",
       "2923                       1.531479                        -0.530178   \n",
       "\n",
       "      Gender#skewShell Weight#log10  Gender#percentile_10Shell Weight#log10  \\\n",
       "0                         -0.960625                               -0.836841   \n",
       "1                         -1.900897                               -0.939302   \n",
       "2                         -1.136631                               -1.522879   \n",
       "3                         -1.900897                               -0.939302   \n",
       "4                         -0.960625                               -0.836841   \n",
       "...                             ...                                     ...   \n",
       "2919                      -1.136631                               -1.522879   \n",
       "2920                      -1.136631                               -1.522879   \n",
       "2921                      -1.136631                               -1.522879   \n",
       "2922                      -1.136631                               -1.522879   \n",
       "2923                      -0.960625                               -0.836841   \n",
       "\n",
       "      Gender#percentile_60Shell Weight#log10  \\\n",
       "0                                  -0.494850   \n",
       "1                                  -0.510042   \n",
       "2                                  -0.866142   \n",
       "3                                  -0.510042   \n",
       "4                                  -0.494850   \n",
       "...                                      ...   \n",
       "2919                               -0.866142   \n",
       "2920                               -0.866142   \n",
       "2921                               -0.866142   \n",
       "2922                               -0.866142   \n",
       "2923                               -0.494850   \n",
       "\n",
       "      Gender#percentile_90Shell Weight#log10  \n",
       "0                                  -0.338662  \n",
       "1                                  -0.349694  \n",
       "2                                  -0.610834  \n",
       "3                                  -0.349694  \n",
       "4                                  -0.338662  \n",
       "...                                      ...  \n",
       "2919                               -0.610834  \n",
       "2920                               -0.610834  \n",
       "2921                               -0.610834  \n",
       "2922                               -0.610834  \n",
       "2923                               -0.338662  \n",
       "\n",
       "[2924 rows x 5293 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_cat_generation(train)\n",
    "feature_cat_generation(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2065a709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category Encoding Completed\n"
     ]
    }
   ],
   "source": [
    "train_lab = train.copy()\n",
    "test_lab = test.copy()\n",
    "\n",
    "for col in train_lab.columns:\n",
    "    if train_lab[col].dtypes=='object':\n",
    "        train_lab[col] = train_lab[col].astype('category')\n",
    "        test_lab[col] = test_lab[col].astype('category')\n",
    "\n",
    "train_lgb_x = train_lab.drop(['Target'], axis=1) # 데이터 나누기\n",
    "train_lgb_y = train_lab['Target']\n",
    "test_lgb_x = test_lab.copy()\n",
    "\n",
    "print('Category Encoding Completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "022e76ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-Hot Encoding Completed\n"
     ]
    }
   ],
   "source": [
    "train_ohe = train.copy()\n",
    "test_ohe = test.copy()\n",
    "\n",
    "train_ohe = pd.get_dummies(train_ohe)\n",
    "test_ohe = pd.get_dummies(test_ohe)\n",
    "\n",
    "train_xgb_x = train_ohe.drop(['Target'], axis=1) # 데이터 나누기\n",
    "train_xgb_y = train_ohe['Target']\n",
    "test_xgb_x = test_ohe.copy()\n",
    "\n",
    "print('One-Hot Encoding Completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1da38c",
   "metadata": {},
   "source": [
    "XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7cfbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "(test==np.inf).sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafeb8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../data/train.csv\")\n",
    "test = pd.read_csv(\"../data/test.csv\")\n",
    "submission = pd.read_csv(\"../data/sample_submission.csv\")\n",
    "\n",
    "train = train.drop('id', axis=1)\n",
    "test = test.drop('id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6180e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.replace(0.0, 0.1)\n",
    "test = test.replace(0.0, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44976b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = []\n",
    "num_cols = []\n",
    "for col in train.columns:\n",
    "    if train[col].dtypes=='object':\n",
    "        cat_cols.append(col)\n",
    "    elif train[col].dtypes=='float64':\n",
    "        num_cols.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d72013",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_num_scaler(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b2e07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = []\n",
    "num_cols = []\n",
    "for col in train.columns:\n",
    "    if train[col].dtypes=='object':\n",
    "        cat_cols.append(col)\n",
    "    elif train[col].dtypes=='float64':\n",
    "        num_cols.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b7dbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "for num_col_first in num_cols:\n",
    "    for num_col_second in num_cols:\n",
    "        if (num_col_first != num_col_second)&((train[num_col_second]==0).sum()==0)&((test[num_col_second]==0).sum()==0):\n",
    "            train[num_col_first+'/'+num_col_second] = train[num_col_first] / train[num_col_second]\n",
    "            train[num_col_first+'*'+num_col_second] = train[num_col_first] * train[num_col_second]\n",
    "            test[num_col_first+'/'+num_col_second] = test[num_col_first] / test[num_col_second]\n",
    "            test[num_col_first+'*'+num_col_second] = test[num_col_first] * test[num_col_second]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a944505",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in test.columns:\n",
    "        if (test[col]==-np.inf).sum()!=0:\n",
    "            print(col)\n",
    "            cond = test[col]==-np.inf\n",
    "            print(test[cond])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da194d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cat_generation(train)\n",
    "feature_cat_generation(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2401bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ohe = train.copy()\n",
    "test_ohe = test.copy()\n",
    "\n",
    "train_ohe = pd.get_dummies(train_ohe)\n",
    "test_ohe = pd.get_dummies(test_ohe)\n",
    "\n",
    "train_xgb_x = train_ohe.drop(['Target'], axis=1) # 데이터 나누기\n",
    "train_xgb_y = train_ohe['Target']\n",
    "test_xgb_x = test_ohe.copy()\n",
    "\n",
    "print('One-Hot Encoding Completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb63fdf",
   "metadata": {},
   "source": [
    "## 이상치 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6aa513",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_name = 'Whole Weight'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c888ee25",
   "metadata": {},
   "outputs": [],
   "source": [
    "high = train[col_name].mean() + 3*train[col_name].std()\n",
    "low = train[col_name].mean() - 3*train[col_name].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2765cb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Highest allowed\", high)\n",
    "print(\"Lowest allowed\", low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694b870b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[(train[col_name] > high) | (train[col_name] < low)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187e4b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[~(train[col_name] > high) & ~(train[col_name] < low)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe1b93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[col_name] = np.where(train[col_name]>high, high, np.where(train[col_name]<low, low, train[col_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efd3099",
   "metadata": {},
   "source": [
    "# 2. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cab1b054",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dict = {}\n",
    "pred_test_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85418724",
   "metadata": {},
   "source": [
    "## (1) LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48c5f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_lgb_x.copy()\n",
    "train_y = train_lgb_y.copy()\n",
    "test_x = test_lgb_x.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03a4b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_objective(trial: Trial) -> float:\n",
    "    score_hp = []\n",
    "    for seed_hp in np.random.randint(0, 1000, num_seed_hp):\n",
    "        params_lgb = {\n",
    "            \"random_state\": seed_hp,\n",
    "            \"verbosity\": -1,\n",
    "#             \"metric\": \"mae\",\n",
    "            \"n_estimators\": 10000,\n",
    "            \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 2e-3, 1e-1), # default=0.1, range=[0,1]\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12), # default=-1\n",
    "            \"reg_alpha\": trial.suggest_loguniform(\"reg_alpha\", 1e-2, 1e+2), # default=0\n",
    "            \"reg_lambda\": trial.suggest_loguniform(\"reg_lambda\", 1e-2, 1e+2), # default=0\n",
    "            \"num_leaves\": trial.suggest_int(\"num_leaves\", 31, 5000), # default=31, range=(1,130172]\n",
    "            \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0.0, 1.0), # feature_fraction, default=1\n",
    "            \"subsample\": trial.suggest_uniform(\"subsample\", 0.0, 1.0), # bagging_fraction, default=1, range=[0,1]\n",
    "            \"subsample_freq\": trial.suggest_int(\"subsample_freq\", 1, 20), # bagging_freq, default=0\n",
    "            \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 1, 40), # min_data_in_leaf, default=20 \n",
    "#             \"max_bin\": trial.suggest_categoricacl(\"max_bin\", [15, 31, 63, 127, 255]),\n",
    "            \"max_bin\": trial.suggest_int(\"max_bin\", 100, 500),\n",
    "        }\n",
    "\n",
    "        kfold = StratifiedKFold(n_splits=splits_hp, random_state=seed_hp, shuffle=True) # Cross-validation cv=5\n",
    "        cv = np.zeros(rows_train)\n",
    "\n",
    "        for n, (train_idx, val_idx) in enumerate(kfold.split(train_x, train_y)):\n",
    "\n",
    "            x_train, x_val = train_x.iloc[train_idx], train_x.iloc[val_idx]\n",
    "            y_train, y_val = train_y.iloc[train_idx].values, train_y.iloc[val_idx].values\n",
    "\n",
    "            lgbmodel = LGBMRegressor(**params_lgb)\n",
    "                                                                                            # 진행상황 보고싶을때 -1을 100으로\n",
    "            lgbmodel.fit(x_train, y_train, eval_set=[(x_val, y_val)], early_stopping_rounds=30, verbose=-1) \n",
    "            cv[val_idx] = lgbmodel.predict(x_val)\n",
    "            \n",
    "        score_hp.append(mean_absolute_error(train_y, cv))\n",
    "    \n",
    "    np.mean(score_hp)\n",
    "    return np.mean(score_hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6766421a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = TPESampler(seed=basic_seed)\n",
    "lgb_study = optuna.create_study(study_name=\"lgb_parameter_opt\", direction=\"minimize\", sampler=sampler)\n",
    "lgb_study.optimize(lgb_objective, n_trials=num_trial)\n",
    "\n",
    "lgb_best_hyperparams = lgb_study.best_trial.params\n",
    "lgb_base_hyperparams = {'n_estimators':10000,\n",
    "#                         'lambda_l1':lgb_best_hyperparams['reg_alpha'],\n",
    "#                         'lambda_l2':lgb_best_hyperparams['reg_lambda']\n",
    "                       }\n",
    "lgb_best_hyperparams.update(lgb_base_hyperparams)\n",
    "\n",
    "with open('../pickle/lgb_best_hyperparams.pickle', 'wb') as fw:\n",
    "    pickle.dump(lgb_best_hyperparams, fw)\n",
    "print(\"The best hyperparameters are:\\n\", lgb_best_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f297638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optuna.visualization.matplotlib.plot_param_importances(lgb_study);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b18fa1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optuna.visualization.matplotlib.plot_slice(lgb_study);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38465e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../pickle/lgb_best_hyperparams.pickle', 'rb') as fw:\n",
    "    lgb_best_hyperparams = pickle.load(fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cba641",
   "metadata": {},
   "outputs": [],
   "source": [
    "lucky_seeds = np.random.randint(0, 1000, num_seed_tr)\n",
    "\n",
    "for i, seed in enumerate(lucky_seeds):\n",
    "\n",
    "    kfold = StratifiedKFold(n_splits=splits_tr, random_state=seed, shuffle=True) # CV 늘려가면서 하기\n",
    "    cv = np.zeros(rows_train)\n",
    "    pred_test = np.zeros(rows_test)\n",
    "\n",
    "    for n, (train_idx, val_idx) in enumerate(kfold.split(train_x, train_y)):\n",
    "        \n",
    "        x_train, x_val = train_x.iloc[train_idx], train_x.iloc[val_idx]\n",
    "        y_train, y_val = train_y.iloc[train_idx].values.ravel(), train_y.iloc[val_idx].values.ravel()\n",
    "\n",
    "        lgbmodel = LGBMRegressor(**lgb_best_hyperparams)\n",
    "        lgbmodel.fit(x_train, y_train, eval_set=[(x_val, y_val)], early_stopping_rounds=30, verbose=-1)\n",
    "        \n",
    "        cv[val_idx] = lgbmodel.predict(x_val)\n",
    "        pred_test += lgbmodel.predict(test_x) / splits_tr\n",
    "        \n",
    "    pred_dict['lgb'+str(seed)] = cv\n",
    "    pred_test_dict['lgb'+str(seed)] = pred_test\n",
    "    print(f'seed {seed}', 'mean_absolute_error :', mean_absolute_error(train_y, cv))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7c3867",
   "metadata": {},
   "source": [
    "## LGBM Dart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90cf44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def lgbd_objective(trial: Trial) -> float:\n",
    "#     score_hp = []\n",
    "#     for seed_hp in np.random.randint(0, 1000, num_seed_hp):\n",
    "#         params_lgbd = {\n",
    "#             \"random_state\": basic_seed,\n",
    "#             \"verbosity\": -1,\n",
    "# #             \"metric\": \"mae\",\n",
    "#             \"n_estimators\": 10000,\n",
    "#             \"boosting_type\": \"dart\",\n",
    "#             \"learning_rate\": trial.suggest_uniform(\"learning_rate\", 0.01, 0.1), # default=0.1, range=[0,1]\n",
    "# #             \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 2e-3, 1e-1), # default=0.1, range=[0,1]\n",
    "#             \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10), # default=-1\n",
    "#             \"reg_alpha\": trial.suggest_loguniform(\"reg_alpha\", 1e-2, 1e+2), # default=0\n",
    "#             \"reg_lambda\": trial.suggest_loguniform(\"reg_lambda\", 1e-2, 1e+2), # default=0\n",
    "#             \"num_leaves\": trial.suggest_int(\"num_leaves\", 31, 3000), # default=31, range=(1,130172]\n",
    "#             \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0.0, 1.0), # feature_fraction, default=1\n",
    "#             \"subsample\": trial.suggest_uniform(\"subsample\", 0.0, 1.0), # bagging_fraction, default=1, range=[0,1]\n",
    "#             \"subsample_freq\": trial.suggest_int(\"subsample_freq\", 1, 20), # bagging_freq, default=0\n",
    "#             \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 1, 40), # min_data_in_leaf, default=20 \n",
    "#             \"max_bin\": trial.suggest_int(\"max_bin\", 100, 500),\n",
    "#             \"drop_rate\": trial.suggest_uniform(\"drop_rate\", 0, 1), # only in dart, default=0.1, range=[0,1]\n",
    "#             \"max_drop\": trial.suggest_int(\"max_drop\", 20, 100), # only in dart, default=50, range=[0,1]\n",
    "#             \"skip_drop\": trial.suggest_uniform(\"skip_drop\", 0, 1), # only in dart, default=0.5, range=[0,1]\n",
    "#         }\n",
    "\n",
    "#         kfold = StratifiedKFold(n_splits=splits_hp, random_state=seed_hp, shuffle=True) # Cross-validation cv=5\n",
    "#         cv = np.zeros(rows_train)\n",
    "\n",
    "#         for n, (train_idx, val_idx) in enumerate(kfold.split(train_x, train_y)):\n",
    "\n",
    "#             x_train, x_val = train_x.iloc[train_idx], train_x.iloc[val_idx]\n",
    "#             y_train, y_val = train_y.iloc[train_idx].values, train_y.iloc[val_idx].values\n",
    "\n",
    "#             lgbdmodel = LGBMRegressor(**params_lgbd)\n",
    "#                                                                                             # 진행상황 보고싶을때 -1을 100으로\n",
    "#             lgbdmodel.fit(x_train, y_train, eval_set=[(x_val, y_val)], early_stopping_rounds=30, verbose=-1) \n",
    "#             cv[val_idx] = lgbdmodel.predict(x_val)\n",
    "            \n",
    "#         score_hp.append(mean_absolute_error(train_y, cv))\n",
    "    \n",
    "#     np.mean(score_hp)\n",
    "#     return np.mean(score_hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bd24db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampler = TPESampler(seed=basic_seed)\n",
    "# lgbd_study = optuna.create_study(study_name=\"lgbd_parameter_opt\", direction=\"minimize\", sampler=sampler)\n",
    "# lgbd_study.optimize(lgbd_objective, n_trials=num_trial)\n",
    "\n",
    "# lgbd_best_hyperparams = lgbd_study.best_trial.params\n",
    "# lgbd_base_hyperparams = {'n_estimators':10000, \"boosting_type\": \"dart\",\n",
    "# #                         'lambda_l1':lgbd_best_hyperparams['reg_alpha'],\n",
    "# #                         'lambda_l2':lgbd_best_hyperparams['reg_lambda']\n",
    "#                        }\n",
    "# lgbd_best_hyperparams.update(lgbd_base_hyperparams)\n",
    "\n",
    "# with open('../pickle/lgbd_best_hyperparams.pickle', 'wb') as fw:\n",
    "#     pickle.dump(lgbd_best_hyperparams, fw)\n",
    "# print(\"The best hyperparameters are:\\n\", lgbd_best_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f15ffe4",
   "metadata": {},
   "source": [
    "## (2) XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "462c3b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_xgb_x.copy()\n",
    "train_y = train_xgb_y.copy()\n",
    "test_x = test_xgb_x.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1cc1248d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_objective(trial: Trial) -> float:\n",
    "    score_hp = []\n",
    "    for seed_hp in np.random.randint(0, 1000, num_seed_hp):\n",
    "        params_xgb = {\n",
    "        \"random_state\": seed_hp,\n",
    "        \"verbose\": None,\n",
    "        \"learning_rate\": trial.suggest_uniform(\"learning_rate\", 1e-3, 1e-2), # eta, default=0.3, range=[0,1]\n",
    "        \"gamma\": trial.suggest_loguniform(\"gamma\", 1e-2, 1e+2), # min_split_loss, default=0, range=[0,∞]\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 4, 12), # default=5, range=[0,∞]\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10), #default=1\n",
    "        \"max_delta_step\" : trial.suggest_int(\"max_delta_step\", 0, 10), #default=0\n",
    "        \"subsample\": trial.suggest_uniform(\"subsample\", 0.0, 1.0), # default=1, range=(0,1]\n",
    "        \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0.0, 0.5), # default=1, range=(0,1]\n",
    "        \"colsample_bylevel\": trial.suggest_uniform(\"colsample_bylevel\", 0.0, 1.0), # default=1, range=(0,1]\n",
    "        \"colsample_bynode\": trial.suggest_uniform(\"colsample_bynode\", 0.0, 1.0), # default=1, range=(0,1]\n",
    "        \"reg_alpha\": trial.suggest_loguniform(\"reg_alpha\", 1e-1, 1e+1), # default=0, range=[0,∞]\n",
    "        \"reg_lambda\": trial.suggest_loguniform(\"reg_lambda\", 1e-1, 1e+1), # default=1, range=[0,∞]\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 100, 400),\n",
    "        }\n",
    "\n",
    "        kfold = StratifiedKFold(n_splits=splits_hp, random_state=seed_hp, shuffle=True) # Cross-validation cv=5\n",
    "        cv = np.zeros(rows_train)\n",
    "\n",
    "        for n, (train_idx, val_idx) in enumerate(kfold.split(train_x, train_y)):\n",
    "\n",
    "            x_train, x_val = train_x.iloc[train_idx], train_x.iloc[val_idx]\n",
    "            y_train, y_val = train_y.iloc[train_idx].values, train_y.iloc[val_idx].values\n",
    "            \n",
    "            dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "            dvalid = xgb.DMatrix(x_val, label=y_val)\n",
    "            watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n",
    "                                                                                                # 진행상황 보고싶을때 None을 100으로\n",
    "            xgbmodel = xgb.train(params_xgb, dtrain, 100000, watchlist, early_stopping_rounds=30, verbose_eval=None)\n",
    "            cv[val_idx] = xgbmodel.predict(dvalid)\n",
    "            \n",
    "        score_hp.append(mean_absolute_error(train_y, cv))\n",
    "    \n",
    "    np.mean(score_hp)\n",
    "#     pred_hp_dict['lgb'+str(seed)] = cv\n",
    "#     print(f'seed {seed}', 'mean_absolute_error :', mean_absolute_error(train_y, cv))\n",
    "    \n",
    "#     return mean_absolute_error(train_y, cv)\n",
    "    return np.mean(score_hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a68532",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-24 19:27:09,761]\u001b[0m A new study created in memory with name: xgb_parameter_opt\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "sampler = TPESampler(seed=basic_seed)\n",
    "xgb_study = optuna.create_study(study_name=\"xgb_parameter_opt\", direction=\"minimize\", sampler=sampler)\n",
    "xgb_study.optimize(xgb_objective, n_trials=num_trial)\n",
    "\n",
    "xgb_best_hyperparams = xgb_study.best_trial.params\n",
    "xgb_base_hyperparams = {\"random_state\": basic_seed}\n",
    "xgb_best_hyperparams.update(xgb_base_hyperparams)\n",
    "\n",
    "with open('../pickle/xgb_best_hyperparams.pickle', 'wb') as fw:\n",
    "    pickle.dump(xgb_best_hyperparams, fw)\n",
    "print(\"The best hyperparameters are:\\n\", xgb_best_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a08459",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = TPESampler(seed=basic_seed)\n",
    "xgb_study = optuna.create_study(study_name=\"xgb_parameter_opt\", direction=\"minimize\", sampler=sampler)\n",
    "xgb_study.optimize(xgb_objective, n_trials=num_trial)\n",
    "\n",
    "xgb_best_hyperparams = xgb_study.best_trial.params\n",
    "xgb_base_hyperparams = {\"random_state\": basic_seed}\n",
    "xgb_best_hyperparams.update(xgb_base_hyperparams)\n",
    "\n",
    "with open('../pickle/xgb_best_hyperparams.pickle', 'wb') as fw:\n",
    "    pickle.dump(xgb_best_hyperparams, fw)\n",
    "print(\"The best hyperparameters are:\\n\", xgb_best_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a48a555",
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.matplotlib.plot_param_importances(xgb_study);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326bafdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.matplotlib.plot_slice(xgb_study);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0985b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../pickle/xgb_best_hyperparams.pickle', 'rb') as fw:\n",
    "    xgb_best_hyperparams = pickle.load(fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4c0c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lucky_seeds = np.random.randint(0, 1000, num_seed_tr)\n",
    "xgtest = xgb.DMatrix(test_x)\n",
    "for i, seed in enumerate(lucky_seeds):\n",
    "\n",
    "    kfold = StratifiedKFold(n_splits=splits_tr, random_state=seed, shuffle = True) # CV 늘려가면서 하기\n",
    "    cv=np.zeros(rows_train)\n",
    "    pred_test = np.zeros(rows_test)\n",
    "\n",
    "    for n, (train_idx, val_idx) in enumerate(kfold.split(train_x, train_y)):\n",
    "        \n",
    "        x_train, x_val = train_x.iloc[train_idx], train_x.iloc[val_idx]\n",
    "        y_train, y_val = train_y.iloc[train_idx].values.ravel(), train_y.iloc[val_idx].values.ravel()\n",
    "        \n",
    "        dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "        dvalid = xgb.DMatrix(x_val, label=y_val)\n",
    "        watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n",
    "        \n",
    "                                                                                            # 진행상황 보고싶을때 None을 100으로\n",
    "        xgbmodel = xgb.train(xgb_best_hyperparams, dtrain, 100000, watchlist, early_stopping_rounds=30, verbose_eval=None)\n",
    "\n",
    "        cv[val_idx] = xgbmodel.predict(dvalid)\n",
    "        pred_test += xgbmodel.predict(xgtest) / splits_tr # CV 바꾸면 이 숫자도 똑같이 바꿔야함\n",
    "        \n",
    "    pred_dict['xgb'+str(seed)] = cv\n",
    "    pred_test_dict['xgb'+str(seed)] = pred_test\n",
    "    print(f'seed {seed}', 'mean_absolute_error :', mean_absolute_error(train_y, cv))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3d4a67",
   "metadata": {},
   "source": [
    "# 3. Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648daaaf",
   "metadata": {},
   "source": [
    "## (1) Collect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb1d754",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_dict(model, pred_dict, pred_test_dict):\n",
    "    pred_dict_local = {}\n",
    "    for key, value in pred_dict.items():\n",
    "        if model in key:\n",
    "            pred_dict_local[key]=value\n",
    "\n",
    "    pred_test_dict_local = {}\n",
    "    for key, value in pred_test_dict.items():\n",
    "        if model in key:\n",
    "            pred_test_dict_local[key]=value\n",
    "\n",
    "    pred_dict_new_local = dict(sorted(\n",
    "        pred_dict_local.items(), \n",
    "        key=lambda x:mean_absolute_error((train_y), list(x[1])), reverse=False)[:sel_seed])\n",
    "    pred_test_dict_new_local = {}\n",
    "    for key, value in pred_dict_new_local.items():\n",
    "        pred_test_dict_new_local[key]=pred_test_dict_local[key]\n",
    "        \n",
    "    return pred_dict_new_local, pred_test_dict_new_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5061cffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dict_lgb, pred_test_dict_lgb = sort_dict('lgb', pred_dict, pred_test_dict)\n",
    "pred_dict_xgb, pred_test_dict_xgb = sort_dict('xgb', pred_dict, pred_test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebce0c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dict(model, pred_dict, pred_test_dict):\n",
    "    with open('../pickle/pred_dict_'+model+'.pickle', 'wb') as fw:\n",
    "        pickle.dump(pred_dict, fw)\n",
    "    with open('../pickle/pred_test_dict_'+model+'.pickle', 'wb') as fw:\n",
    "        pickle.dump(pred_test_dict, fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f026fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dict('lgb', pred_dict_lgb, pred_test_dict_lgb)\n",
    "save_dict('xgb', pred_dict_xgb, pred_test_dict_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df06f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dict(model):\n",
    "    with open('../pickle/pred_dict_'+model+'.pickle', 'rb') as fw:\n",
    "        pred_dict_new_local = pickle.load(fw)\n",
    "    with open('../pickle/pred_test_dict_'+model+'.pickle', 'rb') as fw:\n",
    "        pred_test_dict_new_local = pickle.load(fw)\n",
    "    return pred_dict_new_local, pred_test_dict_new_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312683f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dict_lgb, pred_test_dict_lgb = load_dict('lgb')\n",
    "pred_dict_xgb, pred_test_dict_xgb = load_dict('xgb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4f6fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dict_total = {**pred_dict_lgb, **pred_dict_xgb}\n",
    "pred_test_dict_total = {**pred_test_dict_lgb, **pred_test_dict_xgb}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b98db1d",
   "metadata": {},
   "source": [
    "## (2) HP Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb8e583",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_objective(trial: Trial) -> float:\n",
    "    score_hp = []\n",
    "    for seed_hp in np.random.randint(0, 1000, 5):\n",
    "        params_xgb = {\n",
    "            \"random_state\": basic_seed,\n",
    "            \"verbose\": None,\n",
    "            \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 1e-1), # eta, default=0.3, range=[0,1]\n",
    "            \"gamma\": trial.suggest_loguniform(\"gamma\", 1e-2, 1e+2), # min_split_loss, default=0, range=[0,∞]\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 4, 10), # default=5, range=[0,∞]\n",
    "            \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10), #default=1\n",
    "            \"max_delta_step\" : trial.suggest_int(\"max_delta_step\", 0, 10), #default=0\n",
    "            \"subsample\": trial.suggest_uniform(\"subsample\", 0.0, 1.0), # default=1, range=(0,1]\n",
    "            \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0.0, 1.0), # default=1, range=(0,1]\n",
    "            \"colsample_bylevel\": trial.suggest_uniform(\"colsample_bylevel\", 0.0, 1.0), # default=1, range=(0,1]\n",
    "            \"colsample_bynode\": trial.suggest_uniform(\"colsample_bynode\", 0.0, 1.0), # default=1, range=(0,1]\n",
    "            \"reg_alpha\": trial.suggest_loguniform(\"reg_alpha\", 1e-2, 1e+2), # default=0, range=[0,∞]\n",
    "            \"reg_lambda\": trial.suggest_loguniform(\"reg_lambda\", 1e-2, 1e+2), # default=1, range=[0,∞]\n",
    "            \"max_bin\": trial.suggest_int(\"max_bin\", 100, 400),\n",
    "        }\n",
    "\n",
    "        kfold = StratifiedKFold(n_splits=splits_hp, random_state=seed_hp, shuffle=True)\n",
    "        cv = np.zeros(rows_train)\n",
    "\n",
    "        for n, (train_idx, val_idx) in enumerate(kfold.split(train_x, train_y)):\n",
    "\n",
    "            x_train, x_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "            y_train, y_val = train_y.iloc[train_idx], train_y.iloc[val_idx]\n",
    "\n",
    "            dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "            dvalid = xgb.DMatrix(x_val, label=y_val)\n",
    "            watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n",
    "                                                                                                # 진행상황 보고싶을때 None을 100으로\n",
    "            stack_xgbmodel = xgb.train(params_xgb, dtrain, 100000, watchlist, early_stopping_rounds=30, verbose_eval=None)\n",
    "            cv[val_idx] = stack_xgbmodel.predict(dvalid)\n",
    "\n",
    "        score_hp.append(mean_absolute_error(train_y, cv))\n",
    "    \n",
    "    np.mean(score_hp)\n",
    "#     pred_hp_dict['lgb'+str(seed)] = cv\n",
    "#     print(f'seed {seed}', 'mean_absolute_error :', mean_absolute_error(train_y, cv))\n",
    "    \n",
    "#     return mean_absolute_error(train_y, cv)\n",
    "    return np.mean(score_hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905e8225",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(np.vstack([x for _, x in pred_dict_total.items()]).T)\n",
    "X_test = pd.DataFrame(np.vstack([x for _, x in pred_test_dict_total.items()]).T)\n",
    "\n",
    "sampler = TPESampler(seed=basic_seed)\n",
    "stack_study = optuna.create_study(study_name=\"stack_parameter_opt\", direction=\"minimize\", sampler=sampler)\n",
    "stack_study.optimize(stack_objective, n_trials=num_trial)\n",
    "\n",
    "stack_best_hyperparams = stack_study.best_trial.params\n",
    "stack_base_hyperparams = {\"random_state\": basic_seed}\n",
    "stack_best_hyperparams.update(stack_base_hyperparams)\n",
    "print(\"The best hyperparameters are:\\n\", stack_best_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450cc12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = pd.DataFrame(np.vstack([x for _, x in pred_dict_total.items()]).T)\n",
    "# X_test = pd.DataFrame(np.vstack([x for _, x in pred_test_dict_total.items()]).T)\n",
    "\n",
    "# sampler = TPESampler(seed=basic_seed)\n",
    "# stack_study = optuna.create_study(study_name=\"stack_parameter_opt\", direction=\"minimize\", sampler=sampler)\n",
    "# stack_study.optimize(stack_objective, n_trials=num_trial)\n",
    "\n",
    "# stack_best_hyperparams = stack_study.best_trial.params\n",
    "# stack_base_hyperparams = {\"random_state\": basic_seed}\n",
    "# stack_best_hyperparams.update(stack_base_hyperparams)\n",
    "\n",
    "# with open('../pickle/stack_best_hyperparams.pickle', 'wb') as fw:\n",
    "#     pickle.dump(stack_best_hyperparams, fw)\n",
    "# print(\"The best hyperparameters are:\\n\", stack_best_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b06cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optuna.visualization.matplotlib.plot_param_importances(stack_study);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e71b736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optuna.visualization.matplotlib.plot_slice(stack_study);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609578e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../pickle/stack_best_hyperparams.pickle', 'rb') as fw:\n",
    "    stack_best_hyperparams = pickle.load(fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc49ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.zeros(rows_train)\n",
    "pred_test = np.zeros(rows_test)\n",
    "kfold = StratifiedKFold(n_splits=splits_tr, random_state=basic_seed, shuffle = True)\n",
    "\n",
    "for n, (train_idx, val_idx) in enumerate(kfold.split(X_train, train_y)):\n",
    "    x_train, x_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_train, y_val = train_y.iloc[train_idx], train_y.iloc[val_idx]\n",
    "\n",
    "    dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "    dvalid = xgb.DMatrix(x_val, label=y_val)\n",
    "    watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n",
    "                                                                                        # 진행상황 보고싶을때 None을 100으로\n",
    "    stack_xgbmodel = xgb.train(stack_best_hyperparams, dtrain, 100000, watchlist, early_stopping_rounds=30, verbose_eval=None)\n",
    "    \n",
    "    pred[val_idx] = stack_xgbmodel.predict(dvalid)\n",
    "    pred_test += stack_xgbmodel.predict(xgb.DMatrix(X_test)) / splits_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805c915c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'CV mean_absolute_error: {mean_absolute_error(train_y, pred):.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb2504d",
   "metadata": {},
   "source": [
    "# 4. Blending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38356128",
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_train = pred.copy()\n",
    "stack_test = pred_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00caa2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate = np.arange(0, 15)\n",
    "permute = permutations(candidate, 3)\n",
    "score = {}\n",
    "for i in list(permute):\n",
    "    pred_permute = (sum(pred_dict_lgb.values())/sel_seed * i[0] +\n",
    "                  sum(pred_dict_xgb.values())/sel_seed * i[1] +\n",
    "#               sum(pred_dict_cnn.values())/sel_seed * i[2] +\n",
    "#               sum(pred_dict_rcnn.values())/sel_seed * i[3] +\n",
    "                               stack_train * i[2])\n",
    "    score[i] = mean_absolute_error(train_y, pred_permute/sum(i))\n",
    "\n",
    "score = dict(sorted(score.items(), key=lambda x: x[1], reverse=False)[:5])\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a80421",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = (sum(pred_dict_lgb.values())/sel_seed * list(score.keys())[0][0] +\n",
    "        sum(pred_dict_xgb.values())/sel_seed * list(score.keys())[0][1] +\n",
    "#         sum(pred_dict_cnn.values())/sel_seed * list(score.keys())[0][2] +\n",
    "#         sum(pred_dict_rcnn.values())/sel_seed * list(score.keys())[0][3] +\n",
    "        stack_train * list(score.keys())[0][2]\n",
    "       ) / sum(list(score.keys())[0])\n",
    "mean_absolute_error(train_y, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60bd229",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = (sum(pred_test_dict_lgb.values())/3 * list(score.keys())[0][0] +\n",
    "             sum(pred_test_dict_xgb.values())/3 * list(score.keys())[0][1] +\n",
    "#              sum(pred_test_dict_cnn.values())/3 * list(score.keys())[0][2] +\n",
    "#              sum(pred_test_dict_rcnn.values())/3 * list(score.keys())[0][3] +\n",
    "             stack_test * list(score.keys())[0][2]\n",
    "            ) / sum(list(score.keys())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fc6fc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433d7bc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f73098a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da3e630",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate = np.arange(0, 11)\n",
    "permute = permutations(candidate,5)\n",
    "score = {}\n",
    "for i in list(permute):\n",
    "    pred_permute = (sum(pred_dict_lgb.values())/sel_seed * i[0] +\n",
    "                    sum(pred_dict_xgb.values())/sel_seed * i[1] +\n",
    "                    sum(pred_dict_cnn.values())/sel_seed * i[2] +\n",
    "                    sum(pred_dict_rcnn.values())/sel_seed * i[3] +\n",
    "                    stack_train * i[4]\n",
    "                   ) / \n",
    "    score[i] = mean_absolute_error(train_y, pred_permute/sum(i))\n",
    "\n",
    "score = dict(sorted(score.items(), key=lambda x: x[1], reverse=False)[:5])\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddbd619",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = (sum(pred_dict_lgb.values())/sel_seed * list(score.keys())[0][0] +\n",
    "        sum(pred_dict_xgb.values())/sel_seed * list(score.keys())[0][1] +\n",
    "        sum(pred_dict_cnn.values())/sel_seed * list(score.keys())[0][2] +\n",
    "        sum(pred_dict_rcnn.values())/sel_seed * list(score.keys())[0][3] +\n",
    "        stack_train * list(score.keys())[0][4]\n",
    "       ) / sum(list(score.keys())[0])\n",
    "mean_absolute_error(train_y, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c414698",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = (sum(pred_test_dict_lgb.values())/3 * list(score.keys())[0][0] +\n",
    "             sum(pred_test_dict_xgb.values())/3 * list(score.keys())[0][1] +\n",
    "             sum(pred_test_dict_cnn.values())/3 * list(score.keys())[0][2] +\n",
    "             sum(pred_test_dict_rcnn.values())/3 * list(score.keys())[0][3] +\n",
    "             stack_test * list(score.keys())[0][4]\n",
    "            ) / sum(list(score.keys())[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc79911",
   "metadata": {},
   "source": [
    "# 5. Weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7ab62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = (pred_dict['lgb493']+pred_dict['lgb112'])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14de6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = (pred_test_dict['lgb493']+pred_test_dict['lgb112'])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96896ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_dict = {}\n",
    "for target in tqdm(np.arange(4, 17)):\n",
    "    score_dict = {}\n",
    "    for weight in np.linspace(0.7, 1.3, 601):\n",
    "        score_dict[weight] = mean_absolute_error(train_y, np.where(((pred>target)&(pred<target+1)), pred*weight, pred))\n",
    "    weight_dict[target] = min(score_dict,key=score_dict.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df656f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in weight_dict.items():\n",
    "    pred = np.where(((pred>key)&(pred<key+1)), pred*value, pred)\n",
    "    pred_test = np.where(((pred_test>key)&(pred_test<key+1)), pred_test*value, pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6302743a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in weight_dict.items():\n",
    "    pred = np.where(((pred>key)&(pred<key+1)), pred*value, pred)\n",
    "    pred_test = np.where(((pred_test>key)&(pred_test<key+1)), pred_test*value, pred_test)\n",
    "min_target = list(weight_dict.keys())[0]\n",
    "max_target = list(weight_dict.keys())[-1]\n",
    "min_weight = weight_dict[min_target]\n",
    "max_weight = weight_dict[max_target]\n",
    "pred = np.where(pred<min_target, pred*min_weight, pred)\n",
    "pred = np.where(pred>max_target+1, pred*max_weight, pred)\n",
    "pred_test = np.where(pred_test<min_target, pred_test*min_weight, pred_test)\n",
    "pred_test = np.where(pred_test>max_target+1, pred_test*max_weight, pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9215f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.Target = pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b979f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_name = '20220324'\n",
    "submission_number = '1'\n",
    "submission.to_csv(f'../submission/{submission_name}-{submission_number}.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
