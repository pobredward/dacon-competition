{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2335, 34)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sensor_1</th>\n",
       "      <th>sensor_2</th>\n",
       "      <th>sensor_3</th>\n",
       "      <th>sensor_4</th>\n",
       "      <th>sensor_5</th>\n",
       "      <th>sensor_6</th>\n",
       "      <th>sensor_7</th>\n",
       "      <th>sensor_8</th>\n",
       "      <th>sensor_9</th>\n",
       "      <th>...</th>\n",
       "      <th>sensor_24</th>\n",
       "      <th>sensor_25</th>\n",
       "      <th>sensor_26</th>\n",
       "      <th>sensor_27</th>\n",
       "      <th>sensor_28</th>\n",
       "      <th>sensor_29</th>\n",
       "      <th>sensor_30</th>\n",
       "      <th>sensor_31</th>\n",
       "      <th>sensor_32</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-6.149463</td>\n",
       "      <td>-0.929714</td>\n",
       "      <td>9.058368</td>\n",
       "      <td>-7.017854</td>\n",
       "      <td>-2.958471</td>\n",
       "      <td>0.179233</td>\n",
       "      <td>-0.956591</td>\n",
       "      <td>-0.972401</td>\n",
       "      <td>5.956213</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.026436</td>\n",
       "      <td>-6.006282</td>\n",
       "      <td>-6.005836</td>\n",
       "      <td>7.043084</td>\n",
       "      <td>21.884650</td>\n",
       "      <td>-3.064152</td>\n",
       "      <td>-5.247552</td>\n",
       "      <td>-6.026107</td>\n",
       "      <td>-11.990822</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>-2.238836</td>\n",
       "      <td>-1.003511</td>\n",
       "      <td>5.098079</td>\n",
       "      <td>-10.880357</td>\n",
       "      <td>-0.804562</td>\n",
       "      <td>-2.992123</td>\n",
       "      <td>26.972724</td>\n",
       "      <td>-8.900861</td>\n",
       "      <td>-5.968298</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.996714</td>\n",
       "      <td>-7.933806</td>\n",
       "      <td>-3.136773</td>\n",
       "      <td>8.774211</td>\n",
       "      <td>10.944759</td>\n",
       "      <td>9.858186</td>\n",
       "      <td>-0.969241</td>\n",
       "      <td>-3.935553</td>\n",
       "      <td>-15.892421</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>19.087934</td>\n",
       "      <td>-2.092514</td>\n",
       "      <td>0.946750</td>\n",
       "      <td>-21.831788</td>\n",
       "      <td>9.119235</td>\n",
       "      <td>17.853587</td>\n",
       "      <td>-21.069954</td>\n",
       "      <td>-15.933212</td>\n",
       "      <td>-9.016039</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.889685</td>\n",
       "      <td>54.052330</td>\n",
       "      <td>-6.109238</td>\n",
       "      <td>12.154595</td>\n",
       "      <td>6.095989</td>\n",
       "      <td>-40.195088</td>\n",
       "      <td>-3.958124</td>\n",
       "      <td>-8.079537</td>\n",
       "      <td>-5.160090</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>-2.211629</td>\n",
       "      <td>-1.930904</td>\n",
       "      <td>21.888406</td>\n",
       "      <td>-3.067560</td>\n",
       "      <td>-0.240634</td>\n",
       "      <td>2.985056</td>\n",
       "      <td>-29.073369</td>\n",
       "      <td>0.200774</td>\n",
       "      <td>-1.043742</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.126170</td>\n",
       "      <td>-1.035526</td>\n",
       "      <td>2.178769</td>\n",
       "      <td>10.032723</td>\n",
       "      <td>-1.010897</td>\n",
       "      <td>-3.912848</td>\n",
       "      <td>-2.980338</td>\n",
       "      <td>-12.983597</td>\n",
       "      <td>-3.001077</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3.953852</td>\n",
       "      <td>2.964892</td>\n",
       "      <td>-36.044802</td>\n",
       "      <td>0.899838</td>\n",
       "      <td>26.930210</td>\n",
       "      <td>11.004409</td>\n",
       "      <td>-21.962423</td>\n",
       "      <td>-11.950189</td>\n",
       "      <td>-20.933785</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.051761</td>\n",
       "      <td>10.917567</td>\n",
       "      <td>1.905335</td>\n",
       "      <td>-13.004707</td>\n",
       "      <td>17.169552</td>\n",
       "      <td>2.105194</td>\n",
       "      <td>3.967986</td>\n",
       "      <td>11.861657</td>\n",
       "      <td>-27.088846</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id   sensor_1  sensor_2   sensor_3   sensor_4   sensor_5   sensor_6  \\\n",
       "0   1  -6.149463 -0.929714   9.058368  -7.017854  -2.958471   0.179233   \n",
       "1   2  -2.238836 -1.003511   5.098079 -10.880357  -0.804562  -2.992123   \n",
       "2   3  19.087934 -2.092514   0.946750 -21.831788   9.119235  17.853587   \n",
       "3   4  -2.211629 -1.930904  21.888406  -3.067560  -0.240634   2.985056   \n",
       "4   5   3.953852  2.964892 -36.044802   0.899838  26.930210  11.004409   \n",
       "\n",
       "    sensor_7   sensor_8   sensor_9  ...  sensor_24  sensor_25  sensor_26  \\\n",
       "0  -0.956591  -0.972401   5.956213  ...  -7.026436  -6.006282  -6.005836   \n",
       "1  26.972724  -8.900861  -5.968298  ...  -1.996714  -7.933806  -3.136773   \n",
       "2 -21.069954 -15.933212  -9.016039  ...  -6.889685  54.052330  -6.109238   \n",
       "3 -29.073369   0.200774  -1.043742  ...  -2.126170  -1.035526   2.178769   \n",
       "4 -21.962423 -11.950189 -20.933785  ...  -2.051761  10.917567   1.905335   \n",
       "\n",
       "   sensor_27  sensor_28  sensor_29  sensor_30  sensor_31  sensor_32  target  \n",
       "0   7.043084  21.884650  -3.064152  -5.247552  -6.026107 -11.990822       1  \n",
       "1   8.774211  10.944759   9.858186  -0.969241  -3.935553 -15.892421       1  \n",
       "2  12.154595   6.095989 -40.195088  -3.958124  -8.079537  -5.160090       0  \n",
       "3  10.032723  -1.010897  -3.912848  -2.980338 -12.983597  -3.001077       1  \n",
       "4 -13.004707  17.169552   2.105194   3.967986  11.861657 -27.088846       2  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "\n",
    "tf.random.set_seed(42) #재현을 위한 텐서플로우 seed 설정\n",
    "\n",
    "data_train = pd.read_csv('./data/train.csv')\n",
    "data_test = pd.read_csv('./data/test.csv')\n",
    "print(data_train.shape)\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터를 보게되면 train data의 수가 2335로 매우 적으며, test data의 수가 train data의 약 5배 정도로 많은 것을 볼 수 있습니다.\n",
    "\n",
    "따라서 저는 처음에 모델 튜닝보다는, train data의 수를 늘리는 방법에 대해 고민하는것이 훨씬 이득이라고 생각해서 train data를 증강해보기 위해서 다양하게 접근하였으나, 모두 실패하였습니다. (여기서 너무 많은 시간을 소모했습니다... sensor가 어떤 센서인지 도통 알수가 없네요)\n",
    "\n",
    "현재 사용하고 있는 증강법은 데이터의 노이즈를 추가하는 방법 중 swap noise 하나밖에 없습니다. swap noise에 대해서는 아래 블로그에 잘 나와있습니다.\n",
    "\n",
    "https://velog.io/@vvakki_/Tabular-Data%EC%A0%95%ED%98%95-%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%97%90%EC%84%9C%EC%9D%98-Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = [\"data_train\", \"data test\"]\n",
    "y = [data_train.shape[0], data_test.shape[0]]\n",
    "ax = sns.barplot(x=x, y=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 전처리\n",
    "\n",
    "relu 함수를 activation에서 사용하고 있기 때문에 음수 값을 모두 양수로 만들어 dying relu를 만들지 않도록 했습니다.\n",
    "\n",
    "또한 기존에 하던 것처럼 0~1값으로 Normalization을 진행해 입력의 범위를 맞춰주었으며, 값의 첨도가 높아 log변환을 시도해보았지만 성능이 더욱 좋지 않게 나와 그냥 나눠주었습니다.\n",
    "\n",
    "https://yamalab.tistory.com/48 \n",
    "\n",
    "위 블로그에서 발췌한 글입니다. \n",
    "\n",
    "input값에 음수가 포함이 된다면 기울기가 0이 되버리므로, 미분을 하면 backpropagation 과정 중간에 꺼져버리는 상황이 발생한다. Chain rule로 미분을 하기 때문에 음수가 한번 나오면 뒤에서도 다 꺼진다. 따라서 input 데이터에서 음수값이 포함되지 않도록 0~1사이의 값으로 정규화 시키는 과정을 거치는 것이 좋다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = data_train.drop(['target', 'id'], axis = 1)\n",
    "train_X = (train_X+130)/260 \n",
    "train_X = np.array(train_X)\n",
    "train_X = np.array(train_X).reshape(-1, 8, 4, 1)\n",
    "\n",
    "train_Y = data_train['target']\n",
    "train_Y = np.array(train_Y)\n",
    "\n",
    "X_test = data_test.drop('id', axis = 1)\n",
    "X_test = (X_test+130)/260\n",
    "X_test = np.array(X_test).reshape(-1, 8, 4, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min : 0.008470580769230742 \n",
      "max : 0.9890809815384616\n"
     ]
    }
   ],
   "source": [
    "print(\"min :\", train_X.min(), \"\\nmax :\", train_X.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2335, 8, 4, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2335,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 생성하기\n",
    "\n",
    "\n",
    "기세현님이 공유해주신 방식처럼 CNN으로 예측하는 방식이 가장 성능이 좋게 나와 저도 CNN을 기반으로 접근해보았습니다.\n",
    "\n",
    "https://dacon.io/codeshare/4653\n",
    "\n",
    "모델은 이전에 제가 사물 이미지 분류 경진대회에서 만들었던 ResNet 구조에, bottleneck 구조를 살려서 구현한 모델을 사용했습니다.\n",
    "\n",
    "https://dacon.io/codeshare/4618"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_block(X, filters, kernel_size):\n",
    "    X_shortcut = X\n",
    "    \n",
    "    X = tf.keras.layers.Conv2D(filters, (1,1), padding='SAME')(X)\n",
    "    X = tf.keras.layers.BatchNormalization()(X)\n",
    "    X = tf.keras.layers.Activation('relu')(X)\n",
    "    \n",
    "    X = tf.keras.layers.Conv2D(filters, kernel_size, padding='SAME')(X)\n",
    "    X = tf.keras.layers.BatchNormalization()(X)\n",
    "    X = tf.keras.layers.Activation('relu')(X)\n",
    "    \n",
    "    X = tf.keras.layers.Conv2D(filters*4, (1,1), padding='SAME')(X)\n",
    "    X = tf.keras.layers.BatchNormalization()(X)\n",
    "    \n",
    "    # Add\n",
    "    X = tf.keras.layers.Add()([X, X_shortcut])\n",
    "    X = tf.keras.layers.Activation('relu')(X)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolutional_block(X, filters, kernel_size):\n",
    "    X_shortcut = X\n",
    "    \n",
    "    X = tf.keras.layers.Conv2D(filters, (1,1), padding='SAME')(X)\n",
    "    X = tf.keras.layers.BatchNormalization()(X)\n",
    "    X = tf.keras.layers.Activation('relu')(X)\n",
    "    \n",
    "    X = tf.keras.layers.Conv2D(filters, kernel_size, padding='SAME')(X)\n",
    "    X = tf.keras.layers.BatchNormalization()(X)\n",
    "    X = tf.keras.layers.Activation('relu')(X)\n",
    "    \n",
    "    X = tf.keras.layers.Conv2D(filters*4, (1,1), padding='SAME')(X)\n",
    "    X = tf.keras.layers.BatchNormalization()(X)\n",
    "\n",
    "    X_shortcut = tf.keras.layers.Conv2D(filters*4, (1,1), padding='SAME')(X_shortcut) #use 1x1 conv to make shape same\n",
    "    X_shortcut = tf.keras.layers.BatchNormalization()(X_shortcut)\n",
    "    \n",
    "    # Add\n",
    "    X = tf.keras.layers.Add()([X, X_shortcut])\n",
    "    X = tf.keras.layers.Activation('relu')(X)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "구글에 cnn rectangular images라고 검색하니 나온 이 article에서 음성 신호에서 감정을 인식하기 위해, 음성 신호를 image로 변환한 직사각형 input을 사용하고 있습니다. 여기서 구현한 모델을 보면 직사각형 input에는 직사각형 필터를 사용하고 있기 때문에 저도 **(3,2) (2,1) 처럼 직사각형 필터를 사용했습니다.** (성능이 향상되었습니다.)\n",
    "\n",
    "https://www.researchgate.net/publication/320737885_Deep_features-based_speech_emotion_recognition_for_smart_affective_services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = https://www.researchgate.net/profile/Jamil-Ahmad-13/publication/320737885/figure/fig2/AS:941749673414722@1601542027738/Proposed-CNN-architecture-with-rectangular-kernels.gif>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CustomModel(input_shape = (8, 4, 1), classes = 4):\n",
    "    X_input = tf.keras.layers.Input(input_shape)\n",
    "    X = X_input\n",
    "    \n",
    "    X = convolutional_block(X, 128, (3,2)) #(3,3) 보다는 직사각형 이미지이기때문에 (3,2)처럼 직사각형 필터를 사용\n",
    "    X = identity_block(X, 128, (3,2))\n",
    "    X = identity_block(X, 128, (3,2))\n",
    "    \n",
    "    X = tf.keras.layers.AveragePooling2D(2,2)(X) #Max보다는 Average pool이 성능이 잘나옴\n",
    "\n",
    "    X = convolutional_block(X, 256, (2,1)) #(2,1) 직사각형 필터를 사용\n",
    "    X = identity_block(X, 256, (2,1))\n",
    "    X = identity_block(X, 256, (2,1))\n",
    "    \n",
    "    X = tf.keras.layers.GlobalAveragePooling2D()(X) #Flatten 대신 사용\n",
    "    \n",
    "    X = tf.keras.layers.Dense(128, activation = \"relu\")(X)\n",
    "    \n",
    "    X = tf.keras.layers.Dropout(0.5)(X)\n",
    "    \n",
    "    X = tf.keras.layers.Dense(classes, activation = \"softmax\")(X)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs = X_input, outputs = X, name = \"CustomModel\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 학습하기\n",
    "\n",
    "우선 위에서 설명했던것처럼 train set이 너무 적고, test 셋이 train set에 비해 너무 많기 때문에 k-fold를 이용한 예측값을 사용하여 앙상블하게되면, 성능이 높게 올라가는 것을 알 수 있습니다.\n",
    "\n",
    "따라서 train셋을 조금이라도 늘릴 방법에 대해서 중점적으로 접근해보았고, 많은 실패를 겪고 결국 swap noise 하나만 사용하게 되었습니다.\n",
    "\n",
    "또한 Fold 횟수를 15번으로 설정한 것이 가장 점수가 높게 나와, 15번의 fold를 진행하도록 했습니다. (train data set이 너무 적어서 그런것 같습니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dict = {}\n",
    "pred_test_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                                                  99ee´´                                                                                                                                                                                                                                                                                                                                                                                                                                                                       9                      9 9 9 9ed                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       ······································································································                                                                                                                  ed´´´´´´´´´´´´´´´´´      ed··························································ª·······························································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/zv/m8v1knh55xzd5dkqt5p3cxcm0000gn/T/ipykernel_65365/4135154498.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m### fit model ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         data = model.fit(X_train, \n\u001b[0m\u001b[1;32m     74\u001b[0m                          \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                          \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                 _r=1):\n\u001b[1;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2954\u001b[0m       (graph_function,\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2956\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2957\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1854\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "lucky_seeds = np.random.randint(1, 100, 5) # Lucky seed 늘려가면서 하기\n",
    "for i, seed in enumerate(lucky_seeds):\n",
    "\n",
    "    skf = StratifiedKFold(n_splits = 15, random_state = 42, shuffle = True) #총 15번의 fold 진행\n",
    "    n = 0 #x번째 fold인지 기록\n",
    "    cv=np.zeros((train_X.shape[0], 4))\n",
    "    pred_test = np.zeros((X_test.shape[0], 4), dtype=float)\n",
    "\n",
    "    cnn_pred = [] #모델의 예측값 모두 저장\n",
    "\n",
    "    for train_index, valid_index in skf.split(train_X, train_Y):\n",
    "        n += 1\n",
    "        X_train, X_valid = train_X[train_index], train_X[valid_index]\n",
    "        y_train, y_valid = train_Y[train_index], train_Y[valid_index]\n",
    "\n",
    "        ### Swap Noise ###\n",
    "        X_train_mix = np.array(X_train)\n",
    "        for x in range(X_train_mix.shape[0]):\n",
    "            for i in range(5):\n",
    "                y = np.random.randint(0, 8)\n",
    "                z = np.random.randint(0, 4)\n",
    "\n",
    "                while True:\n",
    "                    c = np.random.randint(0, X_train_mix.shape[0]-1)\n",
    "                    if ((x != c)and(y_train[x] == y_train[c])):\n",
    "                        break\n",
    "\n",
    "                X_train_mix[x][y][z] = X_train[c][y][z]\n",
    "\n",
    "        X_train = np.append(X_train, X_train_mix, axis = 0)\n",
    "        y_train = np.append(y_train, y_train, axis = 0)\n",
    "\n",
    "        ### Mix Data Again ####\n",
    "        X_train, y_train = shuffle(X_train, y_train, random_state=42)\n",
    "\n",
    "        y_train = tf.one_hot(y_train, 4)\n",
    "        y_train = tf.reshape(y_train, [-1,4])\n",
    "        y_train = np.array(y_train)\n",
    "\n",
    "        y_valid = tf.one_hot(y_valid, 4)\n",
    "        y_valid = tf.reshape(y_valid, [-1,4])\n",
    "        y_valid = np.array(y_valid)\n",
    "\n",
    "        ### Create Model ###\n",
    "        model = CustomModel()\n",
    "\n",
    "        ### Compile Model ###\n",
    "        model.compile(optimizer='adam', # 무난한 adam 사용\n",
    "                      loss='categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        ### Create callbacks ###\n",
    "        filename = 'CNN-checkpoint.h5'\n",
    "        checkpoint = tf.keras.callbacks.ModelCheckpoint(filename,             # file명을 지정합니다\n",
    "                                                        monitor='val_accuracy',   # val_accuracy 값이 개선되었을때 호출됩니다\n",
    "                                                        verbose=0,            # 로그를 출력합니다 0일경우 출력 X\n",
    "                                                        save_best_only=True,  # 가장 best 값만 저장합니다\n",
    "                                                        mode='auto'           # auto는 알아서 best를 찾습니다. min/max (loss->min, accuracy->max)\n",
    "                                                       )\n",
    "        earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',  # 모니터 기준 설정 (val loss) \n",
    "                                      patience=12,         # 12 Epoch동안 개선되지 않는다면 종료\n",
    "                                     )\n",
    "        reduceLR = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_accuracy', # val_accuracy 값이 개선되었을때 호출됩니다\n",
    "            factor=0.5, # learning rate이 0.5배 줄어듬\n",
    "            patience=6, # 6 Epoch동안 개선되지 않는다면 호출\n",
    "        )\n",
    "\n",
    "        ### fit model ###\n",
    "        data = model.fit(X_train, \n",
    "                         y_train, \n",
    "                         validation_data=(X_valid, y_valid), \n",
    "                         epochs=60, \n",
    "                         batch_size=32, # batch size가 32일때 가장 좋은 성능을 보임\n",
    "                         callbacks=[reduceLR, earlystopping, checkpoint],\n",
    "                         verbose=0 # 로그 출력을 없앰, 어짜피 아래 print에서 한번에 best_accuracy만 출력할것이기 때문이다.\n",
    "                        )\n",
    "\n",
    "        idx = data.history['val_accuracy'].index(max(data.history['val_accuracy']))\n",
    "\n",
    "#         print(\"fold %d / val_accuracy : %0.4f / val_loss : %0.4f\" %(n,\n",
    "#                                                                     data.history['val_accuracy'][idx], \n",
    "#                                                                     data.history['val_loss'][idx]))\n",
    "\n",
    "        ### predict model ###\n",
    "        model = tf.keras.models.load_model('./CNN-checkpoint.h5') # best accuracy를 기록한 모델을 불러옴\n",
    "\n",
    "        cv[valid_index,:] = model.predict(X_valid)\n",
    "        #print(f'fold{n+1} multi_logloss: {log_loss(y_val, cv[valid_index, :])}') # Fold마다 점수 체크하려면 주석 해제\n",
    "        pred_test += model.predict(X_test) / 15 # CV 바꾸면 이 숫자도 똑같이 바꿔야함\n",
    "\n",
    "    pred_dict['cnn2'+ str(seed)] = cv\n",
    "    pred_test_dict['cnn2'+str(seed)] = pred_test\n",
    "#     print(f'seed {seed}', 'multi_logloss :', log_loss(train_y, cv))\n",
    "    print(f'seed {seed}', 'accuracy_score :', accuracy_score(train_Y, np.argmax(cv, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def sort_dict(model, pred_dict, pred_test_dict):\n",
    "    pred_dict_local = {}\n",
    "    for key, value in pred_dict.items():\n",
    "        if model in key:\n",
    "            pred_dict_local[key]=value\n",
    "\n",
    "    pred_test_dict_local = {}\n",
    "    for key, value in pred_test_dict.items():\n",
    "        if model in key:\n",
    "            pred_test_dict_local[key]=value\n",
    "\n",
    "    pred_dict_new_local = dict(sorted(pred_dict_local.items(), key=lambda x:log_loss(train_Y, list(x[1])), reverse=False)[:3])\n",
    "    pred_test_dict_new_local = {}\n",
    "    for key, value in pred_dict_new_local.items():\n",
    "        pred_test_dict_new_local[key]=pred_test_dict_local[key]\n",
    "        \n",
    "    return pred_dict_new_local, pred_test_dict_new_local\n",
    "pred_dict_cnn2, pred_test_dict_cnn2 = sort_dict('cnn2', pred_dict, pred_test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dict(model, pred_dict, pred_test_dict):\n",
    "    with open('./pkl/pred_dict_'+model+'.pickle', 'wb') as fw:\n",
    "        pickle.dump(pred_dict, fw)\n",
    "\n",
    "    with open('./pkl/pred_test_dict_'+model+'.pickle', 'wb') as fw:\n",
    "        pickle.dump(pred_test_dict, fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dir(dir):\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "        print(\"Created Directory :\", dir)\n",
    "    else:\n",
    "        print(\"Directory already existed :\", dir)\n",
    "create_dir(\"pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dict('cnn2', pred_dict_cnn2, pred_test_dict_cnn2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 제출하기\n",
    "\n",
    "이전에 계산했던 model.predict 값이 저장된 cnn_pred 리스트에 있는 값을 모두 더한 후, argmax를 이용하여 예측값을 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_proba = cnn_pred[0]\n",
    "pred_proba = np.array(pred_proba)\n",
    "print(pred_proba[0])\n",
    "\n",
    "for x in range(1, 15):\n",
    "    pred_proba += cnn_pred[x]\n",
    "    print(cnn_pred[x][0])\n",
    "\n",
    "pred_class = []\n",
    "\n",
    "for i in pred_proba:\n",
    "    pred = np.argmax(i)\n",
    "    pred_class.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sample_submission = pd.read_csv(\"./sample_submission.csv\")\n",
    "\n",
    "sample_submission.target = pred_class\n",
    "sample_submission.to_csv(\"submit_25.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
