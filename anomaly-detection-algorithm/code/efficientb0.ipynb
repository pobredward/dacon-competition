{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf10b07d",
   "metadata": {},
   "source": [
    "# 1. 라이브러리, 데이터 로드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49bdb92",
   "metadata": {},
   "source": [
    "### (1) 라이브러리 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "10a5166a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from tqdm.auto import tqdm\n",
    "import cv2\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "import timm\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import time\n",
    "import albumentations as albu\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# IMAGENET_DEFAULT_MEAN = (0.485, 0.456, 0.406)\n",
    "# IMAGENET_DEFAULT_STD = (0.229, 0.224, 0.225)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7f15a3",
   "metadata": {},
   "source": [
    "### (2) 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5011c3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_img_path = np.array(sorted(glob('../data/train/*.png'))) # train data 경로와 파일 이름 저장\n",
    "test_img_path = np.array(sorted(glob('../data/test/*.png'))) # test data 경로와 파일 이름 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8936a780",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = pd.read_csv(\"../data/train_df.csv\") # 파일 이름과 라벨값이 담긴 데이터프레임 로드\n",
    "\n",
    "train_labels = train_y[\"label\"] # 라벨 값 가져오기\n",
    "\n",
    "label_unique = sorted(np.unique(train_labels)) # 라벨 값 중 unique한 값 88개 추출\n",
    "label_unique = {key:value for key,value in zip(label_unique, range(len(label_unique)))} # 88개 값을 숫자로 인코딩 준비\n",
    "\n",
    "all_label = np.array([label_unique[k] for k in train_labels]) # train data의 label(target) 문자열들을 숫자 형식으로 인코딩"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4834343",
   "metadata": {},
   "source": [
    "### (3) 파라미터 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac4531bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'IMG_SIZE':512,\n",
    "    'EPOCHS':50,\n",
    "    'LEARNING_RATE':1e-2,\n",
    "    'BATCH_SIZE':8,\n",
    "    'SEED':42\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e2e322",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_train = len(all_img_path) # 주어진 train data의 개수\n",
    "rows_test = len(test_img_path) # 주어진 test data의 개수\n",
    "num_trial = 100 # 파라미터 튜닝을 몇 번 진행하는지의 수\n",
    "splits_hp = 5 # 파라미터 튜닝을 진행할 때의 kfold 수\n",
    "splits_tr = 5 # 모델 트레이닝을 진행할 때의 kfold 수\n",
    "basic_seed = 42 # default seed\n",
    "num_seed_tr = 10 # 트레이닝 seed 개수\n",
    "sel_seed = 3 # 선택할 seed 개수\n",
    "num_classes = len(label_unique) # class의 개수\n",
    "\n",
    "pred_dict = {}\n",
    "pred_test_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e049615f",
   "metadata": {},
   "source": [
    "# 2. 함수 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14351a1c",
   "metadata": {},
   "source": [
    "### Dataset 가져오는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd1ab98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, img_path_list, label_list, train_mode=True, transforms=None):\n",
    "        self.transforms = transforms\n",
    "        self.train_mode = train_mode\n",
    "        self.img_path_list = img_path_list\n",
    "        self.label_list = label_list\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.img_path_list[index]\n",
    "        # Get image data\n",
    "        img = cv2.imread(img_path)\n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(image=img)[\"image\"]\n",
    "\n",
    "        if self.train_mode:\n",
    "            label = self.label_list[index]\n",
    "            return image, label\n",
    "        else:\n",
    "            return image\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_path_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f96cb37",
   "metadata": {},
   "source": [
    "### Network 정의 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e23c608d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        self.model = timm.create_model('efficientnet_b0', pretrained=True, num_classes=88)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dd5eeb",
   "metadata": {},
   "source": [
    "### transform 정의 (Data augmentation 시 사용) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8321f69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = albu.Compose([\n",
    "    albu.Resize(CFG['IMG_SIZE'], CFG['IMG_SIZE']),\n",
    "#     albu.RandomCrop(CFG['IMG_SIZE']-64, CFG['IMG_SIZE']-64),\n",
    "#     albu.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1,\n",
    "#                           rotate_limit=30, interpolation=1, border_mode=0,\n",
    "#                           value=0, p=0.5),\n",
    "#     albu.HorizontalFlip(p=0.2),\n",
    "#     albu.VerticalFlip(p=0.2),\n",
    "#     albu.RandomRotate90(p=0.2),\n",
    "#     albu.CLAHE(clip_limit=2, p=0.25),\n",
    "#     albu.Sharpen(p=0.25),\n",
    "#     albu.RandomBrightnessContrast(brightness_limit=(-0.1, 0.1),\n",
    "#                                   contrast_limit=(-0.1, 0.1), p=0.25),\n",
    "#     albu.RandomResizedCrop(height=CFG['IMG_SIZE'], width=CFG['IMG_SIZE'],\n",
    "#                            scale=(0.5, 1.0), ratio=(0.75, 1.3333333333333333),\n",
    "#                            interpolation=1, p=1.0),\n",
    "#     albu.Normalize(mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD, max_pixel_value=512.0, p=1.0),\n",
    "    ToTensorV2()]\n",
    ")\n",
    "\n",
    "valid_transform = albu.Compose([\n",
    "    albu.Resize(CFG['IMG_SIZE'], CFG['IMG_SIZE']),\n",
    "#     albu.HorizontalFlip(p=0.5),\n",
    "#     albu.VerticalFlip(p=0.5),\n",
    "#     albu.Normalize(mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD, max_pixel_value=512.0, p=1.0),\n",
    "    ToTensorV2()]\n",
    ")\n",
    "\n",
    "test_transform = albu.Compose([\n",
    "    albu.Resize(CFG['IMG_SIZE'], CFG['IMG_SIZE']),\n",
    "#     albu.Normalize(mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD, max_pixel_value=512.0, p=1.0),\n",
    "    ToTensorV2()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048cafbe",
   "metadata": {},
   "source": [
    "### 모델 학습 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7993bc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_loader, vali_loader, scheduler, device):\n",
    "    model.to(device)\n",
    "\n",
    "    # Loss Function\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    best_score = 0\n",
    "    \n",
    "    for epoch in range(1,CFG[\"EPOCHS\"]+1):\n",
    "        train_pred=[]\n",
    "        train_y=[]\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for img, label in tqdm(iter(train_loader)):\n",
    "            \n",
    "            img, label = img.float().to(device), label.float().to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Data -> Model -> Output\n",
    "            logit = model(img)\n",
    "            label = label.to(torch.int64)\n",
    "            loss = criterion(logit, label)\n",
    "\n",
    "            # backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()/len(train_loader)\n",
    "            train_pred += logit.argmax(1).detach().cpu().numpy().tolist()\n",
    "            train_y += label.detach().cpu().numpy().tolist()\n",
    "            \n",
    "        # Evaluation Validation set\n",
    "        vali_score = validation(model, vali_loader, device)\n",
    "        \n",
    "        # vali_score가 더 이상 커지지 않으면\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(vali_score)\n",
    "        \n",
    "        print(f'Epoch [{epoch}] Train Score : [{score_function(train_y, train_pred):.5f}] Validation Score : [{vali_score:.5f}]\\n')\n",
    "        \n",
    "        # Model Saved\n",
    "        if best_score < vali_score:\n",
    "            best_score = vali_score\n",
    "            torch.save(model.state_dict(), '../model/best_model.pth')\n",
    "            print('Model Saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206ecc9a",
   "metadata": {},
   "source": [
    "### 검증 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05a9d155",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, vali_loader, device):\n",
    "    model.eval() # Evaluation\n",
    "    logit_list = []\n",
    "    label_list = []\n",
    "    with torch.no_grad():\n",
    "        for img, label in tqdm(iter(vali_loader)):\n",
    "            img, label = img.float().to(device), label.float().to(device)\n",
    "            label = label.to(torch.int64)\n",
    "\n",
    "            logit_list.extend(model(img).argmax(1).detach().cpu().numpy().tolist())\n",
    "            label_list.extend(label.detach().cpu().numpy().tolist())\n",
    "\n",
    "    vali_f1_score = score_function(label_list, logit_list)\n",
    "    return vali_f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d61ae5",
   "metadata": {},
   "source": [
    "### 예측 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1e6bacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomDataset(test_img_path, None, train_mode=False, transforms=test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=False, num_workers=0)\n",
    "\n",
    "def predict(model, test_loader, device):\n",
    "    model.eval()\n",
    "    model_pred = []\n",
    "    with torch.no_grad():\n",
    "        for img in tqdm(iter(test_loader)):\n",
    "            \n",
    "            img = img.float().to(device)\n",
    "\n",
    "            pred_logit = model(img).detach().cpu()\n",
    "            model_pred.extend(pred_logit.tolist())\n",
    "    return model_pred\n",
    "\n",
    "def score_function(real, pred):\n",
    "    score = f1_score(real, pred, average=\"macro\")\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86ccbca",
   "metadata": {},
   "source": [
    "# 3. 학습 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7f6908",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=splits_tr, random_state=0, shuffle=True)\n",
    "cv = np.zeros((rows_train, num_classes))\n",
    "pred_test = np.zeros((rows_test, num_classes))\n",
    "for n, (train_idx, val_idx) in enumerate(kfold.split(all_img_path, all_label)):\n",
    "    \n",
    "    train_img_path, vali_img_path = all_img_path[train_idx], all_img_path[val_idx]\n",
    "    train_label, vali_label = all_label[train_idx], all_label[val_idx]\n",
    "    \n",
    "    # Get Dataloader\n",
    "    train_dataset = CustomDataset(train_img_path.tolist(), train_label.tolist(), train_mode=True, transforms=train_transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=True, num_workers=0)\n",
    "\n",
    "    vali_dataset = CustomDataset(vali_img_path.tolist(), vali_label.tolist(), train_mode=True, transforms=valid_transform)\n",
    "    vali_loader = DataLoader(vali_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=False, num_workers=0)\n",
    "    \n",
    "    model = Network().to(device)\n",
    "    optimizer = torch.optim.SGD(params = model.parameters(), lr = CFG[\"LEARNING_RATE\"])\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode='max', patience=3, factor=0.2)\n",
    "    \n",
    "    train(model, optimizer, train_loader, vali_loader, scheduler, device)\n",
    "    \n",
    "    checkpoint = torch.load('../model/best_model.pth')\n",
    "    model = Network().to(device)\n",
    "    model.load_state_dict(checkpoint)\n",
    "    \n",
    "    vali_dataset = CustomDataset(vali_img_path.tolist(), vali_label.tolist(), train_mode=False, transforms=test_transform)\n",
    "    vali_loader = DataLoader(vali_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=False, num_workers=0)\n",
    "    \n",
    "    cv[val_idx, :] = predict(model, vali_loader, device)\n",
    "    pred_test += np.array(predict(model, test_loader, device)) / splits_tr\n",
    "    print(f\"Fold {n+1} Score: {score_function(vali_label, cv[val_idx].argmax(axis=1))}\")\n",
    "    \n",
    "pred_dict['eff_b0'+str(seed)] = cv\n",
    "pred_test_dict['eff_b0'+str(seed)] = pred_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dadb9d",
   "metadata": {},
   "source": [
    "# 4. 학습 결과 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d1aa8407",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_dict(model, pred_dict, pred_test_dict):\n",
    "    pred_dict_local = {}\n",
    "    for key, value in pred_dict.items():\n",
    "        if model in key:\n",
    "            pred_dict_local[key]=value\n",
    "\n",
    "    pred_test_dict_local = {}\n",
    "    for key, value in pred_test_dict.items():\n",
    "        if model in key:\n",
    "            pred_test_dict_local[key]=value\n",
    "\n",
    "    pred_dict_new_local = dict(sorted(\n",
    "        pred_dict_local.items(), \n",
    "        key=lambda x:score_function((all_label), np.argmax(list(x[1]), axis=1)), reverse=False)[:5])\n",
    "    pred_test_dict_new_local = {}\n",
    "    for key, value in pred_dict_new_local.items():\n",
    "        pred_test_dict_new_local[key]=pred_test_dict_local[key]\n",
    "        \n",
    "    return pred_dict_new_local, pred_test_dict_new_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ec92fa49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dict(model, pred_dict, pred_test_dict):\n",
    "    with open('../pickle/pred_dict_'+model+'.pickle', 'wb') as fw:\n",
    "        pickle.dump(pred_dict, fw)\n",
    "    with open('../pickle/pred_test_dict_'+model+'.pickle', 'wb') as fw:\n",
    "        pickle.dump(pred_test_dict, fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1bc815ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dict_efficientb0, pred_test_dict_efficientb0 = sort_dict('efficientb0', pred_dict, pred_test_dict)\n",
    "save_dict('efficientb0', pred_dict_efficientb0, pred_test_dict_efficientb0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac561815",
   "metadata": {},
   "source": [
    "# 5. 제출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c1b0ce22",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_decoder = {val:key for key, val in label_unique.items()}\n",
    "pred_final = [label_decoder[result] for result in pred_test.argmax(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e0587dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "submission = pd.read_csv('../data/sample_submission.csv')\n",
    "submission[\"label\"] = pred_final\n",
    "submission_time = datetime.today().strftime('%Y-%m-%d-%M-%S')\n",
    "submission.to_csv(f'../submission/{submission_time}.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4e73a637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>tile-glue_strip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>grid-good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>transistor-misplaced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>tile-gray_stroke</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>tile-good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2149</th>\n",
       "      <td>2149</td>\n",
       "      <td>tile-gray_stroke</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150</th>\n",
       "      <td>2150</td>\n",
       "      <td>screw-good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2151</th>\n",
       "      <td>2151</td>\n",
       "      <td>grid-good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2152</th>\n",
       "      <td>2152</td>\n",
       "      <td>cable-good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2153</th>\n",
       "      <td>2153</td>\n",
       "      <td>zipper-good</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2154 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                 label\n",
       "0         0       tile-glue_strip\n",
       "1         1             grid-good\n",
       "2         2  transistor-misplaced\n",
       "3         3      tile-gray_stroke\n",
       "4         4             tile-good\n",
       "...     ...                   ...\n",
       "2149   2149      tile-gray_stroke\n",
       "2150   2150            screw-good\n",
       "2151   2151             grid-good\n",
       "2152   2152            cable-good\n",
       "2153   2153           zipper-good\n",
       "\n",
       "[2154 rows x 2 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
