{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "charming-investigator",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LlcNsGEk7L5r",
   "metadata": {
    "id": "LlcNsGEk7L5r"
   },
   "outputs": [],
   "source": [
    "name = \"resnetrs270_cv10_lr0002_batch32\"\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from tqdm.auto import tqdm\n",
    "import cv2\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "import timm\n",
    "import random\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "device = torch.device('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FWntO1VD7L5u",
   "metadata": {
    "id": "FWntO1VD7L5u"
   },
   "outputs": [],
   "source": [
    "train_png = sorted(glob('../data/train/*.png'))\n",
    "test_png = sorted(glob('../data/test/*.png'))\n",
    "\n",
    "train_y = pd.read_csv(\"../data/train_df.csv\")\n",
    "\n",
    "train_labels = train_y[\"label\"]\n",
    "\n",
    "label_unique = sorted(np.unique(train_labels))\n",
    "label_unique = {key:value for key,value in zip(label_unique, range(len(label_unique)))}\n",
    "\n",
    "train_labels = [label_unique[k] for k in train_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brazilian-samoa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_load(path):\n",
    "    img = cv2.imread(path)[:,:,::-1]\n",
    "    img = cv2.resize(img, (384, 384),interpolation = cv2.INTER_AREA)\n",
    "    return img\n",
    "\n",
    "# train_imgs = [img_load(m) for m in tqdm(train_png)]\n",
    "# test_imgs = [img_load(n) for n in tqdm(test_png)]\n",
    "\n",
    "# np.save('../data/train_imgs_384', np.array(train_imgs))\n",
    "# np.save('../data/test_imgs_384', np.array(test_imgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "L6qBdX7nCp8L",
   "metadata": {
    "id": "L6qBdX7nCp8L"
   },
   "outputs": [],
   "source": [
    "train_imgs = np.load('../data/train_imgs_384.npy')\n",
    "test_imgs = np.load('../data/test_imgs_384.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sscGLiJKPy6H",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sscGLiJKPy6H",
    "outputId": "976516c0-507d-458f-f0c4-6695751605d6"
   },
   "outputs": [],
   "source": [
    "# meanRGB = [np.mean(x, axis=(0,1)) for x in train_imgs]\n",
    "# stdRGB = [np.std(x, axis=(0,1)) for x in train_imgs]\n",
    "\n",
    "# meanR = np.mean([m[0] for m in meanRGB])/255\n",
    "# meanG = np.mean([m[1] for m in meanRGB])/255\n",
    "# meanB = np.mean([m[2] for m in meanRGB])/255\n",
    "\n",
    "# stdR = np.mean([s[0] for s in stdRGB])/255\n",
    "# stdG = np.mean([s[1] for s in stdRGB])/255\n",
    "# stdB = np.mean([s[2] for s in stdRGB])/255\n",
    "\n",
    "# print(\"train 평균\",meanR, meanG, meanB)\n",
    "# print(\"train 표준편차\",stdR, stdG, stdB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JwVIQCrUSCFE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JwVIQCrUSCFE",
    "outputId": "293981ef-7f6a-4602-d208-5fd8473d5261"
   },
   "outputs": [],
   "source": [
    "# meanRGB = [np.mean(x, axis=(0,1)) for x in test_imgs]\n",
    "# stdRGB = [np.std(x, axis=(0,1)) for x in test_imgs]\n",
    "\n",
    "# meanR = np.mean([m[0] for m in meanRGB])/255\n",
    "# meanG = np.mean([m[1] for m in meanRGB])/255\n",
    "# meanB = np.mean([m[2] for m in meanRGB])/255\n",
    "\n",
    "# stdR = np.mean([s[0] for s in stdRGB])/255\n",
    "# stdG = np.mean([s[1] for s in stdRGB])/255\n",
    "# stdB = np.mean([s[2] for s in stdRGB])/255\n",
    "\n",
    "# print(\"test 평균\",meanR, meanG, meanB)\n",
    "# print(\"test 표준편차\",stdR, stdG, stdB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VFXzojoo7L5y",
   "metadata": {
    "id": "VFXzojoo7L5y"
   },
   "outputs": [],
   "source": [
    "class Custom_dataset(Dataset):\n",
    "    def __init__(self, img_paths, labels, mode='train'):\n",
    "        self.img_paths = img_paths\n",
    "        self.labels = labels\n",
    "        self.mode=mode\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.img_paths[idx]\n",
    "        if self.mode == 'train':\n",
    "            train_transform = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean = [0.433038, 0.403458, 0.394151],\n",
    "                                     std = [0.181572, 0.174035, 0.163234]),\n",
    "                transforms.RandomAffine((-45, 45)),\n",
    "            ])\n",
    "            img = train_transform(img)\n",
    "        if self.mode == 'test':\n",
    "            test_transform = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean = [0.418256, 0.393101, 0.386632],\n",
    "                                     std = [0.195055, 0.190053, 0.185323])\n",
    "            ])\n",
    "            img = test_transform(img)\n",
    "\n",
    "        \n",
    "        label = self.labels[idx]\n",
    "        return img, label\n",
    "    \n",
    "class Network(nn.Module):\n",
    "    def __init__(self,mode = 'train'):\n",
    "        super(Network, self).__init__()\n",
    "        self.mode = mode\n",
    "        if self.mode == 'train':\n",
    "            self.model = timm.create_model('resnetrs270', pretrained=True, num_classes=88, drop_path_rate = 0.2)\n",
    "        if self.mode == 'test':\n",
    "            self.model = timm.create_model('resnetrs270', pretrained=True, num_classes=88, drop_path_rate = 0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38qk8sGbYiO_",
   "metadata": {
    "id": "38qk8sGbYiO_"
   },
   "outputs": [],
   "source": [
    "def score_function(real, pred):\n",
    "    score = f1_score(real, pred, average=\"macro\")\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EnOG2n30-Dz5",
   "metadata": {
    "id": "EnOG2n30-Dz5"
   },
   "outputs": [],
   "source": [
    "def main(seed = 2022):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "main(2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governing-status",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train_dict = {}\n",
    "pred_test_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prostate-omega",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "cv = StratifiedKFold(n_splits = 10, random_state = 2022, shuffle=True)\n",
    "batch_size = 32\n",
    "epochs = 70\n",
    "pred_ensemble = []\n",
    "\n",
    "\n",
    "for idx, (train_idx, val_idx) in enumerate(cv.split(train_imgs, np.array(train_labels))):\n",
    "#     print(\"----------fold_{} start!----------\".format(idx))\n",
    "    t_imgs, val_imgs = train_imgs[train_idx],  train_imgs[val_idx]\n",
    "    t_labels, val_labels = np.array(train_labels)[train_idx], np.array(train_labels)[val_idx]\n",
    "\n",
    "    # Train\n",
    "    train_dataset = Custom_dataset(np.array(t_imgs), np.array(t_labels), mode='train')\n",
    "    train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "    # Val\n",
    "    val_dataset = Custom_dataset(np.array(val_imgs), np.array(val_labels), mode='test')\n",
    "    val_loader = DataLoader(val_dataset, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    best=0\n",
    "\n",
    "    model = Network().to(device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4, weight_decay = 2e-2)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scaler = torch.cuda.amp.GradScaler()  \n",
    "\n",
    "    best_f1 = 0\n",
    "    early_stopping = 0\n",
    "    for epoch in range(epochs):\n",
    "        start=time.time()\n",
    "        train_loss = 0\n",
    "        train_pred=[]\n",
    "        train_y=[]\n",
    "        model.train()\n",
    "        for batch in (train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            x = torch.tensor(batch[0], dtype=torch.float32, device=device)\n",
    "            y = torch.tensor(batch[1], dtype=torch.long, device=device)\n",
    "            with torch.cuda.amp.autocast():\n",
    "                pred = model(x)\n",
    "            loss = criterion(pred, y)\n",
    "\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            train_loss += loss.item()/len(train_loader)\n",
    "            train_pred += pred.argmax(1).detach().cpu().numpy().tolist()\n",
    "            train_y += y.detach().cpu().numpy().tolist()\n",
    "        train_f1 = score_function(train_y, train_pred)\n",
    "        state_dict= model.state_dict()\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0 \n",
    "            val_pred = []\n",
    "            val_y = []\n",
    "\n",
    "\n",
    "            for batch in (val_loader):\n",
    "                x_val = torch.tensor(batch[0], dtype = torch.float32, device = device)\n",
    "                y_val = torch.tensor(batch[1], dtype=torch.long, device=device)\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    pred_val = model(x_val)\n",
    "                loss_val = criterion(pred_val, y_val)\n",
    "\n",
    "                val_loss += loss_val.item()/len(val_loader)\n",
    "                val_pred += pred_val.argmax(1).detach().cpu().numpy().tolist()\n",
    "                val_y += y_val.detach().cpu().numpy().tolist()\n",
    "            val_f1 = score_function(val_y, val_pred)\n",
    "            print(f'fold{idx+1} epoch{epoch} score: {val_f1:.5f}')\n",
    "\n",
    "            if val_f1 > best_f1:\n",
    "                best_epoch = epoch\n",
    "                best_loss = val_loss\n",
    "                best_f1 = val_f1\n",
    "                early_stopping = 0\n",
    "\n",
    "                torch.save({'epoch':epoch,\n",
    "                            'state_dict':state_dict,\n",
    "                            'optimizer': optimizer.state_dict(),\n",
    "                            'scaler': scaler.state_dict(),\n",
    "                     }, f'../model/{name}_best_model_{idx+1}.pth')\n",
    "#                 print('-----------------SAVE:{} epoch----------------'.format(best_epoch+1))\n",
    "            else:\n",
    "                early_stopping += 1\n",
    "\n",
    "                # Early Stopping\n",
    "        if early_stopping == 20:\n",
    "            TIME = time.time() - start\n",
    "            print(f'epoch : {epoch+1}/{epochs}    time : {TIME:.0f}s/{TIME*(epochs-epoch-1):.0f}s')\n",
    "            print(f'TRAIN    loss : {train_loss:.5f}    f1 : {train_f1:.5f}')\n",
    "            print(f'Val    loss : {val_loss:.5f}    f1 : {val_f1:.5f}')\n",
    "            break\n",
    "\n",
    "    TIME = time.time() - start\n",
    "    print(f'epoch : {epoch+1}/{epochs}    time : {TIME:.0f}s/{TIME*(epochs-epoch-1):.0f}s')\n",
    "    print(f'TRAIN    loss : {train_loss:.5f}    f1 : {train_f1:.5f}')\n",
    "    print(f'Val    loss : {val_loss:.5f}    f1 : {val_f1:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diagnostic-prime",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = np.zeros((len(train_imgs), 88))\n",
    "pred_test = np.zeros((len(test_imgs), 88))\n",
    "\n",
    "test_dataset = Custom_dataset(np.array(test_imgs), np.array([\"tmp\"]*len(test_imgs)), mode='test')\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "for idx, (train_idx, val_idx) in enumerate(cv.split(train_imgs, np.array(train_labels))):\n",
    "    print(\"----------fold_{} predict start!----------\".format(idx+1))\n",
    "    \n",
    "    t_imgs, val_imgs = train_imgs[train_idx],  train_imgs[val_idx]\n",
    "    t_labels, val_labels = np.array(train_labels)[train_idx], np.array(train_labels)[val_idx]\n",
    "\n",
    "    # Val\n",
    "    val_dataset = Custom_dataset(np.array(val_imgs), np.array(val_labels), mode='test')\n",
    "    val_loader = DataLoader(val_dataset, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    model_test = Network(mode = 'test').to(device)\n",
    "    model_test.load_state_dict(torch.load((f'../model/{name}_best_model_{idx+1}.pth'))['state_dict'])\n",
    "    model_test.eval()\n",
    "    \n",
    "    pred_train_list = []\n",
    "    with torch.no_grad():\n",
    "        for batch in (val_loader):\n",
    "            x = torch.tensor(batch[0], dtype = torch.float32, device = device)\n",
    "            with torch.cuda.amp.autocast():\n",
    "                pred_train_local = model_test(x)\n",
    "                pred_train_list.extend(pred_train_local.detach().cpu().numpy())\n",
    "                \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    model_test = Network(mode = 'test').to(device)\n",
    "    model_test.load_state_dict(torch.load((f'../model/{name}_best_model_{idx+1}.pth'))['state_dict'])\n",
    "    model_test.eval()\n",
    "            \n",
    "    pred_test_list = []\n",
    "    with torch.no_grad():\n",
    "        for batch in (test_loader):\n",
    "            x = torch.tensor(batch[0], dtype = torch.float32, device = device)\n",
    "            with torch.cuda.amp.autocast():\n",
    "                pred_test_local = model_test(x)\n",
    "                pred_test_list.extend(pred_test_local.detach().cpu().numpy())\n",
    "                \n",
    "    pred_train[val_idx, :] = pred_train_list\n",
    "    pred_test += np.array(pred_test_list) / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welsh-luxury",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train_dict[f'{name}_seed{str(2022)}'] = pred_train\n",
    "pred_test_dict[f'{name}_seed{str(2022)}'] = pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabulous-today",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_dict(model, pred_dict, pred_test_dict):\n",
    "    pred_dict_local = {}\n",
    "    for key, value in pred_dict.items():\n",
    "        if model in key:\n",
    "            pred_dict_local[key]=value\n",
    "\n",
    "    pred_test_dict_local = {}\n",
    "    for key, value in pred_test_dict.items():\n",
    "        if model in key:\n",
    "            pred_test_dict_local[key]=value\n",
    "\n",
    "    pred_dict_new_local = dict(sorted(\n",
    "        pred_dict_local.items(), \n",
    "        key=lambda x:score_function((train_labels), np.argmax(list(x[1]), axis=1)), reverse=False)[:5])\n",
    "    pred_test_dict_new_local = {}\n",
    "    for key, value in pred_dict_new_local.items():\n",
    "        pred_test_dict_new_local[key]=pred_test_dict_local[key]\n",
    "        \n",
    "    return pred_dict_new_local, pred_test_dict_new_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "available-envelope",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dict(model, pred_dict, pred_test_dict):\n",
    "    with open('../pickle/pred_train_dict_'+model+'.pickle', 'wb') as fw:\n",
    "        pickle.dump(pred_dict, fw)\n",
    "    with open('../pickle/pred_test_dict_'+model+'.pickle', 'wb') as fw:\n",
    "        pickle.dump(pred_test_dict, fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ambient-button",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train_dict_global, pred_test_dict_global = sort_dict(name, pred_train_dict, pred_test_dict)\n",
    "save_dict(name, pred_train_dict_global, pred_test_dict_global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artificial-wrong",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "[BASELINE]_EfficientNet_b3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
