{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7aebeef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from glob import glob\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "\n",
    "import optuna\n",
    "from optuna import Trial\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.visualization import plot_contour, plot_optimization_history\n",
    "from optuna.visualization import plot_parallel_coordinate, plot_slice, plot_param_importances\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from catboost import Pool, CatBoostClassifier\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cf0651d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../data/train.csv\", index_col=\"id\")\n",
    "test = pd.read_csv(\"../data/test.csv\", index_col=\"id\")\n",
    "submission = pd.read_csv(\"../data/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f727d13a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already existed : ../pickle\n",
      "Directory already existed : ../model\n",
      "Directory already existed : ../submission\n"
     ]
    }
   ],
   "source": [
    "def create_dir(dir):\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "        print(\"Created Directory :\", dir)\n",
    "    else:\n",
    "        print(\"Directory already existed :\", dir)\n",
    "create_dir(\"../pickle\")\n",
    "create_dir(\"../model\")\n",
    "create_dir(\"../submission\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db4dcb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train['text']\n",
    "train_y = train['target']\n",
    "test_x = test['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e48114ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_train = train.shape[0] # 주어진 train data의 row 수\n",
    "rows_test = test.shape[0] # 주어진 test data의 row 수\n",
    "num_classes = len(train_y.unique())\n",
    "num_trial = 100 # 파라미터 튜닝을 몇 번 진행하는지의 수\n",
    "splits_hp = 3 # 파라미터 튜닝을 진행할 때의 kfold 수\n",
    "splits_tr = 3 # 모델 트레이닝을 진행할 때의 kfold 수\n",
    "basic_seed = 42 # default seed\n",
    "num_seed_tr = 5 # 트레이닝 seed 개수\n",
    "sel_seed = 3 # 선택할 seed 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "105cbc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dict = {}\n",
    "pred_test_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fa7996",
   "metadata": {},
   "source": [
    "# XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "434703fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_objective(trial: Trial) -> float:\n",
    "    score_hp = []\n",
    "    for seed_hp in [0]:\n",
    "        params_xgb = {\n",
    "            \"random_state\": seed_hp,\n",
    "            \"verbose\": None,        \n",
    "            \"num_class\": num_classes,\n",
    "            \"objective\": \"multi:softprob\",\n",
    "            \"eval_metric\": \"mlogloss\",\n",
    "#             \"tree_method\": \"gpu_hist\",\n",
    "            \"learning_rate\": trial.suggest_uniform(\"learning_rate\", 5e-2, 1e-1), # eta, default=0.3, range=[0,1]\n",
    "            \"gamma\": trial.suggest_loguniform(\"gamma\", 1e-2, 1e+2), # min_split_loss, default=0, range=[0,∞]\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 4, 12), # default=5, range=[0,∞]\n",
    "            \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10), #default=1\n",
    "            \"max_delta_step\" : trial.suggest_int(\"max_delta_step\", 0, 10), #default=0\n",
    "            \"subsample\": trial.suggest_uniform(\"subsample\", 0.0, 1.0), # default=1, range=(0,1]\n",
    "            \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0.2, 1.0), # default=1, range=(0,1]\n",
    "            \"colsample_bylevel\": trial.suggest_uniform(\"colsample_bylevel\", 0.2, 1.0), # default=1, range=(0,1]\n",
    "            \"colsample_bynode\": trial.suggest_uniform(\"colsample_bynode\", 0.2, 1.0), # default=1, range=(0,1]\n",
    "            \"reg_alpha\": trial.suggest_loguniform(\"reg_alpha\", 1e-1, 1e+1), # default=0, range=[0,∞]\n",
    "            \"reg_lambda\": trial.suggest_loguniform(\"reg_lambda\", 1e-1, 1e+1), # default=1, range=[0,∞]\n",
    "            \"max_bin\": trial.suggest_int(\"max_bin\", 100, 400),\n",
    "        }\n",
    "\n",
    "        kfold = StratifiedKFold(n_splits=splits_hp, random_state=seed_hp, shuffle=True)\n",
    "        cv = np.zeros((rows_train, num_classes))\n",
    "\n",
    "        for n, (train_idx, val_idx) in enumerate(kfold.split(train_x, train_y)):\n",
    "            \n",
    "            x_train, x_val = train_x.iloc[train_idx], train_x.iloc[val_idx]\n",
    "            y_train, y_val = train_y.iloc[train_idx].values.ravel(), train_y.iloc[val_idx].values.ravel()\n",
    "\n",
    "            vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "            vectorizer.fit(x_train)\n",
    "            x_train = vectorizer.transform(x_train)\n",
    "            x_val = vectorizer.transform(x_val)\n",
    "            x_test = vectorizer.transform(test_x)\n",
    "\n",
    "            dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "            dvalid = xgb.DMatrix(x_val, label=y_val)\n",
    "\n",
    "            watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n",
    "            XGBModel = xgb.train(params_xgb, dtrain, 10000, watchlist, early_stopping_rounds=50, verbose_eval=None)\n",
    "            \n",
    "            cv[val_idx, :] = XGBModel.predict(dvalid)\n",
    "        \n",
    "#             print(f\"fold{n+1} log_loss:\", log_loss(y_val, cv[val_idx]))\n",
    "            score_hp.append(log_loss(y_val, cv[val_idx]))\n",
    "            break\n",
    "        \n",
    "#         score_hp.append(log_loss(train_y, cv))\n",
    "    \n",
    "    np.mean(score_hp)\n",
    "    return np.mean(score_hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a042c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = TPESampler(seed=basic_seed)\n",
    "xgb_study = optuna.create_study(study_name=\"xgb_parameter_opt\", direction=\"minimize\", sampler=sampler)\n",
    "xgb_study.optimize(xgb_objective, n_trials=num_trial)\n",
    "\n",
    "xgb_best_hyperparams = xgb_study.best_trial.params\n",
    "xgb_base_hyperparams = {\"random_state\": basic_seed, \"verbose\": None, \"num_class\": num_classes, \n",
    "                        \"objective\": \"multi:softprob\", \"eval_metric\": \"mlogloss\"}\n",
    "xgb_best_hyperparams.update(xgb_base_hyperparams)\n",
    "\n",
    "with open('../pickle/xgb_best_hyperparams.pickle', 'wb') as fw:\n",
    "    pickle.dump(xgb_best_hyperparams, fw)\n",
    "print(\"The best hyperparameters are:\\n\", xgb_best_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37386a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_best_hyperparams = {'learning_rate': 0.08827098485602952,\n",
    "                         'gamma': 0.07068974950624607,\n",
    "                         'max_depth': 5,\n",
    "                         'min_child_weight': 2,\n",
    "                         'max_delta_step': 3,\n",
    "                         'subsample': 0.5247564316322378,\n",
    "                         'colsample_bytree': 0.5455560149136927,\n",
    "                         'colsample_bylevel': 0.43298331215843355,\n",
    "                         'colsample_bynode': 0.6894823157779035,\n",
    "                         'reg_alpha': 0.19010245319870356,\n",
    "                         'reg_lambda': 0.3839629299804172,\n",
    "                         'max_bin': 210,\n",
    "                         'random_state': 42,\n",
    "                         'verbose': None,\n",
    "                         'num_class': 20,\n",
    "                         'objective': 'multi:softprob',\n",
    "                         'eval_metric': 'mlogloss'\n",
    "                       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba31a8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lucky_seeds = np.random.randint(0, 1000, num_seed_tr)\n",
    "\n",
    "for i, seed in enumerate(lucky_seeds):\n",
    "\n",
    "    kfold = StratifiedKFold(n_splits=splits_tr, random_state=seed, shuffle=True)\n",
    "    cv = np.zeros((rows_train, num_classes))\n",
    "    pred_test = np.zeros((rows_test, num_classes))\n",
    "\n",
    "    for n, (train_idx, val_idx) in enumerate(kfold.split(train_x, train_y)):\n",
    "        \n",
    "        x_train, x_val = train_x.iloc[train_idx], train_x.iloc[val_idx]\n",
    "        y_train, y_val = train_y.iloc[train_idx].values.ravel(), train_y.iloc[val_idx].values.ravel()\n",
    "        \n",
    "        vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "        vectorizer.fit(x_train)\n",
    "        x_train = vectorizer.transform(x_train)\n",
    "        x_val = vectorizer.transform(x_val)\n",
    "        x_test = vectorizer.transform(test_x)\n",
    "        \n",
    "        dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "        dvalid = xgb.DMatrix(x_val, label=y_val)\n",
    "        xgtest = xgb.DMatrix(x_test)\n",
    "            \n",
    "        watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n",
    "        \n",
    "        print(f'fold {n+1} start')\n",
    "        \n",
    "        XGBModel = xgb.train(xgb_best_hyperparams, dtrain, 1000, watchlist, early_stopping_rounds=50, verbose_eval=100)\n",
    "        \n",
    "        cv[val_idx, :] = XGBModel.predict(dvalid)\n",
    "        \n",
    "        pred_test += XGBModel.predict(xgtest) / splits_tr\n",
    "        \n",
    "#         print(f'fold {n+1}', 'log_loss :', log_loss(y_val, cv[val_idx]))\n",
    "#         print(f'fold {n+1}', 'accuracy_score :', accuracy_score(y_val, np.argmax(cv[val_idx], axis=1)))\n",
    "        \n",
    "    pred_dict['xgb'+str(seed)] = cv\n",
    "    pred_test_dict['xgb'+str(seed)] = pred_test\n",
    "    print(f'seed {seed}', 'log_loss :', log_loss(train_y, cv))\n",
    "    print(f'seed {seed}', 'accuracy_score :', accuracy_score(train_y, np.argmax(cv, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51baa95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lucky_seeds = np.random.randint(0, 1000, num_seed_tr)\n",
    "\n",
    "# for i, seed in enumerate(lucky_seeds):\n",
    "\n",
    "#     kfold = StratifiedKFold(n_splits=splits_tr, random_state=seed, shuffle=True)\n",
    "#     cv = np.zeros((rows_train, num_classes))\n",
    "#     pred_test = np.zeros((rows_test, num_classes))\n",
    "\n",
    "#     for n, (train_idx, val_idx) in enumerate(kfold.split(train_x, train_y)):\n",
    "        \n",
    "#         x_train, x_val = train_x.iloc[train_idx], train_x.iloc[val_idx]\n",
    "#         y_train, y_val = train_y.iloc[train_idx].values.ravel(), train_y.iloc[val_idx].values.ravel()\n",
    "        \n",
    "# #         print(f'fold {n+1} start')\n",
    "        \n",
    "#         BERTModel = BertClassifier(bert_model=\"bert-base-cased\", random_state=basic_seed,\n",
    "#                                    epochs=5, validation_fraction=0, train_batch_size=8, eval_batch_size=2)\n",
    "#         BERTModel.fit(x_train, y_train)\n",
    "        \n",
    "#         cv[val_idx, :] = BERTModel.predict_proba(x_val)\n",
    "#         pred_test += BERTModel.predict_proba(test_x) / splits_tr\n",
    "        \n",
    "#         print(f'fold {n+1}', 'log_loss :', log_loss(y_val, cv[val_idx]))\n",
    "#         print(f'fold {n+1}', 'accuracy_score :', accuracy_score(y_val, np.argmax(cv[val_idx], axis=1)))\n",
    "        \n",
    "#     pred_dict['bert'+str(seed)] = cv\n",
    "#     pred_test_dict['bert'+str(seed)] = pred_test\n",
    "#     print(f'seed {seed}', 'log_loss :', log_loss(train_y, cv))\n",
    "#     print(f'seed {seed}', 'accuracy_score :', accuracy_score(train_y, np.argmax(cv, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1162ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dict(model):\n",
    "    with open('../pickle/pred_dict_'+model+'.pickle', 'rb') as fw:\n",
    "        pred_dict_new_local = pickle.load(fw)\n",
    "    with open('../pickle/pred_test_dict_'+model+'.pickle', 'rb') as fw:\n",
    "        pred_test_dict_new_local = pickle.load(fw)\n",
    "    return pred_dict_new_local, pred_test_dict_new_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609b7e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dict_mlp, pred_test_dict_mlp = load_dict('mlp_cv15')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c875cbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.zeros((rows_train, num_classes))\n",
    "for _, value in pred_dict_mlp.items():\n",
    "    pred += value\n",
    "pred /= len(pred_dict_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b64c33d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3bba20",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = np.zeros((rows_test, num_classes))\n",
    "for _, value in pred_test_dict_mlp.items():\n",
    "    pred_test += value\n",
    "pred_test /= len(pred_test_dict_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b894ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'accuracy_score: {accuracy_score(train_y, np.argmax(pred, axis=1)):.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5d23d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'accuracy_score: {accuracy_score(train_y, np.argmax(pred*0.95+pred2*0.05, axis=1)):.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2740bae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = np.argmax(pred_test*0.86+pred_test2*0.14, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7857fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1 start\n",
      "Iteration 1, loss = 2.88587743\n",
      "Iteration 2, loss = 1.93844120\n",
      "Iteration 3, loss = 0.85989388\n",
      "Iteration 4, loss = 0.31230709\n"
     ]
    }
   ],
   "source": [
    "lucky_seeds = np.random.randint(0, 1000, num_seed_tr)\n",
    "\n",
    "for i, seed in enumerate(lucky_seeds):\n",
    "\n",
    "    kfold = StratifiedKFold(n_splits=splits_tr, random_state=seed, shuffle=True)\n",
    "    cv = np.zeros((rows_train, num_classes))\n",
    "\n",
    "    for n, (train_idx, val_idx) in enumerate(kfold.split(train_x, train_y)):\n",
    "        \n",
    "        x_train, x_val = train_x.iloc[train_idx], train_x.iloc[val_idx]\n",
    "        y_train, y_val = train_y.iloc[train_idx].values.ravel(), train_y.iloc[val_idx].values.ravel()\n",
    "        \n",
    "        vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "        vectorizer.fit(x_train)\n",
    "        x_train = vectorizer.transform(x_train)\n",
    "        x_val = vectorizer.transform(x_val)\n",
    "        x_test = vectorizer.transform(test_x)\n",
    "        \n",
    "        print(f'fold {n+1} start')\n",
    "        \n",
    "        MLPModel = MLPClassifier(max_iter=12, hidden_layer_sizes=250, random_state=basic_seed, verbose=1)\n",
    "        MLPModel.fit(x_train, y_train)\n",
    "        \n",
    "        cv[val_idx, :] = MLPModel.predict_proba(x_val)\n",
    "\n",
    "#         cat_best_hyperparams = {\"iterations\": 10000, \"learning_rate\": 0.3}\n",
    "#         catmodel = CatBoostClassifier(**cat_best_hyperparams)\n",
    "#         catmodel.fit(x_train, y_train, eval_set=[(x_val, y_val)], early_stopping_rounds=50, verbose=10)\n",
    "        \n",
    "#         cv[val_idx] = catmodel.predict(x_val)\n",
    "        pred_test += MLPModel.predict_proba(x_test) / splits_tr\n",
    "        \n",
    "        print(f'fold {n+1}', 'log_loss :', log_loss(y_val, cv[val_idx]))\n",
    "        print(f'fold {n+1}', 'accuracy_score :', accuracy_score(y_val, np.argmax(cv[val_idx], axis=1)))\n",
    "        \n",
    "    pred_dict['mlp'+str(seed)] = cv\n",
    "    pred_test_dict['mlp'+str(seed)] = pred_test\n",
    "    print(f'seed {seed}', 'log_loss :', log_loss(train_y, cv))\n",
    "    print(f'seed {seed}', 'accuracy_score :', accuracy_score(train_y, np.argmax(cv, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c9752621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1 start\n",
      "Iteration 1, loss = 2.87701584\n",
      "Iteration 2, loss = 1.82490883\n",
      "Iteration 3, loss = 0.71171896\n",
      "Iteration 4, loss = 0.24221684\n",
      "Iteration 5, loss = 0.12334927\n",
      "Iteration 6, loss = 0.08526924\n",
      "Iteration 7, loss = 0.06854069\n",
      "Iteration 8, loss = 0.05963819\n",
      "Iteration 9, loss = 0.05408119\n",
      "Iteration 10, loss = 0.05068640\n",
      "Iteration 11, loss = 0.04802125\n",
      "Iteration 12, loss = 0.04613148\n",
      "fold 1 log_loss : 1.1809629445839431\n",
      "fold 1 accuracy_score : 0.7426900584795322\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2760/1766573642.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mx_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mx_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mx_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'fold {n+1} start'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   1868\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'The TF-IDF vectorizer is not fitted'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1869\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1870\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1871\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1872\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   1252\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1253\u001b[0m         \u001b[1;31m# use the same matrix-building strategy as fit_transform\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1254\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfixed_vocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1255\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1115\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    109\u001b[0m                 \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mngrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m                 \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mngrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_word_ngrams\u001b[1;34m(self, tokens, stop_words)\u001b[0m\n\u001b[0;32m    247\u001b[0m                            min(max_n + 1, n_original_tokens + 1)):\n\u001b[0;32m    248\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_original_tokens\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 249\u001b[1;33m                     \u001b[0mtokens_append\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspace_join\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moriginal_tokens\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lucky_seeds = np.random.randint(0, 1000, num_seed_tr)\n",
    "\n",
    "for i, seed in enumerate(lucky_seeds):\n",
    "\n",
    "    kfold = StratifiedKFold(n_splits=splits_tr, random_state=seed, shuffle=True)\n",
    "    cv = np.zeros((rows_train, num_classes))\n",
    "\n",
    "    for n, (train_idx, val_idx) in enumerate(kfold.split(train_x, train_y)):\n",
    "        \n",
    "        x_train, x_val = train_x.iloc[train_idx], train_x.iloc[val_idx]\n",
    "        y_train, y_val = train_y.iloc[train_idx].values.ravel(), train_y.iloc[val_idx].values.ravel()\n",
    "        \n",
    "        vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "        vectorizer.fit(x_train)\n",
    "        x_train = vectorizer.transform(x_train)\n",
    "        x_val = vectorizer.transform(x_val)\n",
    "        x_test = vectorizer.transform(test_x)\n",
    "        \n",
    "        print(f'fold {n+1} start')\n",
    "        \n",
    "        MLPModel = MLPClassifier(max_iter=12, hidden_layer_sizes=250, random_state=basic_seed, verbose=1)\n",
    "        MLPModel.fit(x_train, y_train)\n",
    "        \n",
    "        cv[val_idx, :] = MLPModel.predict_proba(x_val)\n",
    "\n",
    "#         cat_best_hyperparams = {\"iterations\": 10000, \"learning_rate\": 0.3}\n",
    "#         catmodel = CatBoostClassifier(**cat_best_hyperparams)\n",
    "#         catmodel.fit(x_train, y_train, eval_set=[(x_val, y_val)], early_stopping_rounds=50, verbose=10)\n",
    "        \n",
    "#         cv[val_idx] = catmodel.predict(x_val)\n",
    "        pred_test += MLPModel.predict_proba(x_test) / splits_tr\n",
    "        \n",
    "        print(f'fold {n+1}', 'log_loss :', log_loss(y_val, cv[val_idx]))\n",
    "        print(f'fold {n+1}', 'accuracy_score :', accuracy_score(y_val, np.argmax(cv[val_idx], axis=1)))\n",
    "        \n",
    "    pred_dict['mlp'+str(seed)] = cv\n",
    "    pred_test_dict['mlp'+str(seed)] = pred_test\n",
    "    print(f'seed {seed}', 'log_loss :', log_loss(train_y, cv))\n",
    "    print(f'seed {seed}', 'accuracy_score :', accuracy_score(train_y, np.argmax(cv, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6521a8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1 start\n",
      "Iteration 1, loss = 2.90069585\n",
      "Iteration 2, loss = 2.08529755\n",
      "Iteration 3, loss = 1.04899716\n",
      "Iteration 4, loss = 0.41136879\n",
      "Iteration 5, loss = 0.19385638\n",
      "Iteration 6, loss = 0.11965508\n",
      "Iteration 7, loss = 0.08817034\n",
      "Iteration 8, loss = 0.07188408\n",
      "Iteration 9, loss = 0.06239249\n",
      "Iteration 10, loss = 0.05615692\n",
      "Iteration 11, loss = 0.05192312\n",
      "Iteration 12, loss = 0.04885101\n",
      "fold 1 log_loss : 1.2182042496686905\n",
      "fold 1 accuracy_score : 0.7469135802469136\n",
      "fold 2 start\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2760/83395444.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mMLPModel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMLPClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_layer_sizes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbasic_seed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0mMLPModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mcv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMLPModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lucky_seeds = np.random.randint(0, 1000, num_seed_tr)\n",
    "\n",
    "for i, seed in enumerate(lucky_seeds):\n",
    "\n",
    "    kfold = StratifiedKFold(n_splits=splits_tr, random_state=seed, shuffle=True)\n",
    "    cv = np.zeros((rows_train, num_classes))\n",
    "\n",
    "    for n, (train_idx, val_idx) in enumerate(kfold.split(train_x, train_y)):\n",
    "        \n",
    "        x_train, x_val = train_x.iloc[train_idx], train_x.iloc[val_idx]\n",
    "        y_train, y_val = train_y.iloc[train_idx].values.ravel(), train_y.iloc[val_idx].values.ravel()\n",
    "        \n",
    "        vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "        vectorizer.fit(x_train)\n",
    "        x_train = vectorizer.transform(x_train)\n",
    "        x_val = vectorizer.transform(x_val)\n",
    "        x_test = vectorizer.transform(test_x)\n",
    "        \n",
    "        print(f'fold {n+1} start')\n",
    "        \n",
    "        MLPModel = MLPClassifier(max_iter=12, hidden_layer_sizes=200, random_state=basic_seed, verbose=1)\n",
    "        MLPModel.fit(x_train, y_train)\n",
    "        \n",
    "        cv[val_idx, :] = MLPModel.predict_proba(x_val)\n",
    "\n",
    "#         cat_best_hyperparams = {\"iterations\": 10000, \"learning_rate\": 0.3}\n",
    "#         catmodel = CatBoostClassifier(**cat_best_hyperparams)\n",
    "#         catmodel.fit(x_train, y_train, eval_set=[(x_val, y_val)], early_stopping_rounds=50, verbose=10)\n",
    "        \n",
    "#         cv[val_idx] = catmodel.predict(x_val)\n",
    "        pred_test += MLPModel.predict_proba(x_test) / splits_tr\n",
    "        \n",
    "        print(f'fold {n+1}', 'log_loss :', log_loss(y_val, cv[val_idx]))\n",
    "        print(f'fold {n+1}', 'accuracy_score :', accuracy_score(y_val, np.argmax(cv[val_idx], axis=1)))\n",
    "        \n",
    "    pred_dict['mlp'+str(seed)] = cv\n",
    "    pred_test_dict['mlp'+str(seed)] = pred_test\n",
    "    print(f'seed {seed}', 'log_loss :', log_loss(train_y, cv))\n",
    "    print(f'seed {seed}', 'accuracy_score :', accuracy_score(train_y, np.argmax(cv, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "048de213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1 start\n",
      "Iteration 1, loss = 2.92646606\n",
      "Iteration 2, loss = 2.25876659\n",
      "Iteration 3, loss = 1.33687210\n",
      "Iteration 4, loss = 0.60712800\n",
      "Iteration 5, loss = 0.28498176\n",
      "Iteration 6, loss = 0.16487697\n",
      "Iteration 7, loss = 0.11366884\n",
      "Iteration 8, loss = 0.08830485\n",
      "Iteration 9, loss = 0.07381324\n",
      "Iteration 10, loss = 0.06490315\n",
      "Iteration 11, loss = 0.05833006\n",
      "Iteration 12, loss = 0.05413481\n",
      "fold 1 log_loss : 1.250585081660447\n",
      "fold 1 accuracy_score : 0.7378167641325536\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2760/1149932992.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mx_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mx_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mx_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'fold {n+1} start'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   1868\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'The TF-IDF vectorizer is not fitted'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1869\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1870\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1871\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1872\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   1252\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1253\u001b[0m         \u001b[1;31m# use the same matrix-building strategy as fit_transform\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1254\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfixed_vocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1255\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1151\u001b[0m                           \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindptr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1152\u001b[0m                           dtype=self.dtype)\n\u001b[1;32m-> 1153\u001b[1;33m         \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_indices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1154\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36msort_indices\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1143\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhas_sorted_indices\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1144\u001b[1;33m             _sparsetools.csr_sort_indices(len(self.indptr) - 1, self.indptr,\n\u001b[0m\u001b[0;32m   1145\u001b[0m                                           self.indices, self.data)\n\u001b[0;32m   1146\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhas_sorted_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lucky_seeds = np.random.randint(0, 1000, num_seed_tr)\n",
    "\n",
    "for i, seed in enumerate(lucky_seeds):\n",
    "\n",
    "    kfold = StratifiedKFold(n_splits=splits_tr, random_state=seed, shuffle=True)\n",
    "    cv = np.zeros((rows_train, num_classes))\n",
    "\n",
    "    for n, (train_idx, val_idx) in enumerate(kfold.split(train_x, train_y)):\n",
    "        \n",
    "        x_train, x_val = train_x.iloc[train_idx], train_x.iloc[val_idx]\n",
    "        y_train, y_val = train_y.iloc[train_idx].values.ravel(), train_y.iloc[val_idx].values.ravel()\n",
    "        \n",
    "        vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "        vectorizer.fit(x_train)\n",
    "        x_train = vectorizer.transform(x_train)\n",
    "        x_val = vectorizer.transform(x_val)\n",
    "        x_test = vectorizer.transform(test_x)\n",
    "        \n",
    "        print(f'fold {n+1} start')\n",
    "        \n",
    "        MLPModel = MLPClassifier(max_iter=12, hidden_layer_sizes=150, random_state=basic_seed, verbose=1)\n",
    "        MLPModel.fit(x_train, y_train)\n",
    "        \n",
    "        cv[val_idx, :] = MLPModel.predict_proba(x_val)\n",
    "\n",
    "#         cat_best_hyperparams = {\"iterations\": 10000, \"learning_rate\": 0.3}\n",
    "#         catmodel = CatBoostClassifier(**cat_best_hyperparams)\n",
    "#         catmodel.fit(x_train, y_train, eval_set=[(x_val, y_val)], early_stopping_rounds=50, verbose=10)\n",
    "        \n",
    "#         cv[val_idx] = catmodel.predict(x_val)\n",
    "        pred_test += MLPModel.predict_proba(x_test) / splits_tr\n",
    "        \n",
    "        print(f'fold {n+1}', 'log_loss :', log_loss(y_val, cv[val_idx]))\n",
    "        print(f'fold {n+1}', 'accuracy_score :', accuracy_score(y_val, np.argmax(cv[val_idx], axis=1)))\n",
    "        \n",
    "    pred_dict['mlp'+str(seed)] = cv\n",
    "    pred_test_dict['mlp'+str(seed)] = pred_test\n",
    "    print(f'seed {seed}', 'log_loss :', log_loss(train_y, cv))\n",
    "    print(f'seed {seed}', 'accuracy_score :', accuracy_score(train_y, np.argmax(cv, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "466c7890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1 start\n",
      "Iteration 1, loss = 2.94078559\n",
      "Iteration 2, loss = 2.34423701\n",
      "Iteration 3, loss = 1.49642476\n",
      "Iteration 4, loss = 0.74847547\n",
      "Iteration 5, loss = 0.36632014\n",
      "Iteration 6, loss = 0.20955085\n",
      "Iteration 7, loss = 0.14038125\n",
      "Iteration 8, loss = 0.10584494\n",
      "Iteration 9, loss = 0.08586610\n",
      "Iteration 10, loss = 0.07332111\n",
      "Iteration 11, loss = 0.06517238\n",
      "Iteration 12, loss = 0.05923650\n",
      "fold 1 log_loss : 1.2876162641670004\n",
      "fold 1 accuracy_score : 0.7368421052631579\n",
      "fold 2 start\n",
      "fold 2 log_loss : 2.9940261436997018\n",
      "fold 2 accuracy_score : 0.08771929824561403\n",
      "fold 3 start\n",
      "fold 3 log_loss : 3.0055652886960416\n",
      "fold 3 accuracy_score : 0.05004874878128047\n",
      "seed 75 log_loss : 2.4290067935374666\n",
      "seed 75 accuracy_score : 0.2915628723058594\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2760/1276668168.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mngram_range\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0mx_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mx_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1821\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1822\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_warn_for_unused_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1823\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1824\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1825\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1200\u001b[0m         \u001b[0mmax_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1202\u001b[1;33m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0m\u001b[0;32m   1203\u001b[0m                                           self.fixed_vocabulary_)\n\u001b[0;32m   1204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1115\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    109\u001b[0m                 \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mngrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m                 \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mngrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_word_ngrams\u001b[1;34m(self, tokens, stop_words)\u001b[0m\n\u001b[0;32m    247\u001b[0m                            min(max_n + 1, n_original_tokens + 1)):\n\u001b[0;32m    248\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_original_tokens\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 249\u001b[1;33m                     \u001b[0mtokens_append\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspace_join\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moriginal_tokens\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lucky_seeds = np.random.randint(0, 1000, num_seed_tr)\n",
    "\n",
    "for i, seed in enumerate(lucky_seeds):\n",
    "\n",
    "    kfold = StratifiedKFold(n_splits=splits_tr, random_state=seed, shuffle=True)\n",
    "    cv = np.zeros((rows_train, num_classes))\n",
    "\n",
    "    for n, (train_idx, val_idx) in enumerate(kfold.split(train_x, train_y)):\n",
    "        \n",
    "        x_train, x_val = train_x.iloc[train_idx], train_x.iloc[val_idx]\n",
    "        y_train, y_val = train_y.iloc[train_idx].values.ravel(), train_y.iloc[val_idx].values.ravel()\n",
    "        \n",
    "        vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "        vectorizer.fit(x_train)\n",
    "        x_train = vectorizer.transform(x_train)\n",
    "        x_val = vectorizer.transform(x_val)\n",
    "        x_test = vectorizer.transform(test_x)\n",
    "        \n",
    "        print(f'fold {n+1} start')\n",
    "        \n",
    "        MLPModel = MLPClassifier(max_iter=12, hidden_layer_sizes=120, random_state=basic_seed, verbose=1)\n",
    "        MLPModel.fit(x_train, y_train)\n",
    "        \n",
    "        cv[val_idx, :] = MLPModel.predict_proba(x_val)\n",
    "\n",
    "#         cat_best_hyperparams = {\"iterations\": 10000, \"learning_rate\": 0.3}\n",
    "#         catmodel = CatBoostClassifier(**cat_best_hyperparams)\n",
    "#         catmodel.fit(x_train, y_train, eval_set=[(x_val, y_val)], early_stopping_rounds=50, verbose=10)\n",
    "        \n",
    "#         cv[val_idx] = catmodel.predict(x_val)\n",
    "        pred_test += MLPModel.predict_proba(x_test) / splits_tr\n",
    "        \n",
    "        print(f'fold {n+1}', 'log_loss :', log_loss(y_val, cv[val_idx]))\n",
    "        print(f'fold {n+1}', 'accuracy_score :', accuracy_score(y_val, np.argmax(cv[val_idx], axis=1)))\n",
    "        \n",
    "    pred_dict['mlp'+str(seed)] = cv\n",
    "    pred_test_dict['mlp'+str(seed)] = pred_test\n",
    "    print(f'seed {seed}', 'log_loss :', log_loss(train_y, cv))\n",
    "    print(f'seed {seed}', 'accuracy_score :', accuracy_score(train_y, np.argmax(cv, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ee782b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1 start\n",
      "Iteration 1, loss = 2.93844791\n",
      "Iteration 2, loss = 2.42155991\n",
      "Iteration 3, loss = 1.67550687\n",
      "Iteration 4, loss = 0.92869411\n",
      "Iteration 5, loss = 0.47811938\n",
      "Iteration 6, loss = 0.27241151\n",
      "Iteration 7, loss = 0.17891459\n",
      "Iteration 8, loss = 0.13192336\n",
      "Iteration 9, loss = 0.10524688\n",
      "Iteration 10, loss = 0.08863415\n",
      "Iteration 11, loss = 0.07761108\n",
      "Iteration 12, loss = 0.06997654\n",
      "fold 1 log_loss : 1.3087164072900916\n",
      "fold 1 accuracy_score : 0.7309941520467836\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2760/2137887347.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mngram_range\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mx_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0mx_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mx_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   1868\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'The TF-IDF vectorizer is not fitted'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1869\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1870\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1871\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1872\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   1252\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1253\u001b[0m         \u001b[1;31m# use the same matrix-building strategy as fit_transform\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1254\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfixed_vocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1255\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1115\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    109\u001b[0m                 \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mngrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m                 \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mngrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_word_ngrams\u001b[1;34m(self, tokens, stop_words)\u001b[0m\n\u001b[0;32m    247\u001b[0m                            min(max_n + 1, n_original_tokens + 1)):\n\u001b[0;32m    248\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_original_tokens\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 249\u001b[1;33m                     \u001b[0mtokens_append\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspace_join\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moriginal_tokens\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lucky_seeds = np.random.randint(0, 1000, num_seed_tr)\n",
    "\n",
    "for i, seed in enumerate(lucky_seeds):\n",
    "\n",
    "    kfold = StratifiedKFold(n_splits=splits_tr, random_state=seed, shuffle=True)\n",
    "    cv = np.zeros((rows_train, num_classes))\n",
    "\n",
    "    for n, (train_idx, val_idx) in enumerate(kfold.split(train_x, train_y)):\n",
    "        \n",
    "        x_train, x_val = train_x.iloc[train_idx], train_x.iloc[val_idx]\n",
    "        y_train, y_val = train_y.iloc[train_idx].values.ravel(), train_y.iloc[val_idx].values.ravel()\n",
    "        \n",
    "        vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "        vectorizer.fit(x_train)\n",
    "        x_train = vectorizer.transform(x_train)\n",
    "        x_val = vectorizer.transform(x_val)\n",
    "        x_test = vectorizer.transform(test_x)\n",
    "        \n",
    "        print(f'fold {n+1} start')\n",
    "        \n",
    "        MLPModel = MLPClassifier(max_iter=12, hidden_layer_sizes=100, random_state=basic_seed, verbose=1)\n",
    "        MLPModel.fit(x_train, y_train)\n",
    "        \n",
    "        cv[val_idx, :] = MLPModel.predict_proba(x_val)\n",
    "\n",
    "#         cat_best_hyperparams = {\"iterations\": 10000, \"learning_rate\": 0.3}\n",
    "#         catmodel = CatBoostClassifier(**cat_best_hyperparams)\n",
    "#         catmodel.fit(x_train, y_train, eval_set=[(x_val, y_val)], early_stopping_rounds=50, verbose=10)\n",
    "        \n",
    "#         cv[val_idx] = catmodel.predict(x_val)\n",
    "        pred_test += MLPModel.predict_proba(x_test) / splits_tr\n",
    "        \n",
    "        print(f'fold {n+1}', 'log_loss :', log_loss(y_val, cv[val_idx]))\n",
    "        print(f'fold {n+1}', 'accuracy_score :', accuracy_score(y_val, np.argmax(cv[val_idx], axis=1)))\n",
    "        \n",
    "    pred_dict['mlp'+str(seed)] = cv\n",
    "    pred_test_dict['mlp'+str(seed)] = pred_test\n",
    "    print(f'seed {seed}', 'log_loss :', log_loss(train_y, cv))\n",
    "    print(f'seed {seed}', 'accuracy_score :', accuracy_score(train_y, np.argmax(cv, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d2f0408e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1 start\n",
      "Iteration 1, loss = 2.95933919\n",
      "Iteration 2, loss = 2.62792367\n",
      "Iteration 3, loss = 2.13510445\n",
      "Iteration 4, loss = 1.53656763\n",
      "Iteration 5, loss = 1.00797684\n",
      "Iteration 6, loss = 0.63954663\n",
      "Iteration 7, loss = 0.41913100\n",
      "Iteration 8, loss = 0.29241655\n",
      "Iteration 9, loss = 0.21752007\n",
      "Iteration 10, loss = 0.17064270\n",
      "Iteration 11, loss = 0.13973926\n",
      "Iteration 12, loss = 0.11871930\n",
      "fold 1 log_loss : 1.4402451900990898\n",
      "fold 1 accuracy_score : 0.7170240415854451\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2760/3286595429.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mngram_range\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mx_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0mx_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mx_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   1868\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'The TF-IDF vectorizer is not fitted'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1869\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1870\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1871\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1872\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   1252\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1253\u001b[0m         \u001b[1;31m# use the same matrix-building strategy as fit_transform\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1254\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfixed_vocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1255\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1123\u001b[0m                     \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1125\u001b[1;33m             \u001b[0mj_indices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_counter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1126\u001b[0m             \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_counter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1127\u001b[0m             \u001b[0mindptr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lucky_seeds = np.random.randint(0, 1000, num_seed_tr)\n",
    "\n",
    "for i, seed in enumerate(lucky_seeds):\n",
    "\n",
    "    kfold = StratifiedKFold(n_splits=splits_tr, random_state=seed, shuffle=True)\n",
    "    cv = np.zeros((rows_train, num_classes))\n",
    "    pred_test = np.zeros((rows_test, num_classes))\n",
    "\n",
    "    for n, (train_idx, val_idx) in enumerate(kfold.split(train_x, train_y)):\n",
    "        \n",
    "        x_train, x_val = train_x.iloc[train_idx], train_x.iloc[val_idx]\n",
    "        y_train, y_val = train_y.iloc[train_idx].values.ravel(), train_y.iloc[val_idx].values.ravel()\n",
    "        \n",
    "        vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "        vectorizer.fit(x_train)\n",
    "        x_train = vectorizer.transform(x_train)\n",
    "        x_val = vectorizer.transform(x_val)\n",
    "        x_test = vectorizer.transform(test_x)\n",
    "        \n",
    "        print(f'fold {n+1} start')\n",
    "        \n",
    "        MLPModel = MLPClassifier(max_iter=12, hidden_layer_sizes=100, random_state=basic_seed, verbose=1)\n",
    "        MLPModel.fit(x_train, y_train)\n",
    "        \n",
    "        cv[val_idx, :] = MLPModel.predict_proba(x_val)\n",
    "\n",
    "#         cat_best_hyperparams = {\"iterations\": 10000, \"learning_rate\": 0.3}\n",
    "#         catmodel = CatBoostClassifier(**cat_best_hyperparams)\n",
    "#         catmodel.fit(x_train, y_train, eval_set=[(x_val, y_val)], early_stopping_rounds=50, verbose=10)\n",
    "        \n",
    "#         cv[val_idx] = catmodel.predict(x_val)\n",
    "        pred_test += MLPModel.predict_proba(x_test) / splits_tr\n",
    "        \n",
    "        print(f'fold {n+1}', 'log_loss :', log_loss(y_val, cv[val_idx]))\n",
    "        print(f'fold {n+1}', 'accuracy_score :', accuracy_score(y_val, np.argmax(cv[val_idx], axis=1)))\n",
    "        \n",
    "    pred_dict['mlp'+str(seed)] = cv\n",
    "    pred_test_dict['mlp'+str(seed)] = pred_test\n",
    "    print(f'seed {seed}', 'log_loss :', log_loss(train_y, cv))\n",
    "    print(f'seed {seed}', 'accuracy_score :', accuracy_score(train_y, np.argmax(cv, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca1d7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_dict(model, pred_dict, pred_test_dict):\n",
    "    pred_dict_local = {}\n",
    "    for key, value in pred_dict.items():\n",
    "        if model in key:\n",
    "            pred_dict_local[key]=value\n",
    "\n",
    "    pred_test_dict_local = {}\n",
    "    for key, value in pred_test_dict.items():\n",
    "        if model in key:\n",
    "            pred_test_dict_local[key]=value\n",
    "\n",
    "    pred_dict_new_local = dict(sorted(\n",
    "        pred_dict_local.items(), \n",
    "        key=lambda x:accuracy_score((train_y), np.argmax(list(x[1]), axis=1)), reverse=False)[:5])\n",
    "    pred_test_dict_new_local = {}\n",
    "    for key, value in pred_dict_new_local.items():\n",
    "        pred_test_dict_new_local[key]=pred_test_dict_local[key]\n",
    "        \n",
    "    return pred_dict_new_local, pred_test_dict_new_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7172b061",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dict(model, pred_dict, pred_test_dict):\n",
    "    with open('../pickle/pred_dict_'+model+'.pickle', 'wb') as fw:\n",
    "        pickle.dump(pred_dict, fw)\n",
    "    with open('../pickle/pred_test_dict_'+model+'.pickle', 'wb') as fw:\n",
    "        pickle.dump(pred_test_dict, fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbccbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dict_bert, pred_test_dict_bert = sort_dict('bert', pred_dict, pred_test_dict)\n",
    "save_dict('bert_epoch5_cv15', pred_dict_bert, pred_test_dict_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8a25b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dict_bert, pred_test_dict_bert = sort_dict('bert', pred_dict, pred_test_dict)\n",
    "save_dict('bert', pred_dict_bert, pred_test_dict_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d66d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dict_mlp, pred_test_dict_mlp = sort_dict('mlp', pred_dict, pred_test_dict)\n",
    "save_dict('mlp', pred_dict_mlp, pred_test_dict_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b7a9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.zeros((rows_train, num_classes))\n",
    "for _, value in pred_dict_mlp.items():\n",
    "    pred += value\n",
    "pred /= len(pred_dict_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd0991b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'accuracy_score: {accuracy_score(train_y, np.argmax(pred, axis=1)):.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca6380f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = np.zeros((rows_test, num_classes))\n",
    "for _, value in pred_test_dict_mlp.items():\n",
    "    pred_test += value\n",
    "pred_test /= len(pred_test_dict_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76eea18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dict(model):\n",
    "    with open('../pickle/pred_dict_'+model+'.pickle', 'rb') as fw:\n",
    "        pred_dict_new_local = pickle.load(fw)\n",
    "    with open('../pickle/pred_test_dict_'+model+'.pickle', 'rb') as fw:\n",
    "        pred_test_dict_new_local = pickle.load(fw)\n",
    "    return pred_dict_new_local, pred_test_dict_new_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af519682",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dict_mlp2, pred_test_dict_mlp2 = load_dict('mlp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3a119e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred2 = np.zeros((rows_train, num_classes))\n",
    "for _, value in pred_dict_mlp2.items():\n",
    "    pred2 += value\n",
    "pred2 /= len(pred_dict_mlp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006c97f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test2 = np.zeros((rows_test, num_classes))\n",
    "for _, value in pred_test_dict_mlp2.items():\n",
    "    pred_test2 += value\n",
    "pred_test2 /= len(pred_test_dict_mlp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a195bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'accuracy_score: {accuracy_score(train_y, np.argmax(pred*0.6+pred2*0.4, axis=1)):.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd589551",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = np.argmax(pred_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abd55c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission[\"target\"] = pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7ee0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1670641e",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_name = '20220410'\n",
    "submission_number = '1'\n",
    "submission.to_csv(f'../submission/{submission_name}-{submission_number}.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d166dc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f64d0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
